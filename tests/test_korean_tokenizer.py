"""
Korean Tokenizer Comparison Test

ì´ í…ŒìŠ¤íŠ¸ëŠ” ê¸°ì¡´ ì •ê·œì‹ í† í¬ë‚˜ì´ì €ì™€ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ì˜ ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
íì‡„ë§ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤ (ì™¸ë¶€ API í˜¸ì¶œ ì—†ìŒ).

ì‹¤í–‰ ë°©ë²•:
    python tests/test_korean_tokenizer.py
"""

import sys
import re
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

def test_regex_tokenizer():
    """ê¸°ì¡´ ì •ê·œì‹ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*80)
    print("1. ê¸°ì¡´ ì •ê·œì‹ í† í¬ë‚˜ì´ì € (Regex Tokenizer)")
    print("="*80)

    test_sentences = [
        "ì œ99í˜¸ ì§€ì‹œì‚¬í•­ì˜ ì‹œí–‰ì¼ì€ ì–¸ì œì…ë‹ˆê¹Œ?",
        "í˜œì§„ì´ê°€ ì—„ì²­ í˜¼ë‚¬ë˜ ê·¸ë‚  ì§€ì›ì´ê°€ ì—¬ì¹œì´ë‘ í—¤ì–´ì§„ ê·¸ë‚ ",
        "ì´ ë²•ë ¹ì€ 2024ë…„ 10ì›” 1ì¼ë¶€í„° ì‹œí–‰í•œë‹¤",
        "ê³µê³µê¸°ê´€ì—ì„œ ë¬¸ì„œë¥¼ ì‘ì„±í•  ë•ŒëŠ” ë°˜ë“œì‹œ í‘œì¤€ì•ˆì„ ë”°ë¼ì•¼ í•œë‹¤",
    ]

    for sentence in test_sentences:
        # Simple regex tokenization
        tokens = re.findall(r'[ê°€-í£]+|[a-zA-Z]+|[0-9]+', sentence)
        print(f"\nì›ë¬¸: {sentence}")
        print(f"í† í°: {tokens}")
        print(f"í† í° ìˆ˜: {len(tokens)}ê°œ")


def test_kiwi_tokenizer():
    """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*80)
    print("2. Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° (Morphological Analyzer)")
    print("="*80)

    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
        print("âœ… Kiwi ì´ˆê¸°í™” ì„±ê³µ\n")
    except ImportError:
        print("âŒ kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install kiwipiepy")
        return
    except Exception as e:
        print(f"âŒ Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    test_sentences = [
        "ì œ99í˜¸ ì§€ì‹œì‚¬í•­ì˜ ì‹œí–‰ì¼ì€ ì–¸ì œì…ë‹ˆê¹Œ?",
        "í˜œì§„ì´ê°€ ì—„ì²­ í˜¼ë‚¬ë˜ ê·¸ë‚  ì§€ì›ì´ê°€ ì—¬ì¹œì´ë‘ í—¤ì–´ì§„ ê·¸ë‚ ",
        "ì´ ë²•ë ¹ì€ 2024ë…„ 10ì›” 1ì¼ë¶€í„° ì‹œí–‰í•œë‹¤",
        "ê³µê³µê¸°ê´€ì—ì„œ ë¬¸ì„œë¥¼ ì‘ì„±í•  ë•ŒëŠ” ë°˜ë“œì‹œ í‘œì¤€ì•ˆì„ ë”°ë¼ì•¼ í•œë‹¤",
    ]

    # POS tags to extract (nouns, verbs, adjectives)
    extract_pos = {'NNG', 'NNP', 'VV', 'VA', 'MAG', 'SN', 'SL'}

    for sentence in test_sentences:
        result = kiwi.tokenize(sentence)

        # Extract meaningful morphemes
        tokens = []
        pos_info = []

        for morph in result:
            for form, tag, start, end in morph:
                if tag in extract_pos:
                    tokens.append(form)
                    pos_info.append(f"{form}/{tag}")

        print(f"\nì›ë¬¸: {sentence}")
        print(f"í˜•íƒœì†Œ ë¶„ì„: {' '.join(pos_info)}")
        print(f"ì¶”ì¶œëœ í† í°: {tokens}")
        print(f"í† í° ìˆ˜: {len(tokens)}ê°œ")


def test_search_scenario():
    """ì‹¤ì œ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ"""
    print("\n" + "="*80)
    print("3. ì‹¤ì œ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ")
    print("="*80)

    # ë¬¸ì„œ ì˜ˆì‹œ
    documents = [
        "ë³¸ ì§€ì‹œì‚¬í•­ì€ 2024ë…„ 10ì›” 1ì¼ë¶€í„° ì‹œí–‰í•œë‹¤.",
        "ê³µê³µê¸°ê´€ ë¬¸ì„œ ì‘ì„± í‘œì¤€ì•ˆì€ 2023ë…„ì— ì‹œí–‰ë˜ì—ˆë‹¤.",
        "ë²•ë ¹ ê°œì •ì•ˆì´ êµ­íšŒì—ì„œ í†µê³¼ë˜ì—ˆë‹¤.",
    ]

    # ì‚¬ìš©ì ì§ˆì˜
    query = "ì‹œí–‰ì¼ì€ ì–¸ì œì¸ê°€?"

    print(f"\nğŸ“„ ë¬¸ì„œ ëª©ë¡:")
    for i, doc in enumerate(documents, 1):
        print(f"   {i}. {doc}")

    print(f"\nğŸ” ì‚¬ìš©ì ì§ˆì˜: {query}")

    # ì •ê·œì‹ í† í¬ë‚˜ì´ì € ê²°ê³¼
    print("\n[ì •ê·œì‹ í† í¬ë‚˜ì´ì €]")
    query_tokens_regex = set(re.findall(r'[ê°€-í£]+', query))
    print(f"  ì§ˆì˜ í† í°: {query_tokens_regex}")

    for i, doc in enumerate(documents, 1):
        doc_tokens = set(re.findall(r'[ê°€-í£]+', doc))
        overlap = query_tokens_regex & doc_tokens
        print(f"  ë¬¸ì„œ {i} ë§¤ì¹­: {overlap} ({len(overlap)}ê°œ)")

    # Kiwi í† í¬ë‚˜ì´ì € ê²°ê³¼
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()

        print("\n[Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°]")

        # ì§ˆì˜ í˜•íƒœì†Œ ë¶„ì„
        query_result = kiwi.tokenize(query)
        query_tokens_kiwi = set()
        for morph in query_result:
            for form, tag, _, _ in morph:
                if tag in {'NNG', 'NNP', 'VV', 'VA', 'SN'}:
                    query_tokens_kiwi.add(form)

        print(f"  ì§ˆì˜ í† í°: {query_tokens_kiwi}")

        for i, doc in enumerate(documents, 1):
            doc_result = kiwi.tokenize(doc)
            doc_tokens_kiwi = set()
            for morph in doc_result:
                for form, tag, _, _ in morph:
                    if tag in {'NNG', 'NNP', 'VV', 'VA', 'SN'}:
                        doc_tokens_kiwi.add(form)

            overlap = query_tokens_kiwi & doc_tokens_kiwi
            print(f"  ë¬¸ì„œ {i} ë§¤ì¹­: {overlap} ({len(overlap)}ê°œ)")

        print("\nğŸ’¡ ê²°ê³¼ ë¶„ì„:")
        print("  - ì •ê·œì‹: 'ì‹œí–‰ì¼'ê³¼ 'ì‹œí–‰í•œë‹¤'ë¥¼ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì·¨ê¸‰")
        print("  - Kiwi: 'ì‹œí–‰ì¼' â†’ 'ì‹œí–‰', 'ì‹œí–‰í•œë‹¤' â†’ 'ì‹œí–‰' (ì–´ê·¼ ì¶”ì¶œ) â†’ ë§¤ì¹­ ì„±ê³µ")

    except ImportError:
        print("\n[Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°]")
        print("  âŒ kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "#"*80)
    print("# í•œêµ­ì–´ BM25 í† í¬ë‚˜ì´ì € ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("# íì‡„ë§ í™˜ê²½ í˜¸í™˜: âœ… (ì™¸ë¶€ API í˜¸ì¶œ ì—†ìŒ)")
    print("#"*80)

    # 1. ì •ê·œì‹ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    test_regex_tokenizer()

    # 2. Kiwi í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    test_kiwi_tokenizer()

    # 3. ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ
    test_search_scenario()

    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)
    print("\nê¶Œì¥ ì‚¬í•­:")
    print("  1. pip install kiwipiepy (ì•„ì§ ì„¤ì¹˜ ì•ˆ ëœ ê²½ìš°)")
    print("  2. make index ì‹¤í–‰í•˜ì—¬ ì¸ë±ìŠ¤ ì¬ìƒì„±")
    print("  3. ê¸°ì¡´ ëŒ€ë¹„ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ í™•ì¸")
    print()


if __name__ == "__main__":
    main()
