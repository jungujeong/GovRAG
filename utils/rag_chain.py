import time
import hashlib
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter, defaultdict
import numpy as np

from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import Document

from .vector_store import EnhancedVectorStore
from config import (
    OLLAMA_MODEL, 
    OLLAMA_BASE_URL, 
    TEMPERATURE,
    logger
)

class SimpleRAGChain:
    """ë‹¨ìˆœí•˜ê³  íš¨ê³¼ì ì¸ RAG ì²´ì¸"""
    
    def __init__(self, vector_store=None, max_documents=20):
        """RAG ì²´ì¸ ì´ˆê¸°í™”"""
        # ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
        if vector_store is None:
            self.vector_store = EnhancedVectorStore()
        else:
            self.vector_store = vector_store
        
        self.max_documents = max_documents
        
        # LLM ì´ˆê¸°í™”
        self._initialize_llms()
        
        # í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™”
        self._initialize_prompts()
        
        # ì²´ì¸ ì„¤ì •
        self._setup_chains()
        
        # ê°„ë‹¨í•œ ìºì‹œ (ìµœê·¼ 10ê°œ ì§ˆë¬¸ë§Œ)
        self.query_cache = {}
        self.max_cache_size = 10
        
        # ì¶œì²˜ ì‹ ë¢°ë„ ì¶”ì 
        self.source_reliability = defaultdict(lambda: {'score': 0.5, 'count': 0})
        
        logger.info("ë‹¨ìˆœ RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_llms(self):
        """LLM ì´ˆê¸°í™”"""
        try:
            self.llm = OllamaLLM(
                model=OLLAMA_MODEL,
                base_url=OLLAMA_BASE_URL,
                temperature=TEMPERATURE
            )
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {OLLAMA_MODEL}")
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            raise
        
    def _initialize_prompts(self):
        """í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™” - í•œêµ­ì–´ íŠ¹í™” ê°œì„ """
        # ë©”ì¸ QA í”„ë¡¬í”„íŠ¸ - í•œêµ­ì–´ íŠ¹í™” ê°œì„ 
        self.qa_prompt = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ì „ë¬¸ ë¶„ì„ AIì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ğŸ“‹ ì§ˆë¬¸: {question}

ğŸ“„ ê´€ë ¨ ë¬¸ì„œ:
{context}

ğŸ“Œ ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì—ì„œ ì§ì ‘ ì¸ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”
3. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”
4. ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë‹µë³€í•˜ì„¸ìš”
5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”

ğŸ’¬ ë‹µë³€:"""
        )
        
        # ìš”ì•½ í”„ë¡¬í”„íŠ¸ - í•œêµ­ì–´ íŠ¹í™” ê°œì„ 
        self.summarize_prompt = PromptTemplate.from_template(
            """ë‹¤ìŒ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ğŸ“„ ë¬¸ì„œ ë‚´ìš©:
{document}

ğŸ“Œ ìš”ì•½ ê·œì¹™:
- 3-5ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½
- ì¤‘ìš”í•œ í‚¤ì›Œë“œì™€ ì£¼ìš” ì •ë³´ í¬í•¨
- í•œêµ­ì–´ ë¬¸ë²•ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´ ì‚¬ìš©
- ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ì „ë‹¬

ğŸ“ ìš”ì•½:"""
        )
    
    def _setup_chains(self):
        """ì²´ì¸ ì„¤ì •"""
        self.qa_chain = self.qa_prompt | self.llm | StrOutputParser()
        self.summarize_chain = self.summarize_prompt | self.llm | StrOutputParser()
    
    def _clean_response(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ think íƒœê·¸ ì œê±°"""
        if not response:
            return response
        
        # <think>...</think> íƒœê·¸ì™€ ë‚´ìš©ì„ ëª¨ë‘ ì œê±°
        cleaned = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL | re.IGNORECASE)
        
        # ì—°ì†ëœ ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆ ì •ë¦¬
        cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)
        cleaned = re.sub(r'^\s+|\s+$', '', cleaned, flags=re.MULTILINE)
        
        return cleaned.strip()
    
    def _check_db_status(self) -> int:
        """ë²¡í„° DB ìƒíƒœ í™•ì¸"""
        try:
            if hasattr(self.vector_store, '_check_db_status'):
                return self.vector_store._check_db_status()
            elif hasattr(self.vector_store, '_collection'):
                collection = self.vector_store._collection
                return collection.count()
            else:
                # ê°„ë‹¨í•œ ê²€ìƒ‰ìœ¼ë¡œ DB ìƒíƒœ í™•ì¸
                test_docs = self.vector_store.similarity_search("test", k=1)
                return len(test_docs) if test_docs else 0
        except Exception as e:
            logger.warning(f"DB ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return 0
    
    def _clean_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì •ë¦¬ ë° í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ ì •ë¦¬
        query = query.strip()
        
        # ë„ˆë¬´ ê¸´ ì¿¼ë¦¬ëŠ” ìë¥´ê¸°
        if len(query) > 200:
            query = query[:200]
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ - ë” ì •êµí•˜ê²Œ
        keywords = []
        
        # 1. ê³ ìœ ëª…ì‚¬ íŒ¨í„´ (2ê¸€ì ì´ìƒ)
        proper_nouns = re.findall(r'[ê°€-í£]{2,}(?:ëŒ€ë³´ë¦„|ë‹¬ì§‘íƒœìš°ê¸°|í–‰ì‚¬|ì§€ì›|ì§€ì‹œ)', query)
        keywords.extend(proper_nouns)
        
        # 2. ì¼ë°˜ í‚¤ì›Œë“œ (2ê¸€ì ì´ìƒ)
        general_words = re.findall(r'[ê°€-í£a-zA-Z0-9]{2,}', query)
        
        # ë¶ˆìš©ì–´ ì œê±° - ë” í¬ê´„ì ìœ¼ë¡œ
        stopwords = {
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 
            'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê²ƒ', 'ê·¸', 'ì €', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',
            'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„êµ¬', 'ì™œ', 'ì–´ëŠ',
            'ëŒ€í•œ', 'ê´€í•œ', 'ëŒ€í•´', 'í†µí•´', 'ìœ„í•œ', 'ê°™ì€', 'ë‹¤ë¥¸', 'ëª¨ë“ ', 'ê°ê°',
            'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—'
        }
        
        meaningful_words = [w for w in general_words if w not in stopwords and len(w) >= 2]
        
        # 3. ì¤‘ìš”ë„ ê¸°ë°˜ ì •ë ¬
        word_counts = Counter(meaningful_words)
        
        # ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ ìš°ì„  ì„ íƒ
        important_words = []
        for word, count in word_counts.most_common():
            if word not in keywords:  # ì¤‘ë³µ ì œê±°
                important_words.append(word)
        
        # ìµœì¢… í‚¤ì›Œë“œ ì¡°í•© (ìµœëŒ€ 8ê°œ)
        final_keywords = (keywords + important_words)[:8]
        
        # ì›ë³¸ ì§ˆë¬¸ì´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if len(query) <= 50 and len(final_keywords) <= 3:
            return query
        
        result = ' '.join(final_keywords) if final_keywords else query
        logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ: '{query}' â†’ '{result}'")
        return result
        
    def _search_documents(self, query: str, k: int = 8) -> List[Document]:
        """ë¬¸ì„œ ê²€ìƒ‰ - ì •í™•ë„ ê°œì„ """
        try:
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰: '{query}' (k={k})")
            
            # DB í¬ê¸° í™•ì¸
            db_size = self._check_db_status()
            if db_size == 0:
                logger.warning("ë²¡í„° DBê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
                return []
            
            k = min(k, db_size)
            
            # 1ì°¨ ê²€ìƒ‰ - ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
            search_k = min(k * 3, 30)
            
            # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰
            if hasattr(self.vector_store, 'hierarchical_search'):
                docs = self.vector_store.hierarchical_search(query, k=search_k)
            elif hasattr(self.vector_store, 'similarity_search'):
                docs = self.vector_store.similarity_search(query, k=search_k)
            else:
                logger.error("ê²€ìƒ‰ ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            if not docs:
                return []
    
            # 2ì°¨ í•„í„°ë§ - í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜
            filtered_docs = self._advanced_filter_docs(docs, query)
            
            # ìµœì¢… ê²°ê³¼ (ìƒìœ„ kê°œ)
            result = filtered_docs[:k]
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ â†’ í•„í„°ë§ í›„ {len(filtered_docs)}ê°œ â†’ ìµœì¢… {len(result)}ê°œ")
            return result
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
            
    def _advanced_filter_docs(self, docs: List[Document], query: str) -> List[Document]:
        """ê³ ê¸‰ ë¬¸ì„œ í•„í„°ë§"""
        if not docs:
            return []
        
        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = set(re.findall(r'[ê°€-í£a-zA-Z0-9]{2,}', query.lower()))
        
        scored_docs = []
        for doc in docs:
            doc_text = doc.page_content.lower()
            
            # 1. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            exact_matches = sum(1 for keyword in query_keywords if keyword in doc_text)
            exact_score = exact_matches / len(query_keywords) if query_keywords else 0
            
            # 2. ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜ (í‚¤ì›Œë“œì˜ ì¼ë¶€ê°€ í¬í•¨ëœ ê²½ìš°)
            partial_matches = 0
            for keyword in query_keywords:
                if len(keyword) >= 3:
                    # 3ê¸€ì ì´ìƒ í‚¤ì›Œë“œì˜ ì• 2ê¸€ìê°€ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸
                    if keyword[:2] in doc_text:
                        partial_matches += 0.5
            
            partial_score = partial_matches / len(query_keywords) if query_keywords else 0
            
            # 3. ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜
            quality_score = doc.metadata.get('quality_score', 0.5)
            
            # 4. ë¬¸ì„œ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ ë¬¸ì„œ ì„ í˜¸)
            doc_length = len(doc.page_content)
            if 100 <= doc_length <= 1500:
                length_score = 1.0
            elif 50 <= doc_length <= 2000:
                length_score = 0.8
            else:
                length_score = 0.5
            
            # 5. ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = (
                exact_score * 0.4 +           # ì •í™•í•œ ë§¤ì¹­ 40%
                partial_score * 0.2 +         # ë¶€ë¶„ ë§¤ì¹­ 20%
                quality_score * 0.2 +         # í’ˆì§ˆ 20%
                length_score * 0.2            # ê¸¸ì´ 20%
            )
            
            # ë©”íƒ€ë°ì´í„°ì— ì ìˆ˜ ì €ì¥
            doc.metadata['relevance_score'] = final_score
            doc.metadata['exact_match_score'] = exact_score
            doc.metadata['partial_match_score'] = partial_score
            
            # ìµœì†Œ ì„ê³„ê°’ ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨
            if final_score >= 0.1:  # ì„ê³„ê°’ ë‚®ì¶¤
                scored_docs.append((doc, final_score))
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ë¬¸ì„œë“¤ ë°˜í™˜
        result = [doc for doc, score in scored_docs]
        
        # ë¡œê¹…
        if scored_docs:
            top_score = scored_docs[0][1]
            logger.info(f"í•„í„°ë§ ê²°ê³¼: ìµœê³  ì ìˆ˜ {top_score:.3f}, {len(result)}ê°œ ë¬¸ì„œ ì„ íƒ")
        
        return result
    
    def _filter_relevant_docs(self, docs: List[Document], query: str) -> List[Document]:
        """ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë§Œ í•„í„°ë§ - ë” ì—„ê²©í•˜ê²Œ"""
        if not docs:
            return []
        
        # ì´ë¯¸ ê³ ê¸‰ í•„í„°ë§ì„ ê±°ì³¤ìœ¼ë¯€ë¡œ ì¶”ê°€ í•„í„°ë§ì€ ìµœì†Œí™”
        # ë‹¨ì§€ relevance_score ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬
        docs_with_scores = [(doc, doc.metadata.get('relevance_score', 0)) for doc in docs]
        docs_with_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ë¬¸ì„œë“¤ë§Œ ì„ íƒ (ìµœì†Œ ì ìˆ˜ 0.15 ì´ìƒ)
        filtered = [doc for doc, score in docs_with_scores if score >= 0.15]
        
        # ìµœì†Œ 1ê°œëŠ” ë³´ì¥ (ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´)
        if not filtered and docs:
            filtered = [docs_with_scores[0][0]]
        
        return filtered[:self.max_documents]
    
    def _build_context(self, docs: List[Document]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ê´€ë ¨ ë‚´ìš© ìš°ì„  í¬í•¨"""
        if not docs:
            return ""
        
        context_parts = []
        for i, doc in enumerate(docs):
            # ì¶œì²˜ ì •ë³´ ì •ë¦¬
            source = doc.metadata.get('source', f'ë¬¸ì„œ{i+1}')
            if '/' in source:
                source = source.split('/')[-1]
            elif '\\' in source:
                source = source.split('\\')[-1]
                
            # ì ìˆ˜ ì •ë³´ (ìˆëŠ” ê²½ìš°ë§Œ)
            score_info = ""
            relevance = doc.metadata.get('relevance_score')
            if relevance is not None:
                score_info = f" (ê´€ë ¨ì„±: {relevance:.2f})"
            
            # ë¬¸ì„œ ë‚´ìš© - ìë¥´ì§€ ì•Šê³  ì „ì²´ ì‚¬ìš© (ìµœëŒ€ 2000ìê¹Œì§€)
            content = doc.page_content
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            context_parts.append(f"[{source}{score_info}]\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _extract_sources(self, docs: List[Document], question: str = "") -> List[str]:
        """ì¶œì²˜ ì¶”ì¶œ - ì‹¤ì œ ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ” ë¬¸ì„œë§Œ"""
        sources_with_scores = []
        
        # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = set()
        if question:
            # 1. ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
            keywords = re.findall(r'[ê°€-í£]{3,}', question.lower())
            
            # 2. ì¡°ì‚¬, ì–´ë¯¸ ì œê±°í•˜ì—¬ ì–´ê·¼ ì¶”ì¶œ
            processed_keywords = []
            for keyword in keywords:
                # ì¼ë°˜ì ì¸ ì¡°ì‚¬/ì–´ë¯¸ íŒ¨í„´ ì œê±°
                if keyword.endswith(('ì—ì„œ', 'ì—ê²Œ', 'ì—ëŠ”', 'ì—ë„', 'ì—ë§Œ')):
                    processed_keywords.append(keyword[:-2])
                elif keyword.endswith(('ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ')):
                    processed_keywords.append(keyword[:-1])
                elif keyword.endswith(('ìŠµë‹ˆê¹Œ', 'ìŠµë‹ˆë‹¤', 'í–ˆìŠµë‹ˆê¹Œ', 'ì…ë‹ˆê¹Œ')):
                    # ì˜ë¬¸ì‚¬/ì¡´ëŒ“ë§ ì–´ë¯¸ ì œê±°
                    if keyword.endswith('ìŠµë‹ˆê¹Œ'):
                        processed_keywords.append(keyword[:-3])
                    elif keyword.endswith('ìŠµë‹ˆë‹¤'):
                        processed_keywords.append(keyword[:-3])
                    elif keyword.endswith('í–ˆìŠµë‹ˆê¹Œ'):
                        processed_keywords.append(keyword[:-4])
                    elif keyword.endswith('ì…ë‹ˆê¹Œ'):
                        processed_keywords.append(keyword[:-3])
                else:
                    processed_keywords.append(keyword)
            
            # 3. ë¶ˆìš©ì–´ ì œê±°
            stopwords = {'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„êµ¬', 'ì™œ', 'ì–´ëŠ', 'ì§€ì‹œ', 'í–ˆ', 'ë°”ëŒ'}
            
            # 4. ìµœì¢… í‚¤ì›Œë“œ ì„ íƒ (2ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸)
            final_keywords = [k for k in processed_keywords if len(k) >= 2 and k not in stopwords]
            question_keywords = set(final_keywords)
        
        for doc in docs:
            source = doc.metadata.get('source', 'unknown')
            if '/' in source:
                source = source.split('/')[-1]
            elif '\\' in source:
                source = source.split('\\')[-1]
            
            # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            relevance_score = doc.metadata.get('relevance_score', 0)
            
            # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì§ˆë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            doc_text = doc.page_content.lower()
            keyword_matches = 0
            for keyword in question_keywords:
                if keyword in doc_text:
                    keyword_matches += 1
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ë¥  ê³„ì‚°
            keyword_match_rate = keyword_matches / len(question_keywords) if question_keywords else 0
            
            # ê´€ë ¨ì„±ì´ ë†’ê³  ì‹¤ì œ í‚¤ì›Œë“œê°€ ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œë§Œ ì¶œì²˜ë¡œ í¬í•¨ (ë” ì—„ê²©í•˜ê²Œ)
            if relevance_score >= 0.6 and keyword_match_rate >= 0.5:
                # ì¶œì²˜ ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
                self.source_reliability[source]['count'] += 1
                
                # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
                current_score = self.source_reliability[source]['score']
                count = self.source_reliability[source]['count']
                
                # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
                new_score = (current_score * (count - 1) + relevance_score) / count
                self.source_reliability[source]['score'] = new_score
                
                sources_with_scores.append((source, relevance_score))
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        unique_sources = {}
        for source, score in sources_with_scores:
            if source not in unique_sources or unique_sources[source] < score:
                unique_sources[source] = score
        
        sorted_sources = sorted(unique_sources.items(), key=lambda x: x[1], reverse=True)
        return [source for source, _ in sorted_sources[:1]]  # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ 1ê°œë§Œ
    
    def _validate_relevance(self, docs: List[Document], query: str) -> bool:
        """ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„± ì—„ê²© ê²€ì¦ - ê°œì„ ëœ ë²„ì „"""
        if not docs:
            return False
        
        # ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ - ë” ì •êµí•˜ê²Œ
        query_lower = query.lower()
        
        # 1. ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ (3ê¸€ì ì´ìƒ)
        important_keywords = re.findall(r'[ê°€-í£]{3,}', query_lower)
        
        # 2. ì¼ë°˜ í‚¤ì›Œë“œë“¤ (2ê¸€ì ì´ìƒ)
        general_keywords = re.findall(r'[ê°€-í£a-zA-Z0-9]{2,}', query_lower)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 
            'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê²ƒ', 'ê·¸', 'ì €', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ',
            'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„êµ¬', 'ì™œ', 'ì–´ëŠ',
            'ëŒ€í•œ', 'ê´€í•œ', 'ëŒ€í•´', 'í†µí•´', 'ìœ„í•œ', 'ê°™ì€', 'ë‹¤ë¥¸', 'ëª¨ë“ ', 'ê°ê°',
            'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—',
            'ì§€ì‹œ', 'í–ˆìŠµë‹ˆê¹Œ', 'ë¬´ì—‡ì…ë‹ˆê¹Œ', 'ì–´ë–¤', 'ìœ„í•´'
        }
        
        # ì¤‘ìš” í‚¤ì›Œë“œ ìš°ì„ , ì¼ë°˜ í‚¤ì›Œë“œ ë³´ì¡°
        filtered_important = [k for k in important_keywords if k not in stopwords]
        filtered_general = [k for k in general_keywords if k not in stopwords and k not in filtered_important]
        
        # ìµœì¢… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì¤‘ìš” í‚¤ì›Œë“œ + ì¼ë°˜ í‚¤ì›Œë“œ ìƒìœ„ 5ê°œ)
        final_keywords = filtered_important + filtered_general[:5]
        
        if not final_keywords:
            return True  # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ í†µê³¼
        
        logger.info(f"ê²€ì¦ í‚¤ì›Œë“œ: {final_keywords}")
        
        # ê° ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
        total_relevance = 0
        for doc in docs:
            doc_text = doc.page_content.lower()
            
            # ì •í™•í•œ ë§¤ì¹­
            exact_matches = sum(1 for keyword in final_keywords if keyword in doc_text)
            
            # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œì˜ ì¼ë¶€ê°€ í¬í•¨ëœ ê²½ìš°)
            partial_matches = 0
            for keyword in final_keywords:
                if len(keyword) >= 4:
                    # 4ê¸€ì ì´ìƒ í‚¤ì›Œë“œì˜ ì• 3ê¸€ìê°€ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸
                    if keyword[:3] in doc_text:
                        partial_matches += 0.5
                elif len(keyword) >= 3:
                    # 3ê¸€ì í‚¤ì›Œë“œì˜ ì• 2ê¸€ìê°€ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸
                    if keyword[:2] in doc_text:
                        partial_matches += 0.3
            
            # ê´€ë ¨ì„± ê³„ì‚°
            relevance = (exact_matches + partial_matches) / len(final_keywords) if final_keywords else 0
            total_relevance += relevance
        
        # í‰ê·  ê´€ë ¨ì„±ì´ 5% ì´ìƒì´ë©´ ê´€ë ¨ ìˆë‹¤ê³  íŒë‹¨ (ë§¤ìš° ì™„í™”)
        avg_relevance = total_relevance / len(docs) if docs else 0
        
        logger.info(f"ê´€ë ¨ì„± ê²€ì¦: í‚¤ì›Œë“œ {len(final_keywords)}ê°œ, í‰ê·  ê´€ë ¨ì„± {avg_relevance:.3f}")
        
        return avg_relevance >= 0.05
    
    def query(self, question: str) -> str:
        """ì§ˆë¬¸ ì²˜ë¦¬ - ê´€ë ¨ì„± ê²€ì¦ ê°•í™”"""
        start_time = time.time()
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not question.strip():
                return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            # ìºì‹œ í™•ì¸
            cache_key = hashlib.md5(question.encode()).hexdigest()
            if cache_key in self.query_cache:
                logger.info(f"ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜: {question[:30]}...")
                return self.query_cache[cache_key]
            
            logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {question}")
            
            # 1. ì¿¼ë¦¬ ì •ë¦¬
            cleaned_query = self._clean_query(question)
            logger.info(f"ì •ë¦¬ëœ ì¿¼ë¦¬: {cleaned_query}")
            
            # 2. ë¬¸ì„œ ê²€ìƒ‰
            docs = self._search_documents(cleaned_query, k=min(5, self.max_documents))
            
            if not docs:
                return "ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•˜ê±°ë‚˜ ë¬¸ì„œë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            
            logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(docs)}ê°œ")
            
            # 3. ê´€ë ¨ì„± ì—„ê²© ê²€ì¦ - ë‹¤ì‹œ í™œì„±í™”
            if not self._validate_relevance(docs, question):
                return "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # 4. ê´€ë ¨ì„± í•„í„°ë§ - ìƒìœ„ 2ê°œë§Œ ì„ íƒ
            filtered_docs = self._filter_relevant_docs(docs, cleaned_query)[:2]
            
            if not filtered_docs:
                return "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            logger.info(f"í•„í„°ë§ëœ ë¬¸ì„œ: {len(filtered_docs)}ê°œ")
            
            # 5. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(filtered_docs)
            
            # 6. ë‹µë³€ ìƒì„±
            response = self.qa_chain.invoke({
                "question": question,
                "context": context
            })
            
            # 7. think íƒœê·¸ ì œê±°
            response = self._clean_response(response)
            
            # 7. ë‹µë³€ ê²€ì¦ - "ì°¾ì„ ìˆ˜ ì—†ë‹¤"ëŠ” ë‹µë³€ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶œì²˜ ì¶”ê°€
            if "ì°¾ì„ ìˆ˜ ì—†" not in response and "ì—†ìŠµë‹ˆë‹¤" not in response:
                sources = self._extract_sources(filtered_docs, question)
                if sources and "ì¶œì²˜:" not in response:
                    sources_text = ", ".join(sources)
                    response = f"{response}\n\nì¶œì²˜: {sources_text}"
                elif not sources and "ì¶œì²˜:" not in response:
                    # ì¶œì²˜ê°€ ì—†ì–´ë„ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ìƒì„±í–ˆë‹¤ë©´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ 1ê°œ ë¬¸ì„œë§Œ ì¶œì²˜ë¡œ ì¶”ê°€
                    if filtered_docs:
                        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ 1ê°œë§Œ ì„ íƒ
                        best_doc = max(filtered_docs, key=lambda x: x.metadata.get('relevance_score', 0))
                        source = best_doc.metadata.get('source', '')
                        if source and source != 'ì•Œ ìˆ˜ ì—†ìŒ':
                            if '/' in source:
                                source = source.split('/')[-1]
                            elif '\\' in source:
                                source = source.split('\\')[-1]
                            response = f"{response}\n\nì¶œì²˜: {source}"
            
            # 8. ìºì‹œ ì €ì¥
            if len(self.query_cache) >= self.max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self.query_cache))
                del self.query_cache[oldest_key]
            
            self.query_cache[cache_key] = response
            
            elapsed = time.time() - start_time
            logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            
            return response
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}, ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def summarize(self, document: str) -> str:
        """ë¬¸ì„œ ìš”ì•½"""
        try:
            if not document.strip():
                return "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            if len(document) > 3000:
                document = document[:3000] + "..."
            
            response = self.summarize_chain.invoke({"document": document})
            return self._clean_response(response)
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ìš”ì•½ ì˜¤ë¥˜: {e}")
            return "ë¬¸ì„œ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.query_cache.clear()
        logger.info("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_source_reliability(self) -> Dict[str, Dict]:
        """ì¶œì²˜ ì‹ ë¢°ë„ ì •ë³´ ë°˜í™˜"""
        return dict(self.source_reliability)

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
RAGChain = SimpleRAGChain 