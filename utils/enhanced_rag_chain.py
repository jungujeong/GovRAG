import re
import time
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, Counter
from datetime import datetime

from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import Document

from .enhanced_vector_store import EnhancedVectorStore
from config import (
    OLLAMA_MODEL, 
    OLLAMA_BASE_URL, 
    TEMPERATURE,
    logger
)

class EnhancedRAGChain:
    """ê³ ê¸‰ RAG ì²´ì¸ - ë‹¤ë‹¨ê³„ ì¶”ë¡  ë° ë‹µë³€ ê²€ì¦"""
    
    def __init__(self, vector_store: Optional[EnhancedVectorStore] = None):
        """RAG ì²´ì¸ ì´ˆê¸°í™”"""
        
        # ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
        self.vector_store = vector_store or EnhancedVectorStore()
        
        # LLM ì´ˆê¸°í™”
        self._initialize_llm()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self._initialize_prompts()
        
        # ì²´ì¸ ì„¤ì •
        self._setup_chains()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.query_cache = {}
        self.performance_stats = defaultdict(int)
        
        # ë‹µë³€ í’ˆì§ˆ ì¶”ì 
        self.answer_quality_tracker = defaultdict(list)
        
        logger.info("ê³ ê¸‰ RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™”"""
        try:
            self.llm = OllamaLLM(
                model=OLLAMA_MODEL,
                base_url=OLLAMA_BASE_URL,
                temperature=TEMPERATURE
            )
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {OLLAMA_MODEL}")
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            raise
    
    def _initialize_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        
        # 1. ì§ˆë¬¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.query_analysis_prompt = PromptTemplate.from_template(
            """ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ìž‘ì„±í•˜ì„¸ìš”:
- í•µì‹¬ í‚¤ì›Œë“œ: [ì£¼ìš” í‚¤ì›Œë“œë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„]
- ì§ˆë¬¸ ìœ í˜•: [ì‚¬ì‹¤ í™•ì¸/ë°©ë²• ì„¤ëª…/ë¹„êµ ë¶„ì„/ê¸°íƒ€]
- í•„ìš”í•œ ì •ë³´: [ë‹µë³€ì„ ìœ„í•´ í•„ìš”í•œ êµ¬ì²´ì  ì •ë³´]

ë¶„ì„ ê²°ê³¼:"""
        )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ í‰ê°€ í”„ë¡¬í”„íŠ¸
        self.context_evaluation_prompt = PromptTemplate.from_template(
            """ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸°ì— ì¶©ë¶„í•œì§€ í‰ê°€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë¬¸ì„œ ë‚´ìš©:
{context}

í‰ê°€ ê¸°ì¤€:
1. ê´€ë ¨ì„±: ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ì–´ ìžˆëŠ”ê°€?
2. ì™„ì „ì„±: ì§ˆë¬¸ì— ì™„ì „ížˆ ë‹µí•  ìˆ˜ ìžˆëŠ” ì •ë³´ê°€ ìžˆëŠ”ê°€?
3. ì‹ ë¢°ì„±: ì •ë³´ê°€ ëª…í™•í•˜ê³  ì¼ê´€ëœê°€?

í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”:
- ê´€ë ¨ì„±: [ë†’ìŒ/ë³´í†µ/ë‚®ìŒ]
- ì™„ì „ì„±: [ì™„ì „í•¨/ë¶€ë¶„ì /ë¶ˆì™„ì „í•¨]
- ì‹ ë¢°ì„±: [ë†’ìŒ/ë³´í†µ/ë‚®ìŒ]
- ì¢…í•© í‰ê°€: [ì í•©/ë¶€ë¶„ì í•©/ë¶€ì í•©]

í‰ê°€:"""
        )
        
        # 3. ë©”ì¸ QA í”„ë¡¬í”„íŠ¸ (ê°œì„ ëœ ë²„ì „)
        self.qa_prompt = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ì „ë¬¸ ë¶„ì„ AIìž…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ðŸ“‹ ì§ˆë¬¸: {question}

ðŸ“„ ê´€ë ¨ ë¬¸ì„œ:
{context}

ðŸ“Œ ë‹µë³€ ìž‘ì„± ì§€ì¹¨:
1. ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•ížˆ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì¸ìš©ì„ í¬í•¨í•˜ì—¬ ìƒì„¸ížˆ ë‹µë³€í•˜ì„¸ìš”  
3. ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
4. ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì „í˜€ ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ½ê³  ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”
6. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‚ ì§œ, ê¸°ê´€ëª… ë“±ì„ í¬í•¨í•˜ì„¸ìš”

ðŸ’¬ ë‹µë³€:"""
        )
        
        # 4. ë‹µë³€ ê²€ì¦ í”„ë¡¬í”„íŠ¸
        self.answer_verification_prompt = PromptTemplate.from_template(
            """ë‹¤ìŒ ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ë¬¸ì„œ ë‚´ìš©:
{context}

ìƒì„±ëœ ë‹µë³€:
{answer}

ê²€ì¦ ê¸°ì¤€:
1. ì‚¬ì‹¤ ì •í™•ì„±: ë‹µë³€ì´ ë¬¸ì„œì˜ ì‚¬ì‹¤ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?
2. ì™„ì „ì„±: ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìžˆëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì¶©ë¶„ížˆ í¬í•¨í•˜ëŠ”ê°€?
3. ì¼ê´€ì„±: ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ë˜ëŠ”ê°€?

ê²€ì¦ ê²°ê³¼:
- ì‚¬ì‹¤ ì •í™•ì„±: [ì •í™•/ë¶€ë¶„ì  ì •í™•/ë¶€ì •í™•]
- ì™„ì „ì„±: [ì™„ì „/ë¶€ë¶„ì /ë¶ˆì™„ì „]
- ì¼ê´€ì„±: [ì¼ê´€ë¨/ë¶€ë¶„ì  ì¼ê´€/ë¶ˆì¼ì¹˜]
- ìµœì¢… í‰ê°€: [ê²€ì¦ë¨/ìˆ˜ì • í•„ìš”/ë¶€ì ì ˆ]

ê²€ì¦:"""
        )
        
        # 5. ìš”ì•½ í”„ë¡¬í”„íŠ¸
        self.summarization_prompt = PromptTemplate.from_template(
            """ë‹¤ìŒ ë¬¸ì„œë¥¼ í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ë¬¸ì„œ:
{document}

ìš”ì•½ ì§€ì¹¨:
- 3-5ê°œ ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½
- í•µì‹¬ ë‚´ìš©ê³¼ ì£¼ìš” ì„¸ë¶€ì‚¬í•­ í¬í•¨
- ëª…í™•í•˜ê³  ê°„ê²°í•œ í‘œí˜„ ì‚¬ìš©

ìš”ì•½:"""
        )
    
    def _setup_chains(self):
        """ì²´ì¸ ì„¤ì •"""
        self.query_analysis_chain = self.query_analysis_prompt | self.llm | StrOutputParser()
        self.context_evaluation_chain = self.context_evaluation_prompt | self.llm | StrOutputParser()
        self.qa_chain = self.qa_prompt | self.llm | StrOutputParser()
        self.answer_verification_chain = self.answer_verification_prompt | self.llm | StrOutputParser()
        self.summarization_chain = self.summarization_prompt | self.llm | StrOutputParser()
    
    def _analyze_query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ ë¶„ì„"""
        try:
            analysis_result = self.query_analysis_chain.invoke({"question": question})
            
            # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
            analysis = {}
            for line in analysis_result.split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip('- ').lower().replace(' ', '_')
                    analysis[key] = value.strip()
            
            return analysis
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"í•µì‹¬_í‚¤ì›Œë“œ": question, "ì§ˆë¬¸_ìœ í˜•": "ê¸°íƒ€"}
    
    def _search_with_multiple_strategies(self, question: str, analysis: Dict[str, Any]) -> List[Document]:
        """ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëžµ ì ìš©"""
        try:
            all_results = []
            
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë©”ì¸)
            hybrid_results = self.vector_store.hybrid_search(
                query=question, 
                k=8,
                vector_weight=0.7,
                bm25_weight=0.3
            )
            all_results.extend(hybrid_results)
            
            # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ë³´ì™„)
            if "í•µì‹¬_í‚¤ì›Œë“œ" in analysis:
                keywords = analysis["í•µì‹¬_í‚¤ì›Œë“œ"]
                keyword_results = self.vector_store.keyword_search(
                    query=keywords,
                    k=5
                )
                all_results.extend(keyword_results)
            
            # 3. ì˜ë¯¸ì  ê²€ìƒ‰ (ì¶”ê°€)
            semantic_results = self.vector_store.semantic_search(
                query=question,
                k=5,
                similarity_threshold=0.4
            )
            all_results.extend(semantic_results)
            
            # ì¤‘ë³µ ì œê±° (ë‚´ìš© ê¸°ë°˜)
            unique_results = self._deduplicate_documents(all_results)
            
            # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            return unique_results[:10]
            
        except Exception as e:
            logger.error(f"ë‹¤ì¤‘ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _deduplicate_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ì¤‘ë³µ ì œê±°"""
        seen_content = set()
        unique_docs = []
        
        for doc in documents:
            # ë‚´ìš©ì˜ í•´ì‹œê°’ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
            
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_docs.append(doc)
        
        return unique_docs
    
    def _evaluate_context_relevance(self, question: str, documents: List[Document]) -> List[Document]:
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í‰ê°€"""
        try:
            if not documents:
                return []
            
            # ë¬¸ì„œê°€ ë„ˆë¬´ ë§Žìœ¼ë©´ ìƒìœ„ 5ê°œë§Œ í‰ê°€
            docs_to_evaluate = documents[:5]
            
            evaluated_docs = []
            
            for doc in docs_to_evaluate:
                try:
                    # ì»¨í…ìŠ¤íŠ¸ í‰ê°€
                    evaluation = self.context_evaluation_chain.invoke({
                        "question": question,
                        "context": doc.page_content[:1000]  # ê¸¸ì´ ì œí•œ
                    })
                    
                    # "ì í•©" ë˜ëŠ” "ë¶€ë¶„ì í•©"ì¸ ê²½ìš°ë§Œ í¬í•¨
                    if "ì í•©" in evaluation:
                        evaluated_docs.append(doc)
                    
                except Exception as e:
                    logger.warning(f"ê°œë³„ ë¬¸ì„œ í‰ê°€ ì‹¤íŒ¨: {e}")
                    # í‰ê°€ ì‹¤íŒ¨ì‹œì—ë„ ë¬¸ì„œ í¬í•¨ (ì•ˆì „ìž¥ì¹˜)
                    evaluated_docs.append(doc)
            
            # í‰ê°€ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë¬¸ì„œ ì¤‘ ì¼ë¶€ë¼ë„ ë°˜í™˜
            if not evaluated_docs and documents:
                evaluated_docs = documents[:3]
            
            return evaluated_docs
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return documents[:5]  # í´ë°±: ìƒìœ„ 5ê°œ ë°˜í™˜
    
    def _build_context(self, documents: List[Document]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not documents:
            return ""
        
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            source = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
            page = doc.metadata.get('page', '')
            
            # ì¶œì²˜ ì •ë³´ êµ¬ì„±
            source_info = f"[ì¶œì²˜: {source}"
            if page:
                source_info += f", íŽ˜ì´ì§€ {page}"
            source_info += "]"
            
            # ë¬¸ì„œ ë‚´ìš© (ê¸¸ì´ ì œí•œ)
            content = doc.page_content
            if len(content) > 800:
                content = content[:800] + "..."
            
            context_parts.append(f"{source_info}\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str) -> str:
        """ë‹µë³€ ìƒì„±"""
        try:
            if not context.strip():
                return "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            answer = self.qa_chain.invoke({
                "question": question,
                "context": context
            })
            
            return answer.strip()
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _verify_answer(self, question: str, context: str, answer: str) -> Tuple[str, bool]:
        """ë‹µë³€ ê²€ì¦"""
        try:
            # ê²€ì¦í•˜ì§€ ì•Šì„ ë‹µë³€ë“¤
            skip_verification = [
                "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
            ]
            
            if any(skip_text in answer for skip_text in skip_verification):
                return answer, True
            
            verification = self.answer_verification_chain.invoke({
                "question": question,
                "context": context,
                "answer": answer
            })
            
            # ê²€ì¦ ê²°ê³¼ íŒŒì‹±
            is_verified = "ê²€ì¦ë¨" in verification or "ì •í™•" in verification
            
            return answer, is_verified
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return answer, False  # ê²€ì¦ ì‹¤íŒ¨ì‹œ False ë°˜í™˜
    
    def _extract_sources(self, documents: List[Document]) -> List[str]:
        """ì¶œì²˜ ì •ë³´ ì¶”ì¶œ"""
        sources = []
        seen_sources = set()
        
        for doc in documents:
            source = doc.metadata.get('source', '')
            if source and source not in seen_sources:
                seen_sources.add(source)
                sources.append(source)
        
        return sources
    
    def _format_final_answer(self, answer: str, sources: List[str], 
                           confidence_score: float = 0.0) -> str:
        """ìµœì¢… ë‹µë³€ í¬ë§·íŒ…"""
        formatted_answer = answer
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        if sources:
            source_text = ", ".join(sources)
            formatted_answer += f"\n\nðŸ“„ ì¶œì²˜: {source_text}"
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ê°€ (ì˜µì…˜)
        if confidence_score > 0:
            formatted_answer += f"\nðŸŽ¯ ì‹ ë¢°ë„: {confidence_score:.1%}"
        
        return formatted_answer
    
    def query(self, question: str, use_cache: bool = True) -> str:
        """ë©”ì¸ ì§ˆì˜ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            logger.info(f"RAG ì§ˆì˜ ì‹œìž‘: '{question}'")
            
            # ìºì‹œ í™•ì¸
            cache_key = hashlib.md5(question.encode()).hexdigest()
            if use_cache and cache_key in self.query_cache:
                logger.info("ìºì‹œì—ì„œ ë‹µë³€ ë°˜í™˜")
                return self.query_cache[cache_key]
            
            # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ì„
            analysis = self._analyze_query(question)
            logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {analysis.get('ì§ˆë¬¸_ìœ í˜•', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            
            # 2ë‹¨ê³„: ë‹¤ì¤‘ ì „ëžµ ê²€ìƒ‰
            documents = self._search_with_multiple_strategies(question, analysis)
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            
            if not documents:
                answer = "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return self._format_final_answer(answer, [])
            
            # 3ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í‰ê°€
            relevant_docs = self._evaluate_context_relevance(question, documents)
            logger.info(f"ê´€ë ¨ì„± í‰ê°€ ì™„ë£Œ: {len(relevant_docs)}ê°œ ë¬¸ì„œ ì„ íƒ")
            
            # 4ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(relevant_docs)
            
            # 5ë‹¨ê³„: ë‹µë³€ ìƒì„±
            answer = self._generate_answer(question, context)
            
            # 6ë‹¨ê³„: ë‹µë³€ ê²€ì¦
            verified_answer, is_verified = self._verify_answer(question, context, answer)
            
            # 7ë‹¨ê³„: ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            sources = self._extract_sources(relevant_docs)
            
            # 8ë‹¨ê³„: ìµœì¢… ë‹µë³€ í¬ë§·íŒ…
            confidence = 0.8 if is_verified else 0.6
            final_answer = self._format_final_answer(verified_answer, sources, confidence)
            
            # ìºì‹œ ì €ìž¥
            if use_cache:
                self.query_cache[cache_key] = final_answer
                # ìºì‹œ í¬ê¸° ì œí•œ
                if len(self.query_cache) > 50:
                    oldest_key = next(iter(self.query_cache))
                    del self.query_cache[oldest_key]
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            elapsed_time = time.time() - start_time
            self.performance_stats['total_queries'] += 1
            self.performance_stats['total_time'] += elapsed_time
            self.performance_stats['verified_answers'] += (1 if is_verified else 0)
            
            logger.info(f"RAG ì§ˆì˜ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ, ê²€ì¦ë¨: {is_verified}")
            
            return final_answer
            
        except Exception as e:
            logger.error(f"RAG ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
        stats = dict(self.performance_stats)
        
        if stats.get('total_queries', 0) > 0:
            stats['avg_response_time'] = stats['total_time'] / stats['total_queries']
            stats['verification_rate'] = stats['verified_answers'] / stats['total_queries']
        
        return stats
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.query_cache.clear()
        logger.info("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def summarize_document(self, document: str) -> str:
        """ë¬¸ì„œ ìš”ì•½"""
        try:
            if len(document) < 100:
                return document  # ë„ˆë¬´ ì§§ì€ ë¬¸ì„œëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
            
            summary = self.summarization_chain.invoke({"document": document})
            return summary.strip()
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤." 