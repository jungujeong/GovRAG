# RAG ì‹œìŠ¤í…œ ë²„ê·¸ ìˆ˜ì • ê¸°ë¡ (2025-10-14)

## ê°œìš”

ì´ ë¬¸ì„œëŠ” 2025ë…„ 10ì›” 14ì¼ì— ìˆ˜í–‰í•œ RAG ì‹œìŠ¤í…œì˜ ì£¼ìš” ë²„ê·¸ ìˆ˜ì • ë° ê°œì„  ì‘ì—…ì„ ê¸°ë¡í•©ë‹ˆë‹¤.

---

## 1. Citation Tracking ë²„ê·¸ ìˆ˜ì • (í•µì‹¬ ì •í™•ë„ ê°œì„ )

### ë¬¸ì œ ìƒí™©
- **ì¦ìƒ**: LLMì´ ë‹µë³€ì— [1], [2] citationì„ ìƒì„±í–ˆì§€ë§Œ, ì‘ë‹µì˜ `response_sources`ì—ëŠ” 1ê°œë§Œ í‘œì‹œë¨
- **ì‚¬ìš©ì í”¼ë“œë°±**: "í™í‹°ì˜ˆìˆ ì´Œ" ì§ˆì˜ ì‹œ ë‹µë³€ì—ëŠ” [1][2] ìˆì§€ë§Œ sourcesëŠ” 1ê°œë§Œ ë‚˜ì˜´
- **ê·¼ë³¸ ì›ì¸**: `citation_tracker.py`ì˜ `_add_inline_citations_with_map_tracking` ë©”ì„œë“œê°€ LLMì´ ì´ë¯¸ ìƒì„±í•œ citationì„ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ì¶”ê°€í•˜ë ¤ê³  ì‹œë„í•¨

### í•´ê²° ë°©ë²•

#### íŒŒì¼: `/backend/rag/citation_tracker.py`

**ë³€ê²½ ì „**: citationì„ ìƒˆë¡œ ì¶”ê°€í•˜ë ¤ê³  ì‹œë„
```python
def _add_inline_citations_with_map_tracking(self, answer: str, evidences: List[Dict]):
    # ì˜ëª»ëœ ì ‘ê·¼: LLMì´ ì´ë¯¸ ìƒì„±í•œ [1], [2]ë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ì¶”ê°€í•˜ë ¤ í•¨
    ...
```

**ë³€ê²½ í›„**: LLMì´ ìƒì„±í•œ citationì„ íŒŒì‹±í•˜ì—¬ evidenceì™€ ë§¤í•‘ (lines 386-456)
```python
def _add_inline_citations_with_map_tracking(self, answer: str, evidences: List[Dict]) -> Tuple[str, Dict[str, int]]:
    """Add inline citations and track the mapping used

    CRITICAL FIX: Parse existing [1], [2] citations from LLM response FIRST,
    then match them to evidences to build cited_evidences map.
    """
    cited_evidences = {}

    # STEP 1: Extract ALL citation numbers that LLM already used
    citation_pattern = re.compile(r'\[(\d+)\]')
    all_cited_numbers = set()
    for match in citation_pattern.finditer(answer):
        cite_num = int(match.group(1))
        all_cited_numbers.add(cite_num)

    logger.info(f"ğŸ” Found {len(all_cited_numbers)} citation numbers in LLM response: {sorted(all_cited_numbers)}")

    # STEP 2: Match each citation number to its corresponding evidence
    for cite_num in sorted(all_cited_numbers):
        segments = []
        lines = answer.split('\n')
        for line in lines:
            if f'[{cite_num}]' in line:
                clean_line = re.sub(r'\[(\d+)\]', '', line).strip()
                if clean_line:
                    segments.append(clean_line)

        # Find best matching evidence for this citation number
        best_evidence = None
        best_score = 0.0
        for evidence in evidences:
            evidence_text = evidence.get("text", "")
            total_score = sum(self._calculate_content_similarity(segment, evidence_text) for segment in segments)
            avg_score = total_score / len(segments) if segments else 0.0
            if avg_score > best_score:
                best_score = avg_score
                best_evidence = evidence

        # Assign this evidence to this citation number
        if best_evidence and best_score > 0.2:
            evidence_key = f"{best_evidence.get('doc_id', '')}_{best_evidence.get('page', 0)}_{best_evidence.get('chunk_id', '')}"
            cited_evidences[evidence_key] = cite_num

    # STEP 3: Return answer as-is (LLM already added citations correctly)
    return answer, cited_evidences
```

**íš¨ê³¼**:
- âœ… LLMì´ ìƒì„±í•œ ëª¨ë“  citation ë²ˆí˜¸ë¥¼ ì •í™•íˆ ì¸ì‹
- âœ… ê° citationì„ í•´ë‹¹í•˜ëŠ” evidenceì™€ ì˜¬ë°”ë¥´ê²Œ ë§¤í•‘
- âœ… `response_sources` ê°œìˆ˜ê°€ ì‹¤ì œ citation ê°œìˆ˜ì™€ ì¼ì¹˜

---

## 2. íŠ¹ìˆ˜ ë¬¸ì ì œê±° (ê°€ë…ì„± ê°œì„ )

### ë¬¸ì œ ìƒí™©
- **ì¦ìƒ**: ë‹µë³€ê³¼ ì¶œì²˜ì— íŠ¹ìˆ˜ ë¬¸ì(ó°… ë“± Private Use Area Unicode) í‘œì‹œ
- **ì‚¬ìš©ì í”¼ë“œë°±**: "ë‹µë³€ì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸ ë“±ì´ í¬í•¨ë˜ê³  ê°€ë…ì„±ì´ ë–¨ì–´ì§"
- **ê·¼ë³¸ ì›ì¸**: PDF ë¬¸ì„œ ì¶”ì¶œ ì‹œ Private Use Area (U+E000-U+F8FF) ë¬¸ìê°€ í¬í•¨ë¨

### í•´ê²° ë°©ë²•

#### íŒŒì¼: `/backend/rag/citation_tracker.py`

**ì¶”ê°€í•œ ë©”ì„œë“œ** (lines 617-645):
```python
def _clean_text_snippet(self, text: str) -> str:
    """Clean text snippet by removing special characters and normalizing"""
    if not text:
        return ""

    # Remove private use area Unicode characters (U+E0000 to U+F8FF)
    cleaned = re.sub(r'[\uE000-\uF8FF]', '', text)

    # Remove other problematic Unicode categories
    import unicodedata
    cleaned = ''.join(
        char for char in cleaned
        if unicodedata.category(char) not in ('Cc', 'Cf', 'Cn')
        or char in ('\n', '\t', ' ')
    )

    # Normalize multiple spaces
    cleaned = re.sub(r' {2,}', ' ', cleaned)
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)

    return cleaned.strip()
```

#### íŒŒì¼: `/backend/rag/answer_formatter.py`

**ìˆ˜ì •í•œ ë©”ì„œë“œ** (lines 79-130):
```python
def _normalize_whitespace(self, text: str) -> str:
    """Normalize whitespace AND remove special characters for readability."""
    if not text:
        return text

    # STEP 1: Remove special characters (private use area Unicode, etc.)
    text = self._clean_special_characters(text)

    # STEP 2: Collapse multiple spaces/tabs per line
    lines = text.split('\n')
    normalized_lines = [re.sub(r'[ \t]+', ' ', line.strip()) for line in lines]

    # STEP 3: Remove excessive blank lines (max 2 consecutive)
    result_lines = []
    blank_count = 0
    for line in normalized_lines:
        if not line:
            blank_count += 1
            if blank_count <= 2:
                result_lines.append(line)
        else:
            blank_count = 0
            result_lines.append(line)

    return '\n'.join(result_lines).strip()

def _clean_special_characters(self, text: str) -> str:
    """Remove special characters that harm readability"""
    if not text:
        return ""

    # Remove private use area Unicode characters
    cleaned = re.sub(r'[\uE000-\uF8FF]', '', text)

    # Remove other problematic Unicode categories
    import unicodedata
    cleaned = ''.join(
        char for char in cleaned
        if unicodedata.category(char) not in ('Cc', 'Cf', 'Cn')
        or char in ('\n', '\t', ' ')
    )

    return cleaned
```

**íš¨ê³¼**:
- âœ… ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
- âœ… ì¶œì²˜ snippetì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
- âœ… ê°€ë…ì„± í–¥ìƒ

---

## 3. ì¸ë±ì‹± ë‹¨ê³„ í…ìŠ¤íŠ¸ ì •ì œ (ROOT CAUSE FIX)

### ë¬¸ì œ ìƒí™©
- **ì‚¬ìš©ì í”¼ë“œë°±**: "ìœ ë‹ˆì½”ë“œ ë“± ë¬¸ì„œì— ê·¸ëŒ€ë¡œ ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ì¸ìš©ë˜ëŠ” ê²ƒì´ ë¬¸ì œ ì•„ë‹ê¹Œìš”?"
- **ê·¼ë³¸ ì›ì¸**: ë¬¸ì„œ ì¸ë±ì‹± ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ì§€ ì•Šì•„ íŠ¹ìˆ˜ ë¬¸ìê°€ evidence ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë¨
- **ì´ì „ ì ‘ê·¼**: LLM ì¶œë ¥ í›„ í›„ì²˜ë¦¬ â†’ ë¹„íš¨ìœ¨ì ì´ê³  ë¶ˆì™„ì „

### í•´ê²° ë°©ë²•

#### íŒŒì¼: `/backend/processors/structure_chunker.py`

**ì¶”ê°€í•œ import** (line 5):
```python
import unicodedata
```

**ìˆ˜ì •í•œ ë©”ì„œë“œ** `_create_chunk` (lines 341-361):
```python
def _create_chunk(self, text: str, doc_id: str, section_or_page: int,
                 chunk_id: int, page: int, start_char: int, end_char: int) -> Dict:
    """Create a chunk dictionary

    CRITICAL: Clean text at indexing time to prevent problematic characters
    from entering the evidence database. This is the ROOT CAUSE fix.
    """
    # Clean text before storing in index (ROOT CAUSE FIX)
    cleaned_text = self._clean_text_for_indexing(text)

    return {
        "chunk_id": f"{doc_id}-chunk-{chunk_id}",
        "doc_id": doc_id,
        "section_or_page": section_or_page,
        "page": page,
        "text": cleaned_text,  # Store cleaned text
        "start_char": start_char,
        "end_char": end_char,
        "type": "content",
        "tokens": self._count_tokens(cleaned_text)
    }
```

**ì¶”ê°€í•œ ë©”ì„œë“œ** `_clean_text_for_indexing` (lines 363-407):
```python
def _clean_text_for_indexing(self, text: str) -> str:
    """Clean text at indexing time (ROOT CAUSE FIX)

    This prevents problematic characters from entering the evidence database.
    Better than post-processing LLM output.
    """
    if not text:
        return ""

    # Step 1: Remove Private Use Area Unicode (U+E000-U+F8FF)
    cleaned = re.sub(r'[\uE000-\uF8FF]', '', text)

    # Step 2: Remove problematic Unicode categories
    cleaned = ''.join(
        char for char in cleaned
        if unicodedata.category(char) not in ('Cc', 'Cf', 'Cn')
        or char in ('\n', '\t', ' ')
    )

    # Step 3: Normalize whitespace
    lines = cleaned.split('\n')
    normalized_lines = [re.sub(r'[ \t]+', ' ', line.strip()) for line in lines]

    # Remove excessive blank lines (max 2 consecutive)
    result_lines = []
    blank_count = 0
    for line in normalized_lines:
        if not line:
            blank_count += 1
            if blank_count <= 2:
                result_lines.append(line)
        else:
            blank_count = 0
            result_lines.append(line)

    return '\n'.join(result_lines).strip()
```

**ë˜í•œ ìˆ˜ì •**:
- `_format_directive_for_chunk` - ì²­í¬ ìƒì„± ì „ í…ìŠ¤íŠ¸ ì •ì œ
- `_create_table_chunk` - í‘œ í…ìŠ¤íŠ¸ ì •ì œ
- `_create_footnote_chunk` - ê°ì£¼ í…ìŠ¤íŠ¸ ì •ì œ

**ì¬ì¸ë±ì‹± ìˆ˜í–‰**:
```bash
python3 backend/main_indexer.py
```

**ê²°ê³¼**:
```
2025-10-14 19:16:14 - Indexing complete: 4 successful, 0 failed, 0 skipped, 0 no chunks
2025-10-14 19:16:16 - Whoosh index: 50 documents
```

**íš¨ê³¼**:
- âœ… ê·¼ë³¸ ì›ì¸ í•´ê²°: ì¸ë±ì‹± ë‹¨ê³„ì—ì„œ í…ìŠ¤íŠ¸ ì •ì œ
- âœ… ëª¨ë“  evidenceê°€ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
- âœ… LLM ì¶œë ¥ë„ ìë™ìœ¼ë¡œ ê¹¨ë—í•¨ (evidence ê¸°ë°˜)
- âœ… í›„ì²˜ë¦¬ ë¶€ë‹´ ê°ì†Œ

---

## 4. í”„ë¡ íŠ¸ì—”ë“œ ë¬´í•œ ë¡œë”© ë¬¸ì œ í•´ê²°

### ë¬¸ì œ ìƒí™©
- **ì¦ìƒ**: ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5173 ì ‘ì† ì‹œ ë¬´í•œ ë¡œë”© â†’ Error í˜ì´ì§€ (í°ìƒ‰ ë°°ê²½)
- **ë¸Œë¼ìš°ì € ì½˜ì†” ì˜¤ë¥˜**:
  ```
  GET http://localhost:5173/ 500 (Internal Server Error)
  GET http://localhost:5173/@vite/client 504 (Outdated Request)
  ```
- **ì‚¬ìš©ì í”¼ë“œë°±**: "ì´ëŸ¬í•œ ë¬¸ì œëŠ” ì§€ê¸ˆ ëª‡ë²ˆì§¸ ë°˜ë³µë˜ê³  ìˆìŠµë‹ˆë‹¤"

### ì§„ë‹¨ ê³¼ì •

#### Phase 1: Node.js ë²„ì „ ë¬¸ì œ ë°œê²¬
**ë¡œê·¸ ë¶„ì„** (`logs/frontend.log`):
```
[Failed to load PostCSS config: Failed to load PostCSS config (searchPath: /Users/yummongi/Desktop/claude_rag_gpt5/frontend): [Error] Loading PostCSS Plugin failed: Cannot find module 'dlv'
...
Node.js v22.20.0
```

**ë¬¸ì œ**:
- Node.js v22.20.0ì´ Vite 4.5.14ì™€ í˜¸í™˜ë˜ì§€ ì•ŠìŒ
- esbuild deadlock ë°œìƒ ê°€ëŠ¥ì„±

**í•´ê²°**:
1. `.nvmrc` íŒŒì¼ ìƒì„±:
   ```
   18
   ```

2. Node 18 LTSë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ:
   ```bash
   nvm use 18
   ```

#### Phase 2: node_modules ì†ìƒ ë°œê²¬
**ì‚¬ìš©ì í”¼ë“œë°±**: "node_modules íŒŒì¼ì„ ë³´ë©´ ì¤‘ë³µëœ íŒŒì¼ (ìˆ«ì 2ê°€ ë¶™ì–´ìˆìŒ) ë“¤ì´ ìˆìŠµë‹ˆë‹¤"

**ì§„ë‹¨**:
```bash
find frontend/node_modules -name "* 2" | head -20
```

**ê²°ê³¼**:
```
frontend/node_modules/vite 2
frontend/node_modules/thenify 2
frontend/node_modules/ansi-regex 2
frontend/node_modules/glob 2
frontend/node_modules/queue-microtask 2
... (20ê°œ ì´ìƒ)
```

**ë¬¸ì œ**: ì¤‘ë³µ ë””ë ‰í† ë¦¬ (ê³µë°± + "2" suffix)ë¡œ ì¸í•œ ëª¨ë“ˆ í•´ì„ ì‹¤íŒ¨

#### Phase 3: ëˆ„ë½ëœ íŒ¨í‚¤ì§€ ë°œê²¬
**ì˜¤ë¥˜**:
```
Error [ERR_MODULE_NOT_FOUND]: Cannot find package 'esbuild'
Cannot find module 'dlv'
```

**ì›ì¸**:
- `dlv`: PostCSS/Tailwind ì˜ì¡´ì„±
- `esbuild`: Vite ë²ˆë“¤ëŸ¬ í•„ìˆ˜ íŒ¨í‚¤ì§€

### í•´ê²° ë°©ë²•

#### 1. ì™„ì „ ì¬ì„¤ì¹˜
```bash
cd frontend
rm -rf node_modules package-lock.json
npm cache clean --force
```

#### 2. Node 18 ì‚¬ìš© ë° ì¬ì„¤ì¹˜
```bash
nvm use 18
npm install
```

#### 3. ëˆ„ë½ íŒ¨í‚¤ì§€ ëª…ì‹œì  ì„¤ì¹˜
```bash
npm install esbuild --save-dev
```

**ê²°ê³¼**: `package.json`ì— `esbuild` ì¶”ê°€ë¨:
```json
{
  "devDependencies": {
    "esbuild": "^0.25.10",
    ...
  }
}
```

#### 4. start.sh ìˆ˜ì •
**íŒŒì¼**: `/start.sh` (line 61)

**ë³€ê²½ ì „**:
```bash
(cd frontend && npm run dev) > logs/frontend.log 2>&1 &
```

**ë³€ê²½ í›„**:
```bash
(cd frontend && source ~/.nvm/nvm.sh && nvm use 18 && npm run dev) > logs/frontend.log 2>&1 &
```

**íš¨ê³¼**: ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ Node 18 ì‚¬ìš©

### ê²€ì¦

#### í”„ë¡ íŠ¸ì—”ë“œ ì •ìƒ ì‹œì‘:
```
VITE v4.5.14  ready in 9014 ms

âœ  Local:   http://localhost:5173/
âœ  Network: http://192.168.219.103:5173/
```

#### ë°±ì—”ë“œ ì •ìƒ ì—°ë™:
```
INFO: 127.0.0.1:63474 - "GET /api/health HTTP/1.1" 200 OK
INFO: 127.0.0.1:63480 - "GET /api/documents/list HTTP/1.1" 200 OK
INFO: services.session_manager:Created new session: a109447c-d252-4c4f-9442-47463ae5eaf7
```

**íš¨ê³¼**:
- âœ… í”„ë¡ íŠ¸ì—”ë“œ ì •ìƒ ë¡œë”©
- âœ… ë°±ì—”ë“œ API í†µì‹  ì •ìƒ
- âœ… ë¬¸ì„œ ëª©ë¡, ì„¸ì…˜ ê´€ë¦¬ ì •ìƒ
- âœ… vite.config.js ë¬´í•œ ì¬ì‹œì‘ ë£¨í”„ í•´ê²°

---

## 5. ê¸°íƒ€ ê°œì„ ì‚¬í•­

### Response Postprocessor ë¹„í™œì„±í™”
**íŒŒì¼**: `/backend/rag/answer_formatter.py`

**ë¬¸ì œ**: í›„ì²˜ë¦¬ê¸°ê°€ LLM ì¶œë ¥ì„ ê³¼ë„í•˜ê²Œ ìˆ˜ì •í•˜ì—¬ ê¹¨ì§

**í•´ê²°**: ë¹„í™œì„±í™” (line ì„¤ì •)
```python
self.enabled = False
```

### Topic Change Detection ì„ê³„ê°’ ì¡°ì •
**íŒŒì¼**: `/backend/rag/retrieval/two_stage_retrieval.py`

**ë³€ê²½**:
```python
# Before: TOPIC_CHANGE_THRESHOLD = 0.10
# After:
TOPIC_CHANGE_THRESHOLD = 0.03
```

**íš¨ê³¼**: í›„ì† ì§ˆë¬¸ì—ì„œ topic changeë¥¼ ë” ì •í™•íˆ ê°ì§€

---

## ê²€ì¦ ë° í…ŒìŠ¤íŠ¸

### ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
```bash
curl http://localhost:8000/api/health
```

**ì‘ë‹µ**:
```json
{
  "status": "healthy",
  "components": {
    "ollama": true,
    "whoosh": true,
    "chroma": true
  }
}
```

### ì¸ë±ìŠ¤ ìƒíƒœ
- **ë¬¸ì„œ**: 4ê°œ (êµ¬ì²­ì¥ ì§€ì‹œì‚¬í•­ PDF)
- **ì²­í¬**: 50ê°œ (ì •ì œëœ í…ìŠ¤íŠ¸)
- **Whoosh BM25**: ì •ìƒ
- **ChromaDB Vector**: ì •ìƒ

### í”„ë¡ íŠ¸ì—”ë“œ ì ‘ì†
- **URL**: http://localhost:5173
- **ìƒíƒœ**: ì •ìƒ ë¡œë”©
- **React**: Hot Module Replacement (HMR) ì •ìƒ
- **API í†µì‹ **: ì •ìƒ

---

## ë‹¤ìŒ í…ŒìŠ¤íŠ¸ í•­ëª©

### 1. Citation Tracking ê²€ì¦
**ì¿¼ë¦¬**: "í™í‹°ì˜ˆìˆ ì´Œì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"

**í™•ì¸ ì‚¬í•­**:
- [ ] ë‹µë³€ì— [1], [2] citation í‘œì‹œ
- [ ] `response_sources` ê°œìˆ˜ê°€ citation ê°œìˆ˜ì™€ ì¼ì¹˜
- [ ] ê° sourceì˜ ë¬¸ì„œID, í˜ì´ì§€, snippet ì •í™•ì„±

### 2. íŠ¹ìˆ˜ ë¬¸ì ì œê±° ê²€ì¦
**í™•ì¸ ì‚¬í•­**:
- [ ] ë‹µë³€ í…ìŠ¤íŠ¸ì— ó°… ê°™ì€ íŠ¹ìˆ˜ ë¬¸ì ì—†ìŒ
- [ ] source snippetì— íŠ¹ìˆ˜ ë¬¸ì ì—†ìŒ
- [ ] ê°€ë…ì„± í–¥ìƒ í™•ì¸

### 3. í›„ì† ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
**ì¿¼ë¦¬ ì‹œí€€ìŠ¤**:
1. "í™í‹°ì˜ˆìˆ ì´Œì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
2. "ì–¸ì œ ì§€ì‹œë˜ì—ˆë‚˜ìš”?"

**í™•ì¸ ì‚¬í•­**:
- [ ] ê³ ì •ëœ citation map ìœ ì§€
- [ ] ë‘ ë²ˆì§¸ ì§ˆë¬¸ë„ ì˜¬ë°”ë¥¸ sources ë°˜í™˜
- [ ] citation ë²ˆí˜¸ ì¼ê´€ì„±

### 4. Topic Change í…ŒìŠ¤íŠ¸
**ì¿¼ë¦¬ ì‹œí€€ìŠ¤**:
1. "í™í‹°ì˜ˆìˆ ì´Œì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
2. "ê±´ê°•ë³´í—˜ ê´€ë ¨ ì§€ì‹œì‚¬í•­ì€?"

**í™•ì¸ ì‚¬í•­**:
- [ ] Topic change ê°ì§€ë¨
- [ ] ìƒˆë¡œìš´ contextë¡œ ê²€ìƒ‰ ìˆ˜í–‰
- [ ] ì˜¬ë°”ë¥¸ ë¬¸ì„œì—ì„œ ì‘ë‹µ ìƒì„±

---

## íŒŒì¼ ë³€ê²½ ìš”ì•½

### ìˆ˜ì •ëœ íŒŒì¼
1. `/backend/rag/citation_tracker.py` - Citation íŒŒì‹± ë¡œì§ ì¬ì‘ì„±
2. `/backend/rag/answer_formatter.py` - íŠ¹ìˆ˜ ë¬¸ì ì œê±° ì¶”ê°€
3. `/backend/processors/structure_chunker.py` - ì¸ë±ì‹± ë‹¨ê³„ í…ìŠ¤íŠ¸ ì •ì œ
4. `/start.sh` - Node 18 ìë™ ì‚¬ìš©
5. `/frontend/.nvmrc` - Node ë²„ì „ ê³ ì •
6. `/frontend/package.json` - esbuild ëª…ì‹œì  ì¶”ê°€

### ì¬ì¸ë±ì‹±ëœ ë°ì´í„°
- `/data/index/` - Whoosh BM25 ì¸ë±ìŠ¤ (ì •ì œëœ í…ìŠ¤íŠ¸)
- `/data/chroma/` - ChromaDB ë²¡í„° ìŠ¤í† ì–´ (ì •ì œëœ í…ìŠ¤íŠ¸)

---

## êµí›ˆ ë° Best Practices

### 1. í…ìŠ¤íŠ¸ ì •ì œëŠ” ì¸ë±ì‹± ë‹¨ê³„ì—ì„œ
- âŒ LLM ì¶œë ¥ í›„ í›„ì²˜ë¦¬
- âœ… ë¬¸ì„œ ì¸ë±ì‹± ì‹œ ì •ì œ

**ì´ìœ **:
- Evidence ë°ì´í„°ë² ì´ìŠ¤ì— ê¹¨ë—í•œ í…ìŠ¤íŠ¸ ì €ì¥
- LLMì´ ê¹¨ë—í•œ evidence ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
- í›„ì²˜ë¦¬ ë¶€ë‹´ ê°ì†Œ

### 2. LLM ì¶œë ¥ ì¡´ì¤‘
- âŒ LLMì´ ìƒì„±í•œ citationì„ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ì¶”ê°€
- âœ… LLM ì¶œë ¥ì„ íŒŒì‹±í•˜ì—¬ í™œìš©

**ì´ìœ **:
- LLMì´ ì´ë¯¸ ìµœì í™”ëœ citation ë°°ì¹˜
- í›„ì²˜ë¦¬ë¡œ ì¸í•œ ë¶€ì‘ìš© ë°©ì§€

### 3. Node.js ë²„ì „ ê´€ë¦¬
- âŒ ì‹œìŠ¤í…œ ê¸°ë³¸ Node ì‚¬ìš©
- âœ… `.nvmrc`ë¡œ í”„ë¡œì íŠ¸ë³„ ë²„ì „ ê³ ì •

**ì´ìœ **:
- ë¹Œë“œ ë„êµ¬ í˜¸í™˜ì„± ë³´ì¥
- íŒ€ì› ê°„ ì¼ê´€ì„± ìœ ì§€

### 4. ì˜ì¡´ì„± ëª…ì‹œ
- âŒ ìë™ ì„¤ì¹˜ì—ë§Œ ì˜ì¡´
- âœ… í•„ìˆ˜ íŒ¨í‚¤ì§€ëŠ” `package.json`ì— ëª…ì‹œ

**ì˜ˆ**: `esbuild`, `dlv` ë“±

---

## ì°¸ê³  ìë£Œ

### Git Commits
- `68e5ad3` - improve document summary generation reliability and performance
- `9bac1f0` - resolve frontend infinite loading and improve startup reliability

### ë¡œê·¸ íŒŒì¼
- `/logs/backend.log` - ë°±ì—”ë“œ ì´ˆê¸°í™” ë° API ìš”ì²­
- `/logs/frontend.log` - Vite ì‹œì‘ ë° HMR

### ê´€ë ¨ ì´ìŠˆ
- Frontend infinite loading (ë°˜ë³µ ë°œìƒ)
- Citation mismatch (sources count != citation count)
- Special characters in answer (ê°€ë…ì„± ì €í•˜)

---

**ì‘ì„±ì¼**: 2025-10-14
**ì‘ì„±ì**: Claude Code
**ì‹œìŠ¤í…œ ë²„ì „**: Python 3.12.2, Node.js 18.20.8, Vite 4.5.14
