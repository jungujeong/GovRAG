from fastapi import APIRouter, HTTPException, WebSocket, WebSocketDisconnect, Depends, Query, Request
from fastapi.responses import StreamingResponse
from typing import Dict, List, Optional, Any, Tuple
import asyncio
import json
import logging
from datetime import datetime
from starlette.requests import ClientDisconnect
import re

from schemas import QueryRequest, QueryResponse
from models.session import ChatSession, Message
from services.session_manager import session_manager
from rag.evidence_enforcer import EvidenceEnforcer
from rag.citation_tracker import CitationTracker
from rag.answer_formatter import AnswerFormatter
from rag.response_postprocessor import ResponsePostProcessor
from rag.response_grounder import ResponseGrounder
from rag.conversation_summarizer import ConversationSummarizer
from rag.query_rewriter import QueryRewriter, RewriteContext
from rag.topic_detector import TopicChangeDetector
from rag.doc_scope_resolver import DocScopeResolver, DocScopeResolution
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from rag.hybrid_retriever import HybridRetriever
    from rag.reranker import Reranker
    from rag.generator_ollama import OllamaGenerator
from config import config
from utils.error_handler import ErrorHandler
from utils.rate_limiter import RateLimiter
from services.title_generator import TitleGenerator

logger = logging.getLogger(__name__)

router = APIRouter()

# Component initialization (lazy to make testing lightweight)
retriever: Optional[Any] = None
reranker: Optional[Any] = None
generator: Optional[Any] = None
enforcer = EvidenceEnforcer()
citation_tracker = CitationTracker()
formatter = AnswerFormatter()
postprocessor = ResponsePostProcessor()
error_handler = ErrorHandler()
rate_limiter = RateLimiter(max_requests=30, window_seconds=60)
summarizer: Optional[ConversationSummarizer] = None
query_rewriter: Optional[QueryRewriter] = None
title_generator: Optional[TitleGenerator] = None
topic_detector: Optional[TopicChangeDetector] = None
grounder: Optional[ResponseGrounder] = None
doc_scope_resolver: Optional[DocScopeResolver] = None


def get_retriever():
    global retriever
    if retriever is None:
        from rag.hybrid_retriever import HybridRetriever
        retriever = HybridRetriever()
    return retriever


def get_reranker():
    global reranker
    if reranker is None:
        from rag.reranker import Reranker
        reranker = Reranker()
    return reranker


def get_generator():
    global generator
    if generator is None:
        from rag.generator_ollama import OllamaGenerator
        generator = OllamaGenerator()
    return generator


def get_summarizer() -> ConversationSummarizer:
    global summarizer
    if summarizer is None:
        summarizer = ConversationSummarizer()
    return summarizer


def get_query_rewriter() -> QueryRewriter:
    global query_rewriter
    if query_rewriter is None:
        query_rewriter = QueryRewriter()
    return query_rewriter


def get_topic_detector() -> TopicChangeDetector:
    global topic_detector
    if topic_detector is None:
        topic_detector = TopicChangeDetector(
            similarity_threshold=config.TOPIC_SIMILARITY_THRESHOLD,
            retrieval_confidence_threshold=config.TOPIC_CONFIDENCE_THRESHOLD,
            min_score_threshold=config.TOPIC_MIN_SCORE_THRESHOLD
        )
    return topic_detector


def get_response_grounder() -> ResponseGrounder:
    global grounder
    if grounder is None:
        grounder = ResponseGrounder()
    return grounder


def get_doc_scope_resolver() -> DocScopeResolver:
    global doc_scope_resolver
    if doc_scope_resolver is None:
        doc_scope_resolver = DocScopeResolver(get_topic_detector())
    return doc_scope_resolver


def _deduplicate_doc_ids(doc_ids: Optional[List[str]]) -> List[str]:
    if not doc_ids:
        return []
    seen: set = set()
    ordered: List[str] = []
    for doc_id in doc_ids:
        if not doc_id or doc_id in seen:
            continue
        seen.add(doc_id)
        ordered.append(doc_id)
    return ordered


def _extract_unique_doc_ids(evidences: List[Dict[str, Any]]) -> List[str]:
    return _deduplicate_doc_ids([e.get("doc_id") for e in evidences])


def _average_evidence_score(evidences: List[Dict[str, Any]]) -> Optional[float]:
    scores: List[float] = []
    for evidence in evidences:
        for key in ("score", "similarity", "relevance", "hybrid_score", "normalized_score"):
            value = evidence.get(key)
            if isinstance(value, (int, float)):
                score = float(value)
                if score > 1.0 and key in ("score", "similarity", "relevance"):
                    score = score / 100.0
                scores.append(score)
                break
    if not scores:
        return None
    return sum(scores) / len(scores)


def _collect_memory_facts(response: Dict) -> List[Dict[str, str]]:
    """Extract memory snippets from formatted response by citation index."""
    if not response:
        return []

    pattern = re.compile(r'\[(\d+)\]')
    sources = response.get("sources", []) or []

    number_to_doc: Dict[int, Optional[str]] = {}
    for idx, source in enumerate(sources, 1):
        doc_id = source.get("doc_id")
        display = source.get("display_index") or idx
        if doc_id:
            number_to_doc[int(display)] = doc_id

    texts: List[str] = []
    if response.get("answer"):
        texts.extend(response["answer"].split('\n'))
    for fact in response.get("key_facts", []) or []:
        texts.append(fact)
    if response.get("details"):
        texts.extend(response["details"].split('\n'))

    memory: List[Dict[str, str]] = []
    seen: set = set()

    for line in texts:
        stripped = line.strip()
        if not stripped:
            continue
        doc_ids = {
            number_to_doc.get(int(num))
            for num in pattern.findall(stripped)
            if number_to_doc.get(int(num))
        }
        for doc_id in doc_ids:
            key = (doc_id, stripped)
            if key in seen:
                continue
            memory.append({"doc_id": doc_id, "text": stripped})
            seen.add(key)

    return memory


def _build_memory_evidences(session: ChatSession, doc_scope: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    """Convert stored memory facts into pseudo-evidences."""
    facts = getattr(session, "memory_facts", []) or []
    evidences: List[Dict[str, Any]] = []
    allowed = set(doc_scope) if doc_scope else None

    for idx, fact in enumerate(facts):
        doc_id = fact.get("doc_id")
        text = fact.get("text")
        if not doc_id or not text:
            continue
        if allowed is not None and doc_id not in allowed:
            continue
        evidences.append({
            "text": text,
            "doc_id": doc_id,
            "page": 0,
            "score": 1.5,
            "chunk_id": f"memory-{doc_id}-{idx}",
            "source": "memory"
        })

    return evidences


def _finalize_doc_scope_metadata(
    base: Optional[Dict[str, Any]],
    *,
    mode: str,
    doc_scope_ids: List[str],
    resolved_doc_ids: List[str],
    requested_doc_ids: List[str],
    diagnostics: Optional[Dict[str, Any]] = None,
    average_score: Optional[float] = None,
    topic_change: bool = False,
    topic_reason: Optional[str] = None,
    topic_suggested: Optional[List[str]] = None,
) -> Dict[str, Any]:
    metadata = dict(base or {})
    metadata["mode"] = mode
    metadata["doc_scope"] = mode
    metadata["doc_scope_ids"] = doc_scope_ids
    metadata["resolved_doc_ids"] = resolved_doc_ids
    metadata["requested_doc_ids"] = requested_doc_ids

    if diagnostics:
        metadata["diagnostics"] = diagnostics
    if average_score is not None:
        metadata["average_score"] = average_score
    if topic_change:
        metadata["topic_change_detected"] = True
        if topic_reason:
            metadata["topic_change_reason"] = topic_reason
        if topic_suggested:
            metadata["suggested_doc_ids"] = topic_suggested

    return metadata


def _resolve_evidences(
    *,
    query: str,
    retrieval_query: str,
    retriever_instance,
    requested_doc_ids: List[str],
    session_doc_ids: List[str],
    previous_doc_ids: List[str],
    should_use_previous_sources: bool,
) -> DocScopeResolution:
    resolver = get_doc_scope_resolver()
    return resolver.resolve(
        query=query,
        retrieval_query=retrieval_query,
        retriever=retriever_instance,
        requested_doc_ids=requested_doc_ids,
        session_doc_ids=session_doc_ids,
        previous_doc_ids=previous_doc_ids,
        should_use_previous_sources=should_use_previous_sources,
        topk=config.TOPK_BM25 + config.TOPK_VECTOR,
        allow_topic_expansion=True,
    )


def get_title_generator() -> TitleGenerator:
    global title_generator
    if title_generator is None:
        title_generator = TitleGenerator()
    return title_generator

# WebSocket connections management
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.session_locks: Dict[str, asyncio.Lock] = {}
    
    async def connect(self, websocket: WebSocket, session_id: str):
        await websocket.accept()
        self.active_connections[session_id] = websocket
        if session_id not in self.session_locks:
            self.session_locks[session_id] = asyncio.Lock()
        logger.info(f"WebSocket connected for session {session_id}")
    
    def disconnect(self, session_id: str):
        if session_id in self.active_connections:
            del self.active_connections[session_id]
        logger.info(f"WebSocket disconnected for session {session_id}")
    
    async def send_message(self, session_id: str, message: dict):
        if session_id in self.active_connections:
            try:
                await self.active_connections[session_id].send_json(message)
            except Exception as e:
                logger.error(f"Failed to send WebSocket message: {e}")
    
    async def broadcast_to_session(self, session_id: str, message: dict):
        await self.send_message(session_id, message)

manager = ConnectionManager()

# Session endpoints
from pydantic import BaseModel

class CreateSessionRequest(BaseModel):
    title: Optional[str] = None
    document_ids: Optional[List[str]] = None

@router.post("/sessions")
async def create_session(request: CreateSessionRequest) -> Dict:
    """ìƒˆ ì±„íŒ… ì„¸ì…˜ ìƒì„±"""
    try:
        session = await session_manager.create_session(request.title, _normalize_doc_ids(request.document_ids))
        return {
            "success": True,
            "session": session.to_dict()
        }
    except Exception as e:
        logger.error(f"Failed to create session: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

@router.get("/sessions")
async def list_sessions(
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100)
) -> Dict:
    """ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        session_list = await session_manager.list_sessions(page, page_size)
        return {
            "success": True,
            "sessions": session_list.sessions,
            "total": session_list.total,
            "page": session_list.page,
            "page_size": session_list.page_size
        }
    except Exception as e:
        logger.error(f"Failed to list sessions: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ëª©ë¡ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

@router.get("/sessions/{session_id}")
async def get_session(session_id: str) -> Dict:
    """íŠ¹ì • ì„¸ì…˜ ì¡°íšŒ"""
    try:
        session = await session_manager.get_session(session_id)
        if not session:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "session": session.to_dict()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get session {session_id}: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

class UpdateSessionRequest(BaseModel):
    title: Optional[str] = None
    document_ids: Optional[List[str]] = None

@router.put("/sessions/{session_id}")
async def update_session(
    session_id: str,
    request: UpdateSessionRequest
) -> Dict:
    """ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸"""
    try:
        session = await session_manager.update_session(session_id, request.title, _normalize_doc_ids(request.document_ids))
        if not session:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "session": session.to_dict()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update session {session_id}: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ì—…ë°ì´íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

@router.delete("/sessions/{session_id}")
async def delete_session(session_id: str) -> Dict:
    """ì„¸ì…˜ ì‚­ì œ"""
    try:
        success = await session_manager.delete_session(session_id)
        if not success:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "message": "ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete session {session_id}: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

# Chat endpoints
@router.post("/sessions/{session_id}/messages")
async def send_message(
    session_id: str,
    request: QueryRequest,
    http_request: Request
) -> QueryResponse:
    """ì„¸ì…˜ì— ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ìƒì„±"""
    
    # ì—°ê²° ìƒíƒœ ì²´í¬ (í•­ìƒ í™œì„±í™”)
    cancelled = False
    cancel_event = asyncio.Event()

    async def check_client_disconnect():
        nonlocal cancelled
        try:
            while not cancelled:
                try:
                    if await http_request.is_disconnected():
                        cancelled = True
                        cancel_event.set()
                        logger.info(f"Client disconnected for session {session_id}")
                        # ì¤‘ë‹¨ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì €ì¥ (ì¤‘ë³µ ì €ì¥ ë°©ì§€: ìµœê·¼ ë©”ì‹œì§€ í™•ì¸ì€ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì²˜ë¦¬)
                        await session_manager.add_message(
                            session_id,
                            "assistant",
                            "ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆê±°ë‚˜ ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                            metadata={"interrupted": True, "reason": "client_disconnect"}
                        )
                        break
                except Exception as e:
                    logger.debug(f"Error checking disconnect: {e}")
                    break
                await asyncio.sleep(0.5)
        except asyncio.CancelledError:
            # ì •ìƒ ì·¨ì†Œ
            pass

    disconnect_task = asyncio.create_task(check_client_disconnect())
    
    try:
        # Rate limiting
        if not await rate_limiter.check_limit(session_id):
            raise HTTPException(status_code=429, detail="ë„ˆë¬´ ë§ì€ ìš”ì²­ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”")
        
        # Session validation
        session = await session_manager.get_session(session_id)
        if not session:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # Input validation
        if not request.query or not request.query.strip():
            # ì§ˆë¬¸ì´ ì…ë ¥ë˜ì§€ ì•Šì€ ê²½ìš° - ì¶œì²˜ ì—†ì´ ê°„ë‹¨í•œ ì•ˆë‚´ ë©”ì‹œì§€ë§Œ ë°˜í™˜
            await session_manager.add_message(
                session_id,
                "assistant",
                "ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì‹œë©´ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                sources=[]  # ì¶œì²˜ ì—†ìŒ
            )
            return QueryResponse(
                query="",
                answer="ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì‹œë©´ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                key_facts=[],
                sources=[],  # ì¶œì²˜ ì—†ìŒ
                session_id=session_id
            )
        
        if len(request.query) > 2000:
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ì§§ê²Œ ë‚˜ëˆ„ì–´ ë³´ë‚´ì£¼ì„¸ìš”")
        
        # Check if documents are uploaded
        if not session.document_ids and not getattr(request, "skip_document_check", False):
            raise HTTPException(
                status_code=400,
                detail="ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”",
                headers={"X-Error-Type": "NO_DOCUMENTS"}
            )
        
        # Add user message
        await session_manager.add_message(session_id, "user", request.query)

        previous_summary = session.conversation_summary or ""

        # Get conversation context
        context = await session_manager.get_session_context(session_id, max_messages=10)

        # ì´ì „ ë‹µë³€ì˜ ì¶œì²˜ ì¶”ì  (ì²« ë²ˆì§¸ assistant ë©”ì‹œì§€ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •)
        previous_sources: List[Dict[str, Any]] = []
        previous_doc_ids: List[str] = []
        first_assistant_found = False

        # reset_context í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        if request.reset_context:
            session.first_response_evidences = None
            session.first_response_citation_map = None
            logger.info(f"ğŸ”„ Context reset requested - clearing first response data")
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await session_manager.update_session(
                session_id,
                first_response_evidences=None,
                first_response_citation_map=None,
                metadata={
                    **((session.metadata or {})),
                    "context_reset_at": datetime.now().isoformat()
                }
            )
            first_assistant_found = False
            previous_doc_ids = []
        # ì²« ë‹µë³€ì˜ ê³ ì •ëœ evidencesì—ì„œ ë¬¸ì„œ ë²”ìœ„ë§Œ ì¶”ì¶œ (evidence ìì²´ëŠ” ì¬ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        elif session.first_response_evidences:
            first_assistant_found = True
            # Extract doc_ids from stored evidences (ë¬¸ì„œ ë²”ìœ„ë§Œ ì‚¬ìš©)
            previous_doc_ids = list(set([
                e.get("doc_id")
                for e in session.first_response_evidences
                if e.get("doc_id")
            ]))
            logger.info(f"Using doc scope from first response: docs: {previous_doc_ids} (will search for new evidences)")
        else:
            # ì²« ë²ˆì§¸ assistant ë©”ì‹œì§€ì˜ ì¶œì²˜ë¥¼ ì°¾ì•„ì„œ ê³ ì •
            for message in session.messages:
                if message.role == "assistant" and message.sources and not first_assistant_found:
                    previous_sources = message.sources
                    previous_doc_ids = [
                        src.get("doc_id")
                        for src in previous_sources
                        if isinstance(src, dict) and src.get("doc_id")
                    ]
                    first_assistant_found = True
                    logger.info(f"Using first assistant message sources: {previous_doc_ids}")
                    break

        # ì²« ë²ˆì§¸ assistantê°€ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ ê²ƒì„ ì‚¬ìš© (í´ë°±)
        if not first_assistant_found:
            for message in reversed(session.messages):
                if message.role == "assistant" and message.sources:
                    previous_sources = message.sources
                    previous_doc_ids = [
                        src.get("doc_id")
                        for src in previous_sources
                        if isinstance(src, dict) and src.get("doc_id")
                    ]
                    logger.info(f"Using recent assistant message sources (fallback): {previous_doc_ids}")
                    break

        # Rewrite query using available memory layers
        recent_for_rewrite = context[-4:] if context else []
        rewrite_context = RewriteContext(
            current_query=request.query,
            recent_messages=recent_for_rewrite,
            summary=previous_summary,
            entities=session.recent_entities or [],
            previous_sources=previous_sources
        )
        rewrite_result = get_query_rewriter().rewrite(rewrite_context)
        retrieval_query = request.query if rewrite_result.used_fallback else rewrite_result.search_query

        # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì§€ì •í–ˆëŠ”ì§€ í™•ì¸
        requested_doc_ids = []
        if request.doc_ids:
            requested_doc_ids = [doc_id for doc_id in request.doc_ids if doc_id in session.document_ids]
            if not requested_doc_ids:
                raise HTTPException(status_code=400, detail="ìš”ì²­í•œ ë¬¸ì„œë¥¼ ì„¸ì…˜ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê¸¸ ì›í•˜ëŠ” ê²½ìš° (reset_context=true)
        if request.reset_context:
            # ì„¸ì…˜ ì´ˆê¸°í™”
            session.first_response_evidences = None
            session.first_response_citation_map = None
            session.recent_source_doc_ids = []
            await session_manager.update_session(
                session_id,
                first_response_evidences=None,
                first_response_citation_map=None,
                recent_source_doc_ids=[]
            )
            logger.info("Context reset requested - clearing previous sources")
            previous_doc_ids = []

        # ì´ì „ ë‹µë³€ì˜ sourcesë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •
        should_use_previous_sources = bool(previous_doc_ids) and not requested_doc_ids and not request.reset_context
        topic_change_detected = False
        topic_change_reason: Optional[str] = None
        topic_change_suggested: List[str] = []
        doc_scope_metadata: Dict[str, Any] = {}
        allowed_docs_enforce: Optional[List[str]] = None
        doc_scope_ids: List[str] = []

        # Process query with RAG
        try:
            logger.info(f"Retrieving for query: {request.query}")
            logger.info(f"Session document IDs: {session.document_ids}")

            retriever_instance = get_retriever()
            resolution = _resolve_evidences(
                query=request.query,
                retrieval_query=retrieval_query,
                retriever_instance=retriever_instance,
                requested_doc_ids=requested_doc_ids,
                session_doc_ids=session.document_ids or [],
                previous_doc_ids=previous_doc_ids,
                should_use_previous_sources=should_use_previous_sources,
            )

            if resolution.status == "no_evidence":
                response_text = resolution.error_message or "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                await session_manager.add_message(
                    session_id,
                    "assistant",
                    response_text,
                    error="no_evidence",
                    metadata={"doc_scope": resolution.doc_scope_ids},
                )
                return QueryResponse(
                    query=request.query,
                    answer=response_text,
                    key_facts=[],
                    sources=[],
                    error="no_evidence",
                    session_id=session_id,
                    metadata={"doc_scope": resolution.metadata},
                )

            evidences = resolution.evidences
            doc_scope_metadata = dict(resolution.metadata or {})
            allowed_docs_enforce = resolution.allowed_doc_ids
            topic_change_detected = resolution.topic_change_detected
            topic_change_reason = resolution.topic_change_reason
            topic_change_suggested = resolution.topic_change_suggested or []
            should_use_previous_sources = resolution.allow_fixed_citations
            doc_scope_ids = resolution.doc_scope_ids
            doc_scope_mode = resolution.doc_scope_mode
            doc_scope_mode = resolution.doc_scope_mode
            doc_scope_mode = resolution.doc_scope_mode

            logger.info(
                "Resolved evidence scope: mode=%s, docs=%s, evidences=%d",
                resolution.doc_scope_mode,
                doc_scope_ids,
                len(evidences),
            )

            if resolution.diagnostics:
                logger.debug("Doc scope diagnostics: %s", resolution.diagnostics)


            # ë©”ëª¨ë¦¬ ê¸°ëŠ¥ ë¹„í™œì„±í™” - ì¶œì²˜ ì¼ê´€ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´
            # memory_scope = None
            # if requested_doc_ids:
            #     memory_scope = requested_doc_ids
            # elif should_use_previous_sources and previous_doc_ids:
            #     memory_scope = previous_doc_ids

            # memory_evidences = _build_memory_evidences(session, memory_scope)
            # if memory_evidences:
            #     evidences = memory_evidences + evidences
            #     logger.info(f"Prepended {len(memory_evidences)} memory evidences")

            # ë©”ëª¨ë¦¬ ê¸°ëŠ¥ ì„ì‹œ ë¹„í™œì„±í™”
            logger.info("Memory evidences disabled for source consistency")

            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠê²¼ë‹¤ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
            if cancel_event.is_set():
                raise HTTPException(status_code=499, detail="Client closed request")

            if not evidences:
                response_text = "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                await session_manager.add_message(
                    session_id, 
                    "assistant", 
                    response_text,
                    error="no_evidence"
                )
                return QueryResponse(
                    query=request.query,
                    answer=response_text,
                    key_facts=[],
                    sources=[],
                    error="no_evidence",
                    session_id=session_id
                )
            
            # 2. Rerank if available (í›„ì† ì§ˆë¬¸ì—ì„œë„ í•­ìƒ reranking ìˆ˜í–‰)
            # ì²« ë‹µë³€ evidences ì¬ì‚¬ìš© ë¡œì§ ì œê±° - í•­ìƒ ìƒˆë¡œ ê²€ìƒ‰í•œ evidenceë¥¼ rerank
            reranker_instance = get_reranker()
            if reranker_instance and (
                reranker_instance.model or (
                    reranker_instance.use_onnx and hasattr(reranker_instance, 'ort_session')
                )
            ):
                evidences = reranker_instance.rerank(
                    retrieval_query,
                    evidences,
                    top_k=config.TOPK_RERANK
                )
                logger.info(f"Reranked {len(evidences)} evidences for query")
            else:
                evidences = evidences[:config.TOPK_RERANK]

            # 2.5 ë¦¬ë­í‚¹ í›„ í›„ì† ì§ˆë¬¸ì¸ ê²½ìš° ë¬¸ì„œ ë²”ìœ„ í•„í„°ë§
            if should_use_previous_sources and previous_doc_ids:
                filtered_evidences = []
                for e in evidences:
                    if e.get("doc_id") in previous_doc_ids:
                        filtered_evidences.append(e)
                    else:
                        logger.warning(f"Post-rerank filtering out evidence from unexpected doc: {e.get('doc_id')}")
                evidences = filtered_evidences
                logger.info(f"Post-rerank filter: {len(evidences)} evidences from {previous_doc_ids}")

            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠê²¼ë‹¤ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
            if cancel_event.is_set():
                raise HTTPException(status_code=499, detail="Client closed request")

            # 2.6 ìƒì„± ì „ ìµœì¢… í•„í„°ë§ - í›„ì† ì§ˆë¬¸ì¸ ê²½ìš°
            if should_use_previous_sources and previous_doc_ids:
                # evidencesë¥¼ ë‹¤ì‹œ í•œë²ˆ í™•ì¸
                final_evidences = []
                for e in evidences:
                    doc_id = e.get("doc_id")
                    if doc_id in previous_doc_ids:
                        final_evidences.append(e)
                    else:
                        logger.error(f"Evidence from unexpected document {doc_id} detected and removed")
                evidences = final_evidences
                logger.info(f"Final pre-generation filter: {len(evidences)} evidences from allowed documents")

            # 3. Generate with context (ì·¨ì†Œ ê°€ëŠ¥ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰)
            gen_task = asyncio.create_task(
                get_generator().generate_with_context(
                    request.query,
                    evidences,
                    context=context,
                    doc_scope=doc_scope_metadata,
                    stream=False
                )
            )

            # cancel_event ì™€ ê²½í•©
            cancel_wait = asyncio.create_task(cancel_event.wait())
            done, pending = await asyncio.wait({gen_task, cancel_wait}, return_when=asyncio.FIRST_COMPLETED)
            
            # ì •ë¦¬
            cancel_wait.cancel()

            if gen_task in done:
                response = gen_task.result()
                # Sanitize any think tags from non-streamed content
                def strip_think_sections(text: str) -> str:
                    if not isinstance(text, str) or not text:
                        return text
                    try:
                        import re
                        patterns = [
                            r'(?is)<think>.*?</think>',
                            r'(?is)<thinking>.*?</thinking>',
                            r'(?is)\[think\].*?\[/think\]'
                        ]
                        for p in patterns:
                            text = re.sub(p, '', text)
                        # Remove any stray residual tags
                        text = re.sub(r'(?is)</?thinking?>', '', text)
                        text = re.sub(r'(?is)\[/?think\]', '', text)
                        return text.strip()
                    except Exception:
                        return text
                if isinstance(response, dict):
                    if 'answer' in response:
                        response['answer'] = strip_think_sections(response.get('answer', ''))
                    if 'details' in response:
                        response['details'] = strip_think_sections(response.get('details', ''))
            else:
                # ì·¨ì†Œ ë°œìƒ: ìƒì„± íƒœìŠ¤í¬ ì·¨ì†Œ
                gen_task.cancel()
                try:
                    await gen_task
                except asyncio.CancelledError:
                    pass
                raise HTTPException(status_code=499, detail="Client closed request")
            
            # 4. Ground and verify response (respect resolved scope)
            response = get_response_grounder().ground(response, evidences)
            response = postprocessor.process(response, evidences)

            # 4.1 ìƒì„±ëœ responseì—ì„œ sources ê°•ì œ í•„í„°ë§
            if allowed_docs_enforce and "sources" in response:
                original_sources = response.get("sources", [])
                filtered_sources = []
                for src in original_sources:
                    if src.get("doc_id") in allowed_docs_enforce:
                        filtered_sources.append(src)
                    else:
                        logger.error(f"CRITICAL: Generated source from {src.get('doc_id')} not in allowed docs! Removing.")
                response["sources"] = filtered_sources
                logger.info(f"Filtered sources: {len(original_sources)} -> {len(filtered_sources)}")

            response = enforcer.enforce_evidence(response, evidences, allowed_doc_ids=allowed_docs_enforce)

            # 5. Track citations (with document filtering for follow-up questions)
            allowed_docs = allowed_docs_enforce

            # Use fixed citation map for follow-up questions
            fixed_citation_map = session.first_response_citation_map if should_use_previous_sources and session.first_response_citation_map else None

            if fixed_citation_map:
                logger.info(f"ğŸ”µ FOLLOW-UP - Using fixed citation map")
                logger.info(f"  - Fixed citation map: {fixed_citation_map}")
                logger.info(f"  - Evidences count: {len(evidences)}")
                logger.info(f"  - Allowed docs: {allowed_docs}")

                # Verify evidence count matches first response
                first_evidence_count = session.metadata.get("first_response_evidences_count", 0) if session.metadata else 0
                if len(evidences) != first_evidence_count:
                    logger.error(f"âš ï¸ Evidence count mismatch! First: {first_evidence_count}, Current: {len(evidences)}")

            response = citation_tracker.track_citations(response, evidences, allowed_doc_ids=allowed_docs, fixed_citation_map=fixed_citation_map)
            
            # 6. Format response (with document filtering for follow-up questions)
            allowed_docs_format = allowed_docs_enforce

            # 6.1 ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ ì˜ëª»ëœ ì¶œì²˜ ë²ˆí˜¸ ì œê±°
            if allowed_docs_format:
                # sources ê°œìˆ˜ í™•ì¸
                max_source_num = len(response.get("sources", []))
                answer = response.get("answer", "")

                # [N] í˜•ì‹ì˜ ì¶œì²˜ ë²ˆí˜¸ ì°¾ê¸° ë° í•„í„°ë§
                import re
                def filter_citations(text):
                    def replace_citation(match):
                        num = int(match.group(1))
                        if num <= max_source_num:
                            return match.group(0)
                        else:
                            logger.warning(f"Removing invalid citation [{num}] (max: {max_source_num})")
                            return ""
                    return re.sub(r'\[(\d+)\]', replace_citation, text)

                response["answer"] = filter_citations(answer)
                if "details" in response:
                    response["details"] = filter_citations(response.get("details", ""))
                if "key_facts" in response:
                    response["key_facts"] = [filter_citations(fact) for fact in response.get("key_facts", [])]

            response = formatter.format_response(response, allowed_doc_ids=allowed_docs_format)
            
            # Add assistant message
            sources = response.get("sources", [])

            # í›„ì† ë‹µë³€ì¸ ê²½ìš° sources ê²€ì¦
            if should_use_previous_sources:
                if not sources:
                    logger.error("âš ï¸ No sources found in follow-up response!")
                else:
                    logger.info(f"Follow-up response has {len(sources)} sources")
                    # Log source details for debugging
                    for idx, src in enumerate(sources[:3]):
                        logger.debug(f"Source {idx+1}: doc_id={src.get('doc_id')}, page={src.get('page')}")

            assistant_content = response.get("formatted_text", response.get("answer", ""))

            # ì„¸ì…˜ ì œëª© ì—…ë°ì´íŠ¸ (ì²« ë©”ì‹œì§€ì¸ ê²½ìš°)
            title_updated = False
            new_title = None
            if len(context) == 0:  # ì²« ëŒ€í™”ì¸ ê²½ìš°
                try:
                    new_title = await get_title_generator().generate_title(
                        request.query,
                        assistant_content[:500]
                    )
                    if new_title and new_title != "ìƒˆ ëŒ€í™”":
                        await session_manager.update_session(session_id, title=new_title)
                        title_updated = True
                        logger.info(f"Session {session_id} title updated to: {new_title}")
                except Exception as e:
                    logger.error(f"Failed to generate title: {e}")

            source_doc_ids = [s.get("doc_id") for s in sources if s.get("doc_id")]
            unique_doc_ids = _deduplicate_doc_ids(source_doc_ids)

            doc_scope_metadata = _finalize_doc_scope_metadata(
                doc_scope_metadata,
                mode=doc_scope_mode,
                doc_scope_ids=doc_scope_ids,
                resolved_doc_ids=unique_doc_ids,
                requested_doc_ids=_deduplicate_doc_ids(requested_doc_ids),
                diagnostics=resolution.diagnostics,
                average_score=_average_evidence_score(evidences),
                topic_change=topic_change_detected,
                topic_reason=topic_change_reason,
                topic_suggested=_deduplicate_doc_ids(topic_change_suggested),
            )

            # ì²« ë‹µë³€ì¸ ê²½ìš° evidencesì™€ citation_map ì €ì¥
            is_first_response = len([m for m in session.messages if m.role == "assistant"]) == 1
            if is_first_response and not session.first_response_evidences:
                citation_map = response.get("citation_map", {})
                logger.info("ğŸ”´ FIRST RESPONSE - Saving evidences and citation map")
                logger.info(f"  - Evidences count: {len(evidences)}")
                logger.info(f"  - Citation map: {citation_map}")
                logger.info(f"  - Sources count: {len(sources)}")

                for idx, e in enumerate(evidences[:3]):
                    logger.info(f"  - Evidence {idx}: doc_id={e.get('doc_id')}, page={e.get('page')}")

                await session_manager.update_session(
                    session_id,
                    first_response_evidences=evidences,
                    first_response_citation_map=citation_map,
                    metadata={
                        **((session.metadata or {})),
                        "first_response_evidences_count": len(evidences),
                        "first_response_citation_count": len(citation_map)
                    }
                )

            # Update session memory if summarizer is confident
            latest_context = await session_manager.get_session_context(session_id, max_messages=4)
            summary_result = get_summarizer().summarize(
                latest_context,
                previous_summary=previous_summary,
                previous_entities=session.recent_entities or []
            )

            summary_updated = False
            entities_count = len(session.recent_entities or [])
            summary_confidence = summary_result.confidence if summary_result else 0.0

            if summary_result:
                if summary_result.should_use_summary and not summary_result.used_fallback:
                    await session_manager.update_session(
                        session_id,
                        conversation_summary=summary_result.summary_text,
                        recent_entities=summary_result.entities,
                        recent_source_doc_ids=unique_doc_ids
                    )
                    summary_updated = True
                    entities_count = len(summary_result.entities)
                else:
                    logger.info(
                        "Summary not updated due to confidence gate",
                        extra={
                            "session_id": session_id,
                            "summary_confidence": summary_result.confidence,
                            "used_fallback": summary_result.used_fallback,
                        },
                    )
                    if unique_doc_ids:
                        await session_manager.update_session(
                            session_id,
                            recent_source_doc_ids=unique_doc_ids
                        )
            elif unique_doc_ids:
                await session_manager.update_session(
                    session_id,
                    recent_source_doc_ids=unique_doc_ids
                )

            # ë©”ëª¨ë¦¬ íŒ©íŠ¸ ì €ì¥ ë¹„í™œì„±í™” - ì¶œì²˜ ì¼ê´€ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´
            # memory_facts = _collect_memory_facts(response)
            # if memory_facts:
            #     await session_manager.add_memory_facts(session_id, memory_facts)
            logger.debug("Memory facts collection disabled for source consistency")

            response_metadata = {
                "evidence_count": len(evidences),
                "hallucination_detected": response.get("verification", {}).get("hallucination_detected", False),
                "context_messages": len(context),
                "rewrite": {
                    "used_fallback": rewrite_result.used_fallback,
                    "search_query": rewrite_result.search_query,
                    "reasoning": rewrite_result.reasoning,
                    "sub_queries": rewrite_result.sub_queries,
                },
                "doc_scope": doc_scope_metadata,
                "memory": {
                    "summary_available": bool(previous_summary),
                    "summary_updated": summary_updated,
                    "entities_count": entities_count,
                    "summarizer_confidence": summary_confidence,
                    "source_doc_ids": unique_doc_ids,
                },
                "title_updated": title_updated,
                "new_title": new_title if title_updated else None
            }

            response_metadata["raw_answer"] = response.get("answer", "")
            response_metadata["formatted_text"] = assistant_content
            response_metadata["key_facts"] = response.get("key_facts", [])
            response_metadata["details"] = response.get("details", "")
            response_metadata["grounding"] = response.get("grounding", [])

            await session_manager.add_message(
                session_id,
                "assistant",
                assistant_content,
                sources=sources,
                metadata=response_metadata
            )

            logger.info(
                "Response scope summary",
                extra={
                    "session_id": session_id,
                    "doc_scope": doc_scope_metadata,
                },
            )

            return QueryResponse(
                query=request.query,
                answer=response.get("answer", ""),
                key_facts=response.get("key_facts", []),
                details=response.get("details", ""),
                sources=response.get("sources", []),
                formatted_text=response.get("formatted_text", ""),
                formatted_html=response.get("formatted_html", ""),
                formatted_markdown=response.get("formatted_markdown", ""),
                confidence=response.get("verification", {}).get("confidence", 0),
                session_id=session_id,
                metadata=response_metadata
            )
            
        except asyncio.CancelledError:
            # ì·¨ì†Œëœ ê²½ìš° ì´ë¯¸ ì¤‘ë‹¨ ë©”ì‹œì§€ê°€ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì²˜ë¦¬ ë¶ˆí•„ìš”
            logger.info(f"Request cancelled for session {session_id}")
            raise HTTPException(status_code=499, detail="Client closed request")
        except Exception as e:
            if not cancelled:
                error_msg = error_handler.handle_rag_error(e)
                await session_manager.add_message(
                    session_id,
                    "assistant",
                    error_msg,
                    error=str(e)
                )
                raise HTTPException(status_code=500, detail=error_msg)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Message processing failed: {e}")
        if not cancelled:
            error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            await session_manager.add_message(
                session_id,
                "assistant",
                error_msg,
                error=str(e)
            )
            raise HTTPException(status_code=500, detail=error_msg)
    finally:
        # ì—°ê²° ì²´í¬ íƒœìŠ¤í¬ ì¢…ë£Œ
        cancelled = True
        if disconnect_task:
            disconnect_task.cancel()
            try:
                await disconnect_task
            except asyncio.CancelledError:
                pass

@router.post("/sessions/{session_id}/messages/stream")
async def send_message_stream(
    session_id: str,
    request: QueryRequest,
    http_request: Request
):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ë©”ì‹œì§€ ì²˜ë¦¬"""
    cancel_event = asyncio.Event()
    interrupt_recorded = False

    async def monitor_disconnect():
        nonlocal interrupt_recorded
        try:
            while not cancel_event.is_set():
                if await http_request.is_disconnected():
                    logger.info(f"[stream] Client disconnected for session {session_id}")
                    cancel_event.set()
                    if not interrupt_recorded:
                        try:
                            await session_manager.add_message(
                                session_id,
                                "assistant",
                                "ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                                metadata={"interrupted": True, "reason": "client_disconnect"}
                            )
                            interrupt_recorded = True
                        except Exception as e:
                            logger.error(f"Failed to record interrupt on disconnect: {e}")
                    break
                await asyncio.sleep(0.25)
        except Exception as e:
            logger.debug(f"monitor_disconnect error: {e}")

    monitor_task = asyncio.create_task(monitor_disconnect())

    async def generate():
        full_response = ""
        # Streaming-time think-tag filter state
        in_think = False
        pending = ""
        start_tags = [("<think>", 7), ("<thinking>", 11), ("[think]", 7)]
        end_tags = [("</think>", 8), ("</thinking>", 12), ("[/think]", 8)]

        def find_any(haystack_lower, tags):
            idx = -1
            tag_len = 0
            for t, tlen in tags:
                i = haystack_lower.find(t)
                if i != -1 and (idx == -1 or i < idx):
                    idx = i
                    tag_len = tlen
            return idx, tag_len

        try:
            # Session validation
            session = await session_manager.get_session(session_id)
            if not session:
                yield json.dumps({"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}) + "\n"
                return

            if not request.query or not request.query.strip():
                yield json.dumps({"error": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”"}) + "\n"
                return

            if not session.document_ids and not getattr(request, "skip_document_check", False):
                yield json.dumps({
                    "error": "NO_DOCUMENTS",
                    "message": "ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”"
                }) + "\n"
                return

            # Add user message
            await session_manager.add_message(session_id, "user", request.query)

            previous_summary = session.conversation_summary or ""

            # Build context and rewrite query
            context_messages = await session_manager.get_session_context(session_id, max_messages=10)

            # ì´ì „ ë‹µë³€ì˜ ì¶œì²˜ ì¶”ì  (ì²« ë²ˆì§¸ assistant ë©”ì‹œì§€ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •)
            previous_sources: List[Dict[str, Any]] = []
            previous_doc_ids: List[str] = []
            first_assistant_found = False

            # ì²« ë‹µë³€ì˜ ê³ ì •ëœ evidencesì—ì„œ ë¬¸ì„œ ë²”ìœ„ë§Œ ì¶”ì¶œ (evidence ìì²´ëŠ” ì¬ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            if session.first_response_evidences:
                first_assistant_found = True
                # Extract doc_ids from stored evidences (ë¬¸ì„œ ë²”ìœ„ë§Œ ì‚¬ìš©)
                previous_doc_ids = list(set([
                    e.get("doc_id")
                    for e in session.first_response_evidences
                    if e.get("doc_id")
                ]))
                logger.info(f"Streaming: Using doc scope from first response: docs: {previous_doc_ids} (will search for new evidences)")
            else:
                # ì²« ë²ˆì§¸ assistant ë©”ì‹œì§€ì˜ ì¶œì²˜ë¥¼ ì°¾ì•„ì„œ ê³ ì •
                for message in session.messages:
                    if message.role == "assistant" and message.sources and not first_assistant_found:
                        previous_sources = message.sources
                        previous_doc_ids = [
                            src.get("doc_id")
                            for src in message.sources
                            if isinstance(src, dict) and src.get("doc_id")
                        ]
                        first_assistant_found = True
                        logger.info(f"Streaming: Using first assistant message sources: {previous_doc_ids}")
                        break

            # ì²« ë²ˆì§¸ assistantê°€ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ ê²ƒì„ ì‚¬ìš© (í´ë°±)
            if not first_assistant_found:
                for message in reversed(session.messages):
                    if message.role == "assistant" and message.sources:
                        previous_sources = message.sources
                        previous_doc_ids = [
                            src.get("doc_id")
                            for src in message.sources
                            if isinstance(src, dict) and src.get("doc_id")
                        ]
                        logger.info(f"Streaming: Using recent assistant message sources (fallback): {previous_doc_ids}")
                        break

            recent_for_rewrite = context_messages[-4:] if context_messages else []
            rewrite_context = RewriteContext(
                current_query=request.query,
                recent_messages=recent_for_rewrite,
                summary=previous_summary,
                entities=session.recent_entities or [],
                previous_sources=previous_sources
            )
            rewrite_result = get_query_rewriter().rewrite(rewrite_context)
            retrieval_query = request.query if rewrite_result.used_fallback else rewrite_result.search_query

            # Determine retrieval scope
            requested_doc_ids: List[str] = []
            if request.doc_ids:
                requested_doc_ids = [doc_id for doc_id in request.doc_ids if doc_id in session.document_ids]
                if not requested_doc_ids:
                    yield json.dumps({"error": "ìš”ì²­í•œ ë¬¸ì„œë¥¼ ì„¸ì…˜ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}) + "\n"
                    return

            should_use_previous_sources = bool(previous_doc_ids) and not requested_doc_ids
            doc_scope_metadata: Dict[str, Any] = {}
            allowed_docs_enforce: Optional[List[str]] = None
            doc_scope_ids: List[str] = []
            topic_change_detected = False
            topic_change_reason: Optional[str] = None
            topic_change_suggested: List[str] = []

            # Send status update
            yield json.dumps({
                "status": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...",
                "metadata": {
                    "rewrite_used_fallback": rewrite_result.used_fallback
                }
            }) + "\n"

            retriever_instance = get_retriever()
            resolution = _resolve_evidences(
                query=request.query,
                retrieval_query=retrieval_query,
                retriever_instance=retriever_instance,
                requested_doc_ids=requested_doc_ids,
                session_doc_ids=session.document_ids or [],
                previous_doc_ids=previous_doc_ids,
                should_use_previous_sources=should_use_previous_sources,
            )

            if resolution.status == "no_evidence":
                message = resolution.error_message or "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                yield json.dumps({
                    "error": "no_evidence",
                    "message": message
                }) + "\n"
                await session_manager.add_message(
                    session_id,
                    "assistant",
                    message,
                    error="no_evidence",
                    metadata={"doc_scope": resolution.metadata},
                )
                return

            evidences = resolution.evidences
            doc_scope_metadata = dict(resolution.metadata or {})
            allowed_docs_enforce = resolution.allowed_doc_ids
            topic_change_detected = resolution.topic_change_detected
            topic_change_reason = resolution.topic_change_reason
            topic_change_suggested = resolution.topic_change_suggested or []
            should_use_previous_sources = resolution.allow_fixed_citations
            doc_scope_ids = resolution.doc_scope_ids
            doc_scope_mode = resolution.doc_scope_mode

            if cancel_event.is_set():
                return

            # Send status update
            yield json.dumps({
                "status": "ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ",
                "metadata": {
                    "doc_scope": doc_scope_metadata,
                    "rewrite_used_fallback": rewrite_result.used_fallback
                }
            }) + "\n"

            # Send status update for generation
            yield json.dumps({"status": "ë‹µë³€ ìƒì„± ì¤‘..."}) + "\n"

            # Rerank if available (í›„ì† ì§ˆë¬¸ì—ì„œë„ í•­ìƒ reranking ìˆ˜í–‰)
            # ì²« ë‹µë³€ evidences ì¬ì‚¬ìš© ë¡œì§ ì œê±° - í•­ìƒ ìƒˆë¡œ ê²€ìƒ‰í•œ evidenceë¥¼ rerank
            reranker_instance = get_reranker()
            if reranker_instance and (
                getattr(reranker_instance, "model", None)
                or (getattr(reranker_instance, "use_onnx", False) and hasattr(reranker_instance, "ort_session"))
            ):
                evidences = reranker_instance.rerank(
                    retrieval_query,
                    evidences,
                    top_k=config.TOPK_RERANK
                )
                logger.info(f"Streaming: Reranked {len(evidences)} evidences for query")
            else:
                evidences = evidences[:config.TOPK_RERANK]

            # Stream generation with explicit generator handle for clean aclose
            agen = get_generator().stream_with_context(
                request.query,
                evidences,
                context=context_messages,
                doc_scope=doc_scope_metadata,
                cancel_event=cancel_event
            )
            try:
                async for chunk in agen:
                    if cancel_event.is_set():
                        break
                    if not chunk:
                        continue
                    # Append and filter think sections
                    pending += chunk
                    while True:
                        lbuf = pending.lower()
                        if not in_think:
                            s_idx, s_len = find_any(lbuf, start_tags)
                            if s_idx != -1:
                                # Emit content before <think>
                                if s_idx > 0:
                                    emit = pending[:s_idx]
                                    if emit:
                                        full_response += emit
                                        yield json.dumps({"content": emit}) + "\n"
                                # Enter think region
                                pending = pending[s_idx + s_len:]
                                in_think = True
                                continue
                            else:
                                # No start tag; emit safe prefix keeping small tail to catch split tags
                                tail = 16
                                if len(pending) > tail:
                                    emit = pending[:-tail]
                                    pending = pending[-tail:]
                                    if emit:
                                        full_response += emit
                                        yield json.dumps({"content": emit}) + "\n"
                                break
                        else:
                            # Inside think; look for any end tag
                            e_idx, e_len = find_any(lbuf, end_tags)
                            if e_idx != -1:
                                # Drop up to end tag, exit think
                                pending = pending[e_idx + e_len:]
                                in_think = False
                                continue
                            else:
                                # Still inside think; keep buffer bounded and wait for more
                                if len(pending) > 8192:
                                    pending = pending[-4096:]
                                break
            finally:
                try:
                    await agen.aclose()
                except Exception:
                    pass

            # Flush ALL remaining pending content (important for complete response)
            if not cancel_event.is_set() and pending:
                # If we're still in think mode, we should NOT emit the content
                if not in_think:
                    emit = pending
                    pending = ""
                    if emit:
                        full_response += emit
                        yield json.dumps({"content": emit}) + "\n"
            
            if cancel_event.is_set():
                # ì´ë¯¸ monitor_disconnectì—ì„œ ì¤‘ë‹¨ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í–ˆì„ ìˆ˜ ìˆìŒ
                if not interrupt_recorded:
                    try:
                        await session_manager.add_message(
                            session_id,
                            "assistant",
                            "ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                            metadata={"interrupted": True, "reason": "client_disconnect"}
                        )
                        interrupt_recorded = True
                    except Exception as e:
                        logger.error(f"Failed to record interrupt after cancel: {e}")
                return

            allowed_docs = allowed_docs_enforce

            response_payload = {
                "answer": full_response,
                "key_facts": [],
                "details": "",
                "sources": [],
            }

            response_payload = get_response_grounder().ground(response_payload, evidences)
            response_payload = postprocessor.process(response_payload, evidences)
            response_payload = enforcer.enforce_evidence(
                response_payload,
                evidences,
                allowed_doc_ids=allowed_docs
            )
            # Use fixed citation map for follow-up questions (streaming)
            fixed_citation_map = session.first_response_citation_map if should_use_previous_sources and session.first_response_citation_map else None
            response_payload = citation_tracker.track_citations(
                response_payload,
                evidences,
                allowed_doc_ids=allowed_docs,
                fixed_citation_map=fixed_citation_map
            )
            response_payload = formatter.format_response(
                response_payload,
                allowed_doc_ids=allowed_docs
            )

            assistant_content = response_payload.get("formatted_text", response_payload.get("answer", full_response))
            sources = response_payload.get("sources", [])

            source_doc_ids = [s.get("doc_id") for s in sources if s.get("doc_id")]
            unique_doc_ids: List[str] = []
            for doc_id in source_doc_ids:
                if doc_id and doc_id not in unique_doc_ids:
                    unique_doc_ids.append(doc_id)

            doc_scope_metadata = _finalize_doc_scope_metadata(
                doc_scope_metadata,
                mode=doc_scope_mode,
                doc_scope_ids=doc_scope_ids,
                resolved_doc_ids=unique_doc_ids,
                requested_doc_ids=_deduplicate_doc_ids(requested_doc_ids),
                diagnostics=resolution.diagnostics,
                average_score=_average_evidence_score(evidences),
                topic_change=topic_change_detected,
                topic_reason=topic_change_reason,
                topic_suggested=_deduplicate_doc_ids(topic_change_suggested),
            )

            # ì²« ë‹µë³€ì¸ ê²½ìš° evidencesì™€ citation_map ì €ì¥ (ìŠ¤íŠ¸ë¦¬ë°)
            is_first_response = len([m for m in session.messages if m.role == "assistant"]) == 1
            if is_first_response and not session.first_response_evidences:
                citation_map = response_payload.get("citation_map", {})
                logger.info(f"Streaming: Saving first response evidences: {len(evidences)} items, citation_map: {citation_map}")
                await session_manager.update_session(
                    session_id,
                    first_response_evidences=evidences,
                    first_response_citation_map=citation_map,
                    metadata={
                        **((session.metadata or {})),
                        "first_response_evidences_count": len(evidences),
                        "first_response_citation_count": len(citation_map)
                    }
                )

            summary_updated = False
            entities_count = len(session.recent_entities or [])
            summary_confidence = 0.0

            try:
                latest_context = await session_manager.get_session_context(session_id, max_messages=4)
                summary_result = get_summarizer().summarize(
                    latest_context,
                    previous_summary=previous_summary,
                    previous_entities=session.recent_entities or []
                )
                summary_confidence = summary_result.confidence if summary_result else 0.0
                if summary_result and summary_result.should_use_summary and not summary_result.used_fallback:
                    await session_manager.update_session(
                        session_id,
                        conversation_summary=summary_result.summary_text,
                        recent_entities=summary_result.entities,
                        recent_source_doc_ids=unique_doc_ids
                    )
                    summary_updated = True
                    entities_count = len(summary_result.entities)
                elif summary_result:
                    logger.info(
                        "Streaming summary gate skipped",
                        extra={
                            "session_id": session_id,
                            "confidence": summary_result.confidence,
                            "used_fallback": summary_result.used_fallback,
                        },
                    )
                    if unique_doc_ids:
                        await session_manager.update_session(
                            session_id,
                            recent_source_doc_ids=unique_doc_ids
                        )
                elif unique_doc_ids:
                    await session_manager.update_session(
                        session_id,
                        recent_source_doc_ids=unique_doc_ids
                    )
            except Exception as e:
                logger.error(f"Failed to update summary in stream: {e}")

            response_metadata = {
                "evidence_count": len(evidences),
                "hallucination_detected": response_payload.get("verification", {}).get("hallucination_detected", False),
                "context_messages": len(context_messages),
                "rewrite": {
                    "used_fallback": rewrite_result.used_fallback,
                    "search_query": rewrite_result.search_query,
                    "reasoning": rewrite_result.reasoning,
                    "sub_queries": rewrite_result.sub_queries,
                },
                "doc_scope": doc_scope_metadata,
                "memory": {
                    "summary_available": bool(previous_summary),
                    "summary_updated": summary_updated,
                    "entities_count": entities_count,
                    "summarizer_confidence": summary_confidence,
                    "source_doc_ids": unique_doc_ids,
                },
                "streaming": True,
            }

            response_metadata["raw_answer"] = response_payload.get("answer", "")
            response_metadata["formatted_text"] = assistant_content
            response_metadata["key_facts"] = response_payload.get("key_facts", [])
            response_metadata["details"] = response_payload.get("details", "")
            response_metadata["grounding"] = response_payload.get("grounding", [])

            await session_manager.add_message(
                session_id,
                "assistant",
                assistant_content,
                sources=sources,
                metadata=response_metadata
            )

            # ë©”ëª¨ë¦¬ íŒ©íŠ¸ ì €ì¥ ë¹„í™œì„±í™” - ì¶œì²˜ ì¼ê´€ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´
            # memory_facts = _collect_memory_facts(response_payload)
            # if memory_facts:
            #     await session_manager.add_memory_facts(session_id, memory_facts)
            logger.debug("Streaming: Memory facts collection disabled for source consistency")

            # Send final data with sources
            yield json.dumps({
                "complete": True,
                "answer": assistant_content,
                "sources": sources,
                "metadata": response_metadata,
            }) + "\n"
            
        except ClientDisconnect:
            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ: ì¤‘ë‹¨ ë©”ì‹œì§€ ì €ì¥ í›„ ì¢…ë£Œ
            try:
                await session_manager.add_message(
                    session_id,
                    "assistant",
                    "ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    metadata={"interrupted": True, "reason": "client_disconnect"}
                )
                interrupt_recorded = True
            except Exception as e:
                logger.error(f"Failed to record interrupt on disconnect: {e}")
            return
        except asyncio.CancelledError:
            # ì„œë²„ íƒœìŠ¤í¬ ì·¨ì†Œ
            try:
                await session_manager.add_message(
                    session_id,
                    "assistant",
                    "ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    metadata={"interrupted": True, "reason": "server_cancel"}
                )
            except Exception as e:
                logger.error(f"Failed to record server cancel: {e}")
            return
        except Exception as e:
            logger.error(f"Streaming failed: {e}")
            # ì—ëŸ¬ëŠ” í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡(ì—°ê²°ì´ ì‚´ì•„ìˆì„ ë•Œë§Œ)
            try:
                yield json.dumps({"error": str(e)}) + "\n"
            except Exception:
                pass
        finally:
            try:
                cancel_event.set()
                monitor_task.cancel()
                try:
                    await monitor_task
                except asyncio.CancelledError:
                    pass
            except Exception:
                pass
    
    return StreamingResponse(
        generate(),
        media_type="application/x-ndjson"
    )

@router.websocket("/sessions/{session_id}/ws")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """WebSocket ì—°ê²° for real-time chat"""
    await manager.connect(websocket, session_id)
    
    try:
        while True:
            # Receive message
            data = await websocket.receive_json()
            
            # Process based on message type
            if data.get("type") == "ping":
                await manager.send_message(session_id, {"type": "pong"})
                
            elif data.get("type") == "message":
                query = data.get("content", "")
                
                # Validate input
                if not query.strip():
                    await manager.send_message(session_id, {
                        "type": "error",
                        "message": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”"
                    })
                    continue
                
                # Get session
                session = await session_manager.get_session(session_id)
                if not session:
                    await manager.send_message(session_id, {
                        "type": "error",
                        "message": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    })
                    continue
                
                # Process with lock to prevent concurrent processing
                async with manager.session_locks.get(session_id, asyncio.Lock()):
                    try:
                        # Add user message
                        await session_manager.add_message(session_id, "user", query)
                        
                        # Send status
                        await manager.send_message(session_id, {
                            "type": "status",
                            "message": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."
                        })
                        
                        # Get evidences
                        evidences = retriever.retrieve(query, document_ids=session.document_ids)
                        
                        if not evidences:
                            await manager.send_message(session_id, {
                                "type": "response",
                                "content": "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                "complete": True
                            })
                            continue
                        
                        # Send status
                        await manager.send_message(session_id, {
                            "type": "status",
                            "message": "ë‹µë³€ ìƒì„± ì¤‘..."
                        })
                        
                        # Get context
                        context = await session_manager.get_session_context(session_id)
                        
                        # Stream response
                        full_response = ""
                        async for chunk in generator.stream_with_context(
                            query,
                            evidences,
                            context=context
                        ):
                            if chunk:
                                full_response += chunk
                                await manager.send_message(session_id, {
                                    "type": "response",
                                    "content": chunk,
                                    "complete": False
                                })
                        
                        # Process citations
                        response_data = citation_tracker.track_citations(
                            {"answer": full_response},
                            evidences
                        )
                        
                        # Save complete message
                        await session_manager.add_message(
                            session_id,
                            "assistant",
                            full_response,
                            sources=response_data.get("sources", [])
                        )

                        # Send completion with sources
                        await manager.send_message(session_id, {
                            "type": "response",
                            "complete": True,
                            "sources": response_data.get("sources", [])
                        })
                        
                    except Exception as e:
                        logger.error(f"WebSocket processing error: {e}")
                        await manager.send_message(session_id, {
                            "type": "error",
                            "message": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
                        })
            
            elif data.get("type") == "stop":
                # Handle stop request
                await manager.send_message(session_id, {
                    "type": "stopped",
                    "message": "ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤"
                })
                
    except WebSocketDisconnect:
        manager.disconnect(session_id)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        manager.disconnect(session_id)

@router.post("/sessions/{session_id}/interrupt")
async def interrupt_session(session_id: str) -> Dict:
    """ì„¸ì…˜ ì¤‘ë‹¨ ì²˜ë¦¬"""
    try:
        # ì´ë¯¸ ì§ì „ ë©”ì‹œì§€ê°€ ì¤‘ë‹¨ìœ¼ë¡œ ê¸°ë¡ë˜ì–´ ìˆìœ¼ë©´ ì¤‘ë³µ ê¸°ë¡ ë°©ì§€
        session = await session_manager.get_session(session_id)
        if session and session.messages:
            last = session.messages[-1]
            if (last.metadata and last.metadata.get('interrupted')) or (
                isinstance(last.content, str) and 'ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤' in last.content
            ):
                return {"success": True, "message": "ì´ë¯¸ ì¤‘ë‹¨ ë©”ì‹œì§€ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤"}

        # ì¤‘ë‹¨ ë©”ì‹œì§€ ì¶”ê°€
        await session_manager.add_message(
            session_id,
            "assistant",
            "ë‹µë³€ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
            metadata={"interrupted": True, "reason": "user_action"}
        )
        
        return {
            "success": True,
            "message": "ì¤‘ë‹¨ ë©”ì‹œì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
    except Exception as e:
        logger.error(f"Failed to save interrupt message: {e}")
        raise HTTPException(status_code=500, detail="ì¤‘ë‹¨ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

@router.delete("/sessions/{session_id}/messages")
async def clear_messages(session_id: str) -> Dict:
    """ì„¸ì…˜ ë©”ì‹œì§€ ì´ˆê¸°í™”"""
    try:
        success = await session_manager.clear_session_messages(session_id)
        if not success:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "message": "ëŒ€í™” ë‚´ì—­ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to clear messages: {e}")
        raise HTTPException(status_code=500, detail="ë©”ì‹œì§€ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

@router.get("/sessions/{session_id}/export")
async def export_session(session_id: str) -> Dict:
    """ì„¸ì…˜ ë‚´ë³´ë‚´ê¸°"""
    try:
        session_data = await session_manager.export_session(session_id)
        if not session_data:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "session_data": session_data,
            "exported_at": datetime.now().isoformat()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to export session: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ë‚´ë³´ë‚´ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
def _normalize_doc_ids(doc_ids: Optional[List[str]]) -> Optional[List[str]]:
    """Normalize document IDs with Unicode normalization"""
    if not doc_ids:
        return doc_ids

    import unicodedata
    normed = []
    for d in doc_ids:
        try:
            if isinstance(d, str):
                # Apply Unicode normalization first
                d = unicodedata.normalize('NFC', d)
                # Keep the original ID as-is to match indexed documents
                # Don't remove extensions here - they should match what was indexed
                normed.append(d)
        except Exception:
            continue
    return normed
