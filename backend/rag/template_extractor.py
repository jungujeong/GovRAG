"""
Template-Based Evidence Extractor - ZERO HALLUCINATION

Evidenceì—ì„œ ì§ì ‘ ì¶”ì¶œë§Œ í•˜ê³ , LLM ìƒì„± ì—†ìŒ.
100% Evidence ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ì—¬ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€.
"""

from typing import List, Dict, Tuple
import re
from rag.fuzzy_matcher import get_fuzzy_matcher
from rag.korean_analyzer import get_korean_analyzer
import logging

logger = logging.getLogger(__name__)


class TemplateExtractor:
    """
    Template-based evidence extraction

    LLM ìƒì„± ëŒ€ì‹  Evidenceì—ì„œ ì§ì ‘ ì¶”ì¶œ:
    1. Queryì™€ ë§¤ì¹­ë˜ëŠ” ë¬¸ìž¥ ì°¾ê¸° (Fuzzy Matching)
    2. ì ìˆ˜ìˆœ ì •ë ¬
    3. Template ì¡°í•© (Evidence ì›ë¬¸ ê·¸ëŒ€ë¡œ)

    ìž¥ì :
    - Jaccard similarity 1.0 (100% Evidence ì‚¬ìš©)
    - í• ë£¨ì‹œë„¤ì´ì…˜ ë¶ˆê°€ëŠ¥
    - ì¡°ì‚¬ ë³€í˜• ì—†ìŒ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
    """

    def __init__(self, match_threshold: float = 0.15):
        """
        Args:
            match_threshold: ìµœì†Œ ìœ ì‚¬ë„ (ê¸°ë³¸ 0.15)
        """
        self.fuzzy_matcher = get_fuzzy_matcher()
        self.korean_analyzer = get_korean_analyzer()
        self.match_threshold = match_threshold

    def extract(
        self,
        query: str,
        evidences: List[Dict]
    ) -> Dict:
        """
        Extract evidence-only answer

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            evidences: Evidence ë¦¬ìŠ¤íŠ¸

        Returns:
            {
                "answer": str,
                "sources": List[Dict],
                "metadata": Dict
            }
        """
        logger.debug(f"Template Extractor: query='{query}', evidences={len(evidences)}")

        # 1. Queryì™€ ë§¤ì¹­ë˜ëŠ” ë¬¸ìž¥ ì°¾ê¸°
        matched_sentences = self._find_matched_sentences(query, evidences)

        logger.debug(f"Found {len(matched_sentences)} matched sentences")

        # 2. ë§¤ì¹­ëœ ë¬¸ìž¥ì´ ì—†ìœ¼ë©´ no-answer template
        if not matched_sentences:
            return self._no_answer_template(evidences)

        # 3. Template ì¡°í•© (ìƒìœ„ Nê°œë§Œ)
        answer = self._assemble_template(matched_sentences, top_k=3)

        # 4. Sources ìƒì„± (Evidence ê·¸ëŒ€ë¡œ)
        sources = self._create_sources(matched_sentences, evidences)

        # 5. Metadata
        metadata = {
            "extraction_method": "template",
            "matched_sentences": len(matched_sentences),
            "used_evidences": len(set(m["evidence_idx"] for m in matched_sentences)),
            "jaccard_expected": 1.0,
            "hallucination_risk": 0.0
        }

        return {
            "answer": answer,
            "sources": sources,
            "metadata": metadata
        }

    def _find_matched_sentences(
        self,
        query: str,
        evidences: List[Dict]
    ) -> List[Dict]:
        """
        Find sentences in evidences that match the query

        Uses hybrid matching:
        1. Fuzzy similarity (n-gram, Levenshtein)
        2. Keyword overlap (content words)

        Args:
            query: Search query
            evidences: List of evidence dictionaries

        Returns:
            List of matched sentence dictionaries with scores
        """
        matched = []

        # Extract query keywords
        query_keywords = set(self.korean_analyzer.analyze(query))

        for ev_idx, evidence in enumerate(evidences):
            text = evidence.get("text", "")

            # Split into sentences
            sentences = self._split_sentences(text)

            for sent_idx, sentence in enumerate(sentences):
                # 1. Fuzzy similarity score
                fuzzy_score = self.fuzzy_matcher.combined_similarity(query, sentence)

                # 2. Keyword overlap score
                sentence_keywords = set(self.korean_analyzer.analyze(sentence))
                if len(query_keywords) > 0:
                    overlap = len(query_keywords & sentence_keywords)
                    keyword_score = overlap / len(query_keywords)
                else:
                    keyword_score = 0.0

                # 3. Combined score (weighted average)
                combined_score = (fuzzy_score * 0.4) + (keyword_score * 0.6)

                if combined_score >= self.match_threshold:
                    matched.append({
                        "text": sentence,
                        "score": combined_score,
                        "fuzzy_score": fuzzy_score,
                        "keyword_score": keyword_score,
                        "evidence_idx": ev_idx,
                        "sentence_idx": sent_idx,
                        "doc_id": evidence.get("doc_id", ""),
                        "page": evidence.get("page", 0),
                        "citation": ev_idx + 1
                    })

        # Sort by score (descending)
        matched.sort(key=lambda x: x["score"], reverse=True)

        logger.debug(f"Matched {len(matched)} sentences with query keywords: {query_keywords}")

        return matched

    def _split_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences

        Args:
            text: Input text

        Returns:
            List of sentences
        """
        # Korean sentence endings: ë‹¤, ìš”, ìŒ, ê¹Œ, ì§€, etc.
        # Also split by newlines
        sentences = re.split(r'[.!?\n]+', text)

        # Clean and filter
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]

        return sentences

    def _assemble_template(
        self,
        matched_sentences: List[Dict],
        top_k: int = 3
    ) -> str:
        """
        Assemble answer from matched sentences

        Args:
            matched_sentences: Matched sentence dictionaries
            top_k: Number of top sentences to include

        Returns:
            Assembled answer text
        """
        # Take top K sentences
        top_sentences = matched_sentences[:top_k]

        # Template header
        answer = "ðŸ“„ ë¬¸ì„œ ë‚´ìš©:\n\n"

        # Add each matched sentence
        for match in top_sentences:
            answer += f"â€¢ {match['text']} [{match['citation']}]\n\n"

        return answer.strip()

    def _create_sources(
        self,
        matched_sentences: List[Dict],
        evidences: List[Dict]
    ) -> List[Dict]:
        """
        Create sources from matched sentences

        Args:
            matched_sentences: Matched sentence dictionaries
            evidences: Original evidence list

        Returns:
            List of source dictionaries
        """
        # Get unique evidence indices used
        used_evidence_indices = sorted(set(m["evidence_idx"] for m in matched_sentences))

        sources = []
        for idx in used_evidence_indices:
            evidence = evidences[idx]
            sources.append({
                "index": idx + 1,
                "citation_number": idx + 1,
                "doc_id": evidence.get("doc_id", ""),
                "page": evidence.get("page", 0),
                "start_char": evidence.get("start_char", 0),
                "end_char": evidence.get("end_char", 0),
                "text_snippet": evidence.get("text", "")[:200],
                "score": evidence.get("score", 0.0)
            })

        return sources

    def _no_answer_template(self, evidences: List[Dict]) -> Dict:
        """
        No-answer template when no matches found

        Args:
            evidences: Evidence list (for metadata)

        Returns:
            No-answer response dictionary
        """
        answer = (
            "ðŸ’¬ ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            "ì§ˆë¬¸ì„ ë‹¤ì‹œ ìž‘ì„±í•˜ì‹œê±°ë‚˜, êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”."
        )

        # Still provide all evidences as sources for transparency
        sources = []
        for idx, evidence in enumerate(evidences):
            sources.append({
                "index": idx + 1,
                "citation_number": idx + 1,
                "doc_id": evidence.get("doc_id", ""),
                "page": evidence.get("page", 0),
                "start_char": evidence.get("start_char", 0),
                "end_char": evidence.get("end_char", 0),
                "text_snippet": evidence.get("text", "")[:200],
                "score": evidence.get("score", 0.0)
            })

        metadata = {
            "extraction_method": "no_match",
            "matched_sentences": 0,
            "used_evidences": 0,
            "jaccard_expected": 0.0,
            "hallucination_risk": 0.0
        }

        return {
            "answer": answer,
            "sources": sources,
            "metadata": metadata
        }


# Global instance
_template_extractor_instance = None


def get_template_extractor() -> TemplateExtractor:
    """Get global template extractor instance"""
    global _template_extractor_instance
    if _template_extractor_instance is None:
        _template_extractor_instance = TemplateExtractor()
    return _template_extractor_instance
