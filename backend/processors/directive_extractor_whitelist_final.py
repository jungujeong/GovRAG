#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
êµ¬ì²­ì¥ ì§€ì‹œ/í›ˆì‹œ/ë³´ê³  PDF â†’ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ì„œ ì¶”ì¶œ ì‹œìŠ¤í…œ (ìµœì¢… ì™„ì„±íŒ)

í•µì‹¬ ê°œì„ ì‚¬í•­:
- KNOWN_DEPARTMENTS í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë¡œ ì •í™•í•œ ë¶€ì„œë§Œ ì¶”ì¶œ
- ì œëª© ë ë¶€ì„œëª… ìë™ ì œê±° ë° ë³„ë„ ìˆ˜ì§‘
- ë¶€ì„œì—´ì—ì„œ n-gram ì¡°í•©ìœ¼ë¡œ ë¶„ë¦¬ëœ ë¶€ì„œëª… ì¬ì¡°ë¦½ ("ì‹œì„¤ê´€"+"ë¦¬ì‚¬ì—…ì†Œ"â†’"ì‹œì„¤ê´€ë¦¬ì‚¬ì—…ì†Œ")
- ì•ˆì „í•œ ì¢Œí‘œ ê²½ê³„: ë³¸ë¬¸(last_col_start-12pt), ë¶€ì„œì—´(last_col_start+6pt)
- í—¤ë” í‚¤ì›Œë“œ ê°•ë ¥ í•„í„°ë§ ë° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í¬í•¨

ì•ˆì „ ê²½ê³„ê°’ ì„¤ì • ì´ìœ :
- MAIN_BOUNDARY_OFFSET = -12pt : ë¶€ì„œì—´ê³¼ ë³¸ë¬¸ ì™„ì „ ë¶„ë¦¬, ì—¬ë°± ì¶©ë¶„íˆ í™•ë³´
- DEPT_BOUNDARY_OFFSET = +6pt  : ë¶€ì„œì—´ ì‹œì‘ì ì„ ëª…í™•íˆ í•˜ì—¬ ì¡ìŒ ë‹¨ì–´ ìµœì†Œí™”
"""

import os
import re
import json
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Any, Set
from itertools import combinations

import fitz  # PyMuPDF

logging.basicConfig(level=logging.INFO)
log = logging.getLogger("directive_whitelist")

# ----------------------- ì„¤ì • ìƒìˆ˜ -----------------------

# ì•ˆì „ ê²½ê³„ê°’ (ìƒë‹¨ ì£¼ì„ ì°¸ì¡°)
MAIN_BOUNDARY_OFFSET = -12  # ë³¸ë¬¸ ì˜ì—­: last_col_start + ì´ ê°’ ì´í•˜ë§Œ ì‚¬ìš©
DEPT_BOUNDARY_OFFSET = +6   # ë¶€ì„œì—´ ì˜ì—­: last_col_start + ì´ ê°’ ì´ìƒë§Œ ì‚¬ìš©

# í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¶€ì„œ ëª©ë¡ (ì‚¬ìš©ì ì œê³µ)
KNOWN_DEPARTMENTS = {
    # í•µì‹¬ ë¶€ì„œ/êµ­/ì‹¤
    "ì´ë¬´ê³¼","ê¸°íšì˜ˆì‚°ê³¼","ê¸°íšì¡°ì •ì‹¤","ê°ì‚¬ì‹¤","í™ë³´ë‹´ë‹¹ê´€",
    "í–‰ì •ì§€ì›ê³¼","í–‰ì •ì§€ì›êµ­","ì•ˆì „ë„ì‹œêµ­","ë¯¸ë˜ì„±ì¥êµ­","ë³µì§€í™˜ê²½êµ­",

    # ê²½ì œ/ì‚°ì—…/ì¼ìë¦¬
    "ê²½ì œì¼ìë¦¬ê³¼","ì¼ìë¦¬ê²½ì œê³¼","ì‚°ì—…ê²½ì œê³¼","íˆ¬ììœ ì¹˜ê³¼",

    # ë„ì‹œÂ·ê±´ì„¤Â·êµí†µ
    "ë„ì‹œê³„íšê³¼","ë„ì‹œì¬ìƒê³¼","ë„ì‹œì •ë¹„ê³¼","ë„ì‹œê´€ë¦¬ê³¼",
    "ê±´ì„¤ê³¼","ê±´ì„¤ê´€ë¦¬ê³¼","ê±´ì¶•ê³¼","ë„ë¡œê³¼","í•˜ì²œê³¼",
    "êµí†µí–‰ì •ê³¼","ì£¼ì°¨ê´€ë¦¬ê³¼","ìŠ¤ë§ˆíŠ¸ë„ì‹œê³¼","ì •ë³´í†µì‹ ê³¼",

    # ì•ˆì „Â·ì¬ë‚œ
    "ì•ˆì „ì´ê´„ê³¼","ì¬ë‚œì•ˆì „ê³¼","ë¯¼ë°©ìœ„ê³¼",

    # í™˜ê²½Â·ì²­ì†ŒÂ·ê³µì›Â·ìì›ìˆœí™˜
    "í™˜ê²½ìœ„ìƒê³¼","ìœ„ìƒí™˜ê²½ê³¼","ì²­ì†Œí–‰ì •ê³¼","ìì›ìˆœí™˜ê³¼",
    "ê³µì›ë…¹ì§€ê³¼","ì‚°ë¦¼ë…¹ì§€ê³¼","í™˜ê²½ì •ì±…ê³¼",

    # ë¬¸í™”Â·ê´€ê´‘Â·ì²´ìœ¡Â·êµìœ¡
    "ë¬¸í™”ì˜ˆìˆ ê³¼","ë¬¸í™”ì²´ìœ¡ê³¼","ê´€ê´‘ì§„í¥ê³¼","ì²´ìœ¡ì§€ì›ê³¼",
    "í‰ìƒêµìœ¡ê³¼","êµìœ¡ì •ì±…ê³¼","í‰ìƒí•™ìŠµê³¼","ë¬¸í™”ê´€ê´‘ê³¼",

    # ë³µì§€Â·ë³´ê±´
    "ë³µì§€ì •ì±…ê³¼","ì‚¬íšŒë³µì§€ê³¼","ì–´ë¥´ì‹ ì¥ì• ì¸ê³¼","ë…¸ì¸ë³µì§€ê³¼",
    "ê°€ì¡±ì •ì±…ê³¼","ì—¬ì„±ê°€ì¡±ê³¼","ì•„ë™ë³´ìœ¡ê³¼","ì²­ë…„ì •ì±…ê³¼",
    "ê±´ê°•ì¦ì§„ê³¼","ë³´ê±´ì†Œ",

    # ì„¸ë¬´Â·ì¬ë¬´Â·íšŒê³„Â·ë¯¼ì›
    "ì„¸ë¬´ê³¼","ì¬ë¬´ê³¼","íšŒê³„ê³¼","ë¯¼ì›ì—¬ê¶Œê³¼","ë¯¼ì›ë´‰ì‚¬ê³¼",

    # ì „ëµ/íŠ¹ìˆ˜ ì¡°ì§
    "ì „ëµì‚¬ì—…ê³¼","ë„ì‹œì¬ìƒì§€ì›ì„¼í„°","ì‹œì„¤ê´€ë¦¬ì‚¬ì—…ì†Œ","ì˜íšŒì‚¬ë¬´êµ­",

    # ê´€Â·ì„¼í„°Â·ë‹¨(ì‹¤ì œ ë§ì´ ì“°ì´ëŠ” ëª…ì¹­)
    "ì²­ì†Œë…„ìƒë‹´ë³µì§€ì„¼í„°","ì¥ì• ì¸ë³µì§€ê´€","ë…¸ì¸ë³µì§€ê´€","ì—¬ì„±ì¸ë ¥ê°œë°œì„¼í„°",

    # ì „ë¶€ì„œ/ì „ë™(ì§‘í•© ì§€ì‹œìš©)
    "ì „ë¶€ì„œ","ì „ë™","ì „ ë™","ì „ ë¶€ ì„œ",
}

# í—¤ë”/ì¡ìŒ í‚¤ì›Œë“œ (ê°•í™”)
HEADER_KEYWORDS = re.compile(
    r'(êµ¬ì²­ì¥\s*(ì§€ì‹œ|í›ˆì‹œ|ë³´ê³ )\s*ì‚¬í•­|ì¼\s*ë ¨|ì²˜\s*ë¦¬|ì§€\s*ì‹œ|ê¸°\s*í•œ|ì£¼\s*ê´€|ê´€\s*ë ¨|ë‹´\s*ë‹¹|ë¶€ì„œ(?!\s*$)|ì²˜ë¦¬ê¸°í•œ|ì²˜ë¦¬ì£¼ê´€|ë¶€ì„œê¸°í•œê´€|ì£¼ê´€ë¶€ì„œ|ê´€ë ¨ë¶€ì„œ|ì¼ë ¨|ë²ˆí˜¸|êµ¬ë¶„|ì‚¬í•­|ê³„ì†|í›ˆ\s*ì‹œ|ë³´\s*ê³ )',
    re.I
)

NOISE_KEYWORDS = re.compile(
    r'(ì²˜ë¦¬|ê¸°í•œ|ì£¼ê´€|ê´€ë ¨|ë‹´ë‹¹|ë²ˆí˜¸|êµ¬ë¶„|ê³„ì†)',
    re.I
)

# ë‚ ì§œ íŒ¨í„´ë“¤
DATE_RX = re.compile(r'(?P<y>2,?0\d{2})\.\s*(?P<m>\d{1,2})\.\s*(?P<d>\d{1,2})\.?')
YEAR_ONLY_RX = re.compile(r'2,?0\d{2}\.')
MONTH_DAY_RX = re.compile(r'\b\d{1,2}\.\s*\d{1,2}\.')
ALL_DATE_PATTERNS = [DATE_RX, YEAR_ONLY_RX, MONTH_DAY_RX]

# ----------------------- ë³´ì¡° í•¨ìˆ˜ -----------------------

def detect_page_category(raw_text: str) -> str:
    """í˜ì´ì§€ ì¹´í…Œê³ ë¦¬ íƒì§€"""
    head = "\n".join(raw_text.splitlines()[:20])
    if re.search(r'í›ˆ\s*ì‹œ', head): return "í›ˆì‹œ"
    if re.search(r'ë³´\s*ê³ ', head): return "ë³´ê³ "
    return "ì§€ì‹œ"

def find_first_circle_y(page) -> float:
    """ì²« ë²ˆì§¸ 'â—‹' ìœ„ì¹˜ íƒì§€"""
    rects = page.search_for("â—‹", quads=False)
    if rects:
        return min(r.y0 for r in rects)
    ys = []
    for w in page.get_text("words", sort=True):
        if len(w) >= 5 and "â—‹" in (w[4] or ""):
            ys.append(w[1])
    return min(ys) if ys else -1.0

def detect_column_edges(page) -> Tuple[List[float], float]:
    """ì—´ ê²½ê³„ ê°ì§€ ë° ë§ˆì§€ë§‰ ì—´ ì‹œì‘ì  ë°˜í™˜"""
    words = page.get_text("words", sort=True)
    if not words:
        w = page.rect.width
        edges = [w*i/5 for i in range(6)]
        return edges, edges[-2]

    h = page.rect.height
    header_words = [w for w in words if w[1] < h*0.2]
    if not header_words:
        header_words = words[:60]

    centers = sorted(((w[0]+w[2])/2) for w in header_words)
    gaps = []
    for i in range(1, len(centers)):
        gap = centers[i] - centers[i-1]
        if gap > 30:
            gaps.append((gap, (centers[i-1]+centers[i])/2))

    if gaps:
        gaps.sort(reverse=True)
        boundaries = [x for _, x in gaps[:5]]
        edges = [0.0] + sorted(boundaries) + [page.rect.width]
    else:
        w = page.rect.width
        edges = [w*i/5 for i in range(6)]

    last_col_start = edges[-2] if len(edges) >= 2 else page.rect.width*0.8
    return edges, last_col_start

# ----------------------- ì œëª©ì—ì„œ ë¶€ì„œ ì¶”ì¶œ -----------------------

def normalize_spacing_for_departments(text: str) -> str:
    """ë¶€ì„œ ê´€ë ¨ ë„ì–´ì“°ê¸° ì •ê·œí™”"""
    text = re.sub(r'ì „\s*ë¶€\s*ì„œ', 'ì „ë¶€ì„œ', text)
    text = re.sub(r'ì „\s*ë™', 'ì „ë™', text)
    return text

def strip_trailing_departments_from_title(title: str, known_depts: Set[str]) -> Tuple[str, List[str]]:
    """
    ì œëª© ëì—ì„œ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ì„œëª… ì¶”ì¶œ ë° ì œê±°
    ë°˜í™˜: (ì •ì œëœ_ì œëª©, ì¶”ì¶œëœ_ë¶€ì„œ_ë¦¬ìŠ¤íŠ¸)
    """
    # 1) ë„ì–´ì“°ê¸° ì •ê·œí™”
    normalized = normalize_spacing_for_departments(title)
    
    extracted_depts = []
    cleaned_title = normalized
    
    # 2) ì˜¤ë¥¸ìª½ ëì—ì„œ ë¶€ì„œëª… ë°˜ë³µ ì œê±°
    max_iterations = 10
    for _ in range(max_iterations):
        found_dept = None
        longest_match = 0
        
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ë¶€ì„œëª…ì„ ì²´í¬ (ê¸´ ê²ƒë¶€í„°)
        for dept in sorted(known_depts, key=len, reverse=True):
            # ì œëª© ëì— ì´ ë¶€ì„œê°€ ìˆëŠ”ì§€ í™•ì¸ (ì•ë’¤ ê³µë°± í¬í•¨ ê°€ëŠ¥)
            pattern = rf'\s*{re.escape(dept)}\s*$'
            if re.search(pattern, cleaned_title, re.I):
                if len(dept) > longest_match:
                    found_dept = dept
                    longest_match = len(dept)
        
        if not found_dept:
            break
            
        # ê°€ì¥ ê¸´ ë§¤ì¹­ ë¶€ì„œë¥¼ ì œê±°
        pattern = rf'\s*{re.escape(found_dept)}\s*$'
        cleaned_title = re.sub(pattern, '', cleaned_title, flags=re.I).strip()
        extracted_depts.append(found_dept)
    
    # ì¶”ì¶œ ìˆœì„œ ë’¤ì§‘ê¸° (ì˜¤ë¥¸ìª½ë¶€í„° ì¶”ì¶œí–ˆìœ¼ë¯€ë¡œ)
    extracted_depts.reverse()
    
    return cleaned_title, extracted_depts

# ----------------------- ë³¸ë¬¸ ë¼ì¸ ì¬êµ¬ì„± -----------------------

def rebuild_main_content_lines(page, cut_y: float, last_col_start: float) -> List[Tuple[float, str]]:
    """
    ë³¸ë¬¸ ì˜ì—­ë§Œ ë¼ì¸ ì¬êµ¬ì„± (ë¶€ì„œì—´ ì™„ì „ ë°°ì œ)
    ì•ˆì „ ê²½ê³„: last_col_start + MAIN_BOUNDARY_OFFSET ì´í•˜ë§Œ ì‚¬ìš©
    """
    words = page.get_text("words", sort=True)
    if not words:
        return []
    
    main_boundary = last_col_start + MAIN_BOUNDARY_OFFSET
    
    # ë³¸ë¬¸ ì˜ì—­ wordsë§Œ ìˆ˜ì§‘
    buf = []
    for w in words:
        if len(w) < 5: 
            continue
        x0, y0, x1, y1, t = w[:5]
        
        # í—¤ë” ì»·
        if cut_y > 0 and y0 < cut_y - 1.5:
            continue
            
        # ë³¸ë¬¸ ì˜ì—­ë§Œ (ë¶€ì„œì—´ ì™„ì „ ë°°ì œ)
        if x1 > main_boundary:
            continue
            
        t = (t or "").strip()
        if not t: 
            continue
        buf.append((x0, y0, x1, y1, t))

    if not buf: 
        return []

    # y ê¸°ì¤€ ì •ë ¬ í›„ ë¼ì¸ í´ëŸ¬ìŠ¤í„°ë§
    buf.sort(key=lambda z: (round(z[1], 1), z[0]))
    lines_words, cur = [], [buf[0]]
    
    for w in buf[1:]:
        if abs(w[1] - cur[-1][1]) <= 3.5:
            cur.append(w)
        else:
            lines_words.append(cur)
            cur = [w]
    lines_words.append(cur)

    # ê° ë¼ì¸ì„ x ì •ë ¬í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±
    lines = []
    for line in lines_words:
        line.sort(key=lambda z: z[0])
        y0 = min(z[1] for z in line)
        text = " ".join(z[4] for z in line)
        lines.append((y0, text))

    # í—¤ë”ì„± ë¼ì¸ ë° êµ¬ë¶„ì„  ì œê±°
    filtered = []
    for y, s in lines:
        st = s.strip()
        if not st: 
            continue
        if HEADER_KEYWORDS.search(st) and not st.lstrip().startswith("â—‹"):
            continue
        if re.match(r'^[â”‚â”ƒâ”Œâ”â””â”˜â”œâ”¤â”¬â”´â”¼â”€â”\s]+$', st):  # í‘œ êµ¬ë¶„ì„ 
            continue
        if re.match(r'^\s*-?\s*\d{1,3}\s*-?\s*$', st):  # í˜ì´ì§€ ë²ˆí˜¸
            continue
        filtered.append((y, st))

    # ë¼ì¸ ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for y, st in filtered:
        key = (re.sub(r'\s+', ' ', st), round(y, 1))
        if key in seen: 
            continue
        seen.add(key)
        uniq.append((y, st))

    # ì²« ë²ˆì§¸ 'â—‹' ë¼ì¸ ì´ì „ ì¶”ê°€ ì œê±°
    bullet_ys = [y for y, s in uniq if s.lstrip().startswith("â—‹")]
    if bullet_ys:
        cut2 = min(bullet_ys) - 1.5
        uniq = [(y, s) for y, s in uniq if y >= cut2]

    return uniq

# ----------------------- Yì¶• ê¸°ë°˜ ë¸”ë¡-ë¶€ì„œ ë§¤ì¹­ ì‹œìŠ¤í…œ -----------------------

def build_blocks_with_y_ranges(page, last_col_start: float) -> List[Dict]:
    """
    ë°˜í™˜: [{'text': block_text, 'y_top': float, 'y_bottom': float}]
    - ë³¸ë¬¸ì—´(x1 <= last_col_start - margin)ë§Œ ì‚¬ìš©í•´ ë¸”ë¡ yë²”ìœ„ ê³„ì‚°
    - ë¸”ë¡ ê²½ê³„ëŠ” 'â—‹' í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ wordsë¥¼ ê·¸ë£¹í™”
    """
    words = page.get_text("words", sort=True)  # (x0,y0,x1,y1,txt, ...)
    if not words: 
        return []

    main_max_x = last_col_start + MAIN_BOUNDARY_OFFSET
    
    # ë³¸ë¬¸ì˜ì—­ ë‹¨ì–´ë§Œ ì¶”ë ¤ì„œ â—‹ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í•‘
    main_words = []
    for w in words:
        if len(w) < 5:
            continue
        x0, y0, x1, y1, txt = w[:5]
        if x1 <= main_max_x and txt and txt.strip():
            main_words.append((x0, y0, x1, y1, txt.strip()))
    
    if not main_words:
        return []

    # 'â—‹' í† í°ì˜ ì‹œì‘ ì¸ë±ìŠ¤ ìˆ˜ì§‘
    bullet_indices = []
    for i, w in enumerate(main_words):
        if "â—‹" in w[4]:
            bullet_indices.append(i)
    
    if not bullet_indices:
        return []

    # ë¸”ë¡ë³„ ê·¸ë£¹ ìƒì„±
    blocks = []
    for j, start in enumerate(bullet_indices):
        end = bullet_indices[j+1] if j+1 < len(bullet_indices) else len(main_words)
        
        chunk = main_words[start:end]
        if not chunk:
            continue
            
        # í…ìŠ¤íŠ¸ êµ¬ì„± (yì¶• ì •ë ¬ í›„ xì¶• ì •ë ¬)
        chunk_by_lines = {}
        for w in chunk:
            y_key = round(w[1], 1)  # y ì¢Œí‘œë¥¼ í‚¤ë¡œ ì‚¬ìš©
            if y_key not in chunk_by_lines:
                chunk_by_lines[y_key] = []
            chunk_by_lines[y_key].append(w)
        
        # ë¼ì¸ë³„ë¡œ x ì •ë ¬í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±
        sorted_lines = []
        for y_key in sorted(chunk_by_lines.keys()):
            line_words = sorted(chunk_by_lines[y_key], key=lambda w: w[0])
            line_text = " ".join(w[4] for w in line_words)
            sorted_lines.append(line_text)
        
        chunk_text = "\n".join(sorted_lines)
        
        # ì„¸ë¡œ ë²”ìœ„ ê³„ì‚°
        y_top = min(w[1] for w in chunk)
        y_bottom = max(w[3] for w in chunk)

        blocks.append({
            "text": chunk_text,
            "y_top": y_top,
            "y_bottom": y_bottom,
        })
    
    return blocks

def extract_dept_rows(page, last_col_start: float, known_departments: Set[str]) -> List[Dict]:
    """
    ë°˜í™˜: [{'y_center': float, 'raw': 'ì›ì‹œí–‰í…ìŠ¤íŠ¸', 'depts': ['ì‹œì„¤ê´€ë¦¬ì‚¬ì—…ì†Œ','ê´€ê´‘ì§„í¥ê³¼', ...]}]
    - last_col_start + margin ë³´ë‹¤ xì¤‘ì‹¬ì´ í° ë‹¨ì–´ë§Œ ìˆ˜ì§‘í•˜ì—¬ yê¸°ë°˜ í–‰ìœ¼ë¡œ ë¬¶ìŒ
    - í† í°ì„ 1/2/3-gramìœ¼ë¡œ ê²°í•© â†’ KNOWN_DEPARTMENTS êµì°¨
    - í—¤ë”/ì¡ìŒ ë¼ì¸ì€ ë²„ë¦¼
    """
    words = page.get_text("words", sort=True)
    if not words:
        return []

    dept_min_x = last_col_start + DEPT_BOUNDARY_OFFSET
    
    # ë§ˆì§€ë§‰ ì—´ í›„ë³´ ë‹¨ì–´ ìˆ˜ì§‘
    dept_candidates = []
    for w in words:
        if len(w) < 5: 
            continue
        x0, y0, x1, y1, txt = w[:5]
        if not txt or not txt.strip():
            continue
        x_center = (x0 + x1) / 2.0
        if x_center >= dept_min_x:
            dept_candidates.append((x0, y0, x1, y1, txt.strip()))

    if not dept_candidates:
        return []

    # yë¡œ ì •ë ¬ í›„ ê°™ì€ ì¤„ í´ëŸ¬ìŠ¤í„°ë§ (Â±8pt)
    dept_candidates.sort(key=lambda z: z[1])
    rows, current_row = [], [dept_candidates[0]]
    
    for candidate in dept_candidates[1:]:
        if abs(candidate[1] - current_row[-1][1]) <= 8.0:
            current_row.append(candidate)
        else:
            rows.append(current_row)
            current_row = [candidate]
    rows.append(current_row)

    # í–‰ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë§Œë“¤ê³  ë¶€ì„œ í›„ë³´ ìƒì„±
    result = []
    for row in rows:
        row.sort(key=lambda z: z[0])  # x ì •ë ¬
        y_center = sum((z[1] + z[3]) / 2.0 for z in row) / len(row)

        raw_text = " ".join(z[4] for z in row)
        
        # í—¤ë”/ì¡ìŒ ì°¨ë‹¨
        if NOISE_KEYWORDS.search(raw_text):
            result.append({"y_center": y_center, "raw": raw_text, "depts": []})
            continue

        # ì½¤ë§ˆ/êµ¬ë¶„ì ì •ë¦¬ + ì „ë¶€ì„œ/ì „ë™ ì •ê·œí™”
        cleaned = re.sub(r'[,\u00B7Â·/]+', ' ', raw_text)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = normalize_spacing_for_departments(cleaned).strip()

        # í† í° ë‚˜ëˆ” í›„ 1~3ê·¸ë¨ í›„ë³´ ìƒì„± â†’ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ êµì°¨
        tokens = cleaned.split()
        candidate_names = set()

        # 1-gram ë§¤ì¹­
        for token in tokens:
            if token in known_departments:
                candidate_names.add(token)

        # 2-gram, 3-gram (ê³µë°± ì œê±° ê²°í•©)
        for n in (2, 3):
            for i in range(len(tokens) - n + 1):
                name = "".join(tokens[i:i+n])
                if name in known_departments:
                    candidate_names.add(name)

        # ì›ë¬¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        final_depts = []
        for dept in candidate_names:
            if dept not in final_depts:
                final_depts.append(dept)
        
        result.append({
            "y_center": y_center,
            "raw": raw_text,
            "depts": final_depts
        })

    return result

def find_departments_from_tokens(tokens: List[str], known_depts: Set[str]) -> List[str]:
    """
    í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ n-gram ì¡°í•©ì„ í†µí•´ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¶€ì„œ ì°¾ê¸°
    ì˜ˆ: ["ì‹œì„¤ê´€", "ë¦¬ì‚¬ì—…ì†Œ"] â†’ "ì‹œì„¤ê´€ë¦¬ì‚¬ì—…ì†Œ"
    """
    found = []
    used_indices = set()
    
    # ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë¶€ì„œëª…ë“¤ (ê¸´ ê²ƒë¶€í„° ë§¤ì¹­)
    sorted_depts = sorted(known_depts, key=len, reverse=True)
    
    for dept in sorted_depts:
        # 1-gram ì§ì ‘ ë§¤ì¹­
        for i, token in enumerate(tokens):
            if i in used_indices:
                continue
            if token.lower() == dept.lower():
                found.append(dept)
                used_indices.add(i)
                break
        else:
            # n-gram ì¡°í•© ë§¤ì¹­ (2~4-gram)
            for n in range(2, min(5, len(tokens) + 1)):
                for combo_indices in combinations(range(len(tokens)), n):
                    if any(i in used_indices for i in combo_indices):
                        continue
                    
                    combined = "".join(tokens[i] for i in combo_indices)
                    if combined.lower() == dept.lower():
                        found.append(dept)
                        used_indices.update(combo_indices)
                        break
                if dept in found:
                    break
    
    return found

# ----------------------- í…ìŠ¤íŠ¸ ì²˜ë¦¬ -----------------------

def minimal_text_cleanup(text: str) -> str:
    """ìµœì†Œí•œì˜ í…ìŠ¤íŠ¸ ì •ì œ (ì˜ë¯¸ ë³€ê²½ ë°©ì§€)"""
    # ê¸°ë³¸ ë‹¨ìœ„ ê²°í•©
    text = re.sub(r'(\d+)\s*ì›”', r'\1ì›”', text)
    text = re.sub(r'(\d+)\s*ì¼', r'\1ì¼', text)
    text = re.sub(r'(\d+)\s*%', r'\1%', text)
    text = re.sub(r'ì œ\s*(\d+)\s*íšŒ', r'ì œ\1íšŒ', text)
    
    # ë¶€ì„œ ë„ì–´ì“°ê¸° ì •ê·œí™”
    text = normalize_spacing_for_departments(text)
    
    # ì¤‘ë³µ êµ¬ë‘ì  ì¶•ì†Œ
    text = re.sub(r',,+', ',', text)
    text = re.sub(r'\.\.+', '.', text)
    text = re.sub(r'\s{2,}', ' ', text)
    
    return text.strip()

def extract_deadline_from_text(text: str) -> str:
    """ë‚ ì§œ íŒ¨í„´ì—ì„œ ì²˜ë¦¬ê¸°í•œ ì¶”ì¶œ"""
    m = DATE_RX.search(text)
    if not m: 
        return ""
    
    y, mth, d = m.group('y'), m.group('m'), m.group('d')
    y = y.replace(',', '')
    return f"{int(y)}. {int(mth)}. {int(d)}."

def remove_all_dates_from_text(text: str) -> str:
    """ëª¨ë“  ë‚ ì§œ íŒ¨í„´ ì œê±°"""
    cleaned = text
    for pattern in ALL_DATE_PATTERNS:
        cleaned = pattern.sub(' ', cleaned)
    cleaned = re.sub(r'\s{2,}', ' ', cleaned)
    cleaned = re.sub(r'\.{2,}', '.', cleaned)
    return cleaned.strip()

# ----------------------- ë¸”ë¡ ì²˜ë¦¬ -----------------------

def merge_lines_to_text(lines: List[Tuple[float, str]]) -> str:
    """ë¼ì¸ë“¤ì„ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ë¡œ ë³‘í•©"""
    if not lines:
        return ""
    
    merged_lines = []
    current_line = ""
    
    for _, text in lines:
        text = text.strip()
        if not text:
            continue
            
        if current_line:
            # ë¬¸ì¥ ëì´ë©´ ê°œí–‰, ì•„ë‹ˆë©´ ê³µë°±ìœ¼ë¡œ ì—°ê²°
            if re.search(r'[.?!ë‹¤]\s*$', current_line):
                merged_lines.append(current_line)
                current_line = text
            else:
                current_line += " " + text
        else:
            current_line = text
    
    if current_line:
        merged_lines.append(current_line)
    
    text = "\n".join(merged_lines)
    return minimal_text_cleanup(text)

def split_blocks_by_bullet(text: str) -> List[str]:
    """'â—‹' ê¸°ì¤€ìœ¼ë¡œ ë¸”ë¡ ë¶„ë¦¬"""
    if not text:
        return []
        
    parts = re.split(r'(?=^\s*â—‹)', text, flags=re.M)
    blocks = []
    
    for part in parts:
        part = part.strip()
        if not part.startswith("â—‹"):
            continue
        if len(part) < 8:
            continue
        blocks.append(part)
    
    return blocks

def process_block_content(block_text: str) -> Tuple[str, str, List[str]]:
    """
    ë¸”ë¡ì—ì„œ ì œëª©/ë³¸ë¬¸ ë¶„ë¦¬ ë° ì œëª© ê¼¬ë¦¬ ë¶€ì„œ ì¶”ì¶œ
    ë°˜í™˜: (ì œëª©, ë³¸ë¬¸, ì œëª©ì—ì„œ_ì¶”ì¶œëœ_ë¶€ì„œë“¤)
    """
    lines = block_text.splitlines()
    if not lines:
        return "", "", []
    
    # ì œëª© ì²˜ë¦¬: ì²« ì¤„ì—ì„œ 'â—‹' ì œê±° í›„ ë¶€ì„œ ì¶”ì¶œ
    title_raw = lines[0].lstrip().lstrip('â—‹').strip()
    clean_title, title_depts = strip_trailing_departments_from_title(title_raw, KNOWN_DEPARTMENTS)
    
    # ë³¸ë¬¸ ì²˜ë¦¬: ë‚˜ë¨¸ì§€ ì¤„ë“¤
    body_lines = [ln.strip() for ln in lines[1:] if ln.strip()]
    body = " ".join(body_lines).strip()
    
    if body:
        # ë¬¸ì¥ ë‹¨ìœ„ ê°œí–‰ ì •ë¦¬
        body = re.sub(r'\s*([.?!])\s*', r'\1\n', body)
        body = re.sub(r'\n{3,}', '\n\n', body).strip()
        if not body.startswith('-'):
            body = '- ' + body
    
    return clean_title, body, title_depts

# ----------------------- ë¶€ì„œ ë§¤ì¹­ ë¡œì§ -----------------------

def measure_block_y_ranges(page, main_lines: List[Tuple[float, str]], blocks: List[str]) -> List[Tuple[float, float]]:
    """ê° ë¸”ë¡ì˜ y ë²”ìœ„ ì¶”ì •"""
    bullet_lines = [(y, s) for (y, s) in main_lines if s.lstrip().startswith("â—‹")]
    starts = [y for y, _ in bullet_lines]
    
    ranges = []
    for i, y_start in enumerate(starts):
        if i < len(starts) - 1:
            y_end = starts[i + 1] - 0.1
        else:
            y_end = page.rect.height - 5
        ranges.append((y_start, y_end))
    
    # ë¸”ë¡ ìˆ˜ì™€ ë§ì¶¤
    while len(ranges) < len(blocks):
        if ranges:
            ranges.append((ranges[-1][0], ranges[-1][1]))
        else:
            ranges.append((0.0, page.rect.height))
    
    return ranges[:len(blocks)]

def assign_departments_by_y(blocks: List[Dict], dept_rows: List[Dict], known_departments: Set[str]) -> List[List[str]]:
    """
    ê° ë¸”ë¡ì— ëŒ€í•´:
      1) y-overlap ìˆëŠ” í–‰ë“¤ì˜ ë¶€ì„œë¥¼ ëª¨ë‘ ìˆ˜ì§‘
      2) ì—†ìœ¼ë©´ y_centerê°€ ê°€ì¥ ê°€ê¹Œìš´ í–‰ 1ê°œ ì„ íƒ
      3) ì¤‘ë³µ ì œê±° + KNOWN_DEPARTMENTS êµì°¨(ë§ˆì§€ë§‰ ë°©ì–´)
    ë°˜í™˜: blocksì™€ ê°™ì€ ì¸ë±ìŠ¤ ìˆœì„œì˜ [ë¶€ì„œë¦¬ìŠ¤íŠ¸]
    """
    results = []
    for block in blocks:
        y_top, y_bottom = block["y_top"], block["y_bottom"]
        matched_depts = []
        
        # 1) Y-overlap ì²´í¬ (ê²¹ì¹˜ëŠ” ë¶€ì„œí–‰ë“¤ì˜ ë¶€ì„œ ìˆ˜ì§‘)
        overlap_found = False
        for row in dept_rows:
            y_center = row["y_center"]
            # ë¸”ë¡ Y ë²”ìœ„ì™€ ë¶€ì„œí–‰ Yê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (Â±5pt í—ˆìš©)
            if y_top - 5 <= y_center <= y_bottom + 5:
                matched_depts.extend(row["depts"])
                overlap_found = True
        
        # 2) Fallback: ê°€ì¥ ê°€ê¹Œìš´ ë¶€ì„œí–‰ ì„ íƒ (overlapì´ ì—†ì„ ë•Œ)
        if not matched_depts and dept_rows:
            nearest_row = min(dept_rows, key=lambda r: min(
                abs(r["y_center"] - y_top), 
                abs(r["y_center"] - y_bottom)
            ))
            matched_depts.extend(nearest_row["depts"])
        
        # 3) ì •ë¦¬: ì¤‘ë³µ ì œê±° + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
        unique_depts = []
        seen = set()
        for dept in matched_depts:
            if dept in known_departments and dept not in seen:
                seen.add(dept)
                unique_depts.append(dept)
        
        results.append(unique_depts)
    
    return results

def merge_department_lists(column_depts: List[str], title_depts: List[str]) -> List[str]:
    """
    ë¶€ì„œì—´ ë¶€ì„œ + ì œëª© ë¶€ì„œ í†µí•© ë° í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìµœì¢… í•„í„°ë§
    """
    all_depts = column_depts + title_depts
    
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
    filtered_depts = [d for d in all_depts if d in KNOWN_DEPARTMENTS]
    
    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
    seen = set()
    final_depts = []
    for dept in filtered_depts:
        if dept not in seen:
            seen.add(dept)
            final_depts.append(dept)
    
    return final_depts

# ----------------------- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¡œê¹… -----------------------

def validate_and_log_block(block_idx: int, title: str, depts: List[str], deadline: str):
    """ë¸”ë¡ ë‹¨ìœ„ ê²€ì¦ ë° ë¡œê¹…"""
    warnings = []
    
    # 1) ì œëª©ì— ë¶€ì„œ ì ‘ë¯¸ì‚¬ ë‚¨ìŒ ì²´í¬
    org_suffixes = ["ê³¼", "ì†Œ", "êµ­", "ì‹¤", "ê´€", "ì„¼í„°", "ì‚¬ì—…ì†Œ", "íŒ€", "ë‹¨"]
    for suffix in org_suffixes:
        if title.strip().endswith(suffix):
            warnings.append(f"ì œëª© ëì— '{suffix}' ì ‘ë¯¸ì‚¬ ë‚¨ìŒ")
            break
    
    # 2) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°– ë¶€ì„œ ì²´í¬
    invalid_depts = [d for d in depts if d not in KNOWN_DEPARTMENTS]
    if invalid_depts:
        warnings.append(f"í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°– ë¶€ì„œ: {invalid_depts}")
    
    # ë¡œê¹…
    log.info(f"Block {block_idx:2d}: ì œëª©=[{title[:50]}{'...' if len(title)>50 else ''}]")
    log.info(f"           ë¶€ì„œ={depts} ê¸°í•œ=[{deadline}]")
    
    if warnings:
        log.warning(f"           ê²½ê³ : {' | '.join(warnings)}")

# ----------------------- ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ -----------------------

def process_pdf_with_whitelist(pdf_path: str) -> Tuple[List[Dict[str, Any]], str]:
    """í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ PDF ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    doc = fitz.open(pdf_path)
    source = Path(pdf_path).name
    log.info(f"Processing {source} ({doc.page_count} pages)")

    all_records = []
    all_pretty_blocks = []

    for page_num, page in enumerate(doc, start=1):
        raw_text = page.get_text("text")
        category = detect_page_category(raw_text)

        # ì—´ êµ¬ì¡° ë¶„ì„
        col_edges, last_col_start = detect_column_edges(page)
        cut_y = find_first_circle_y(page)
        
        log.info(f"Page {page_num}: last_col_start={last_col_start:.1f}pt, cut_y={cut_y:.1f}pt")

        # Yì¶• ê¸°ë°˜ ë¸”ë¡-ë¶€ì„œ ë§¤ì¹­ ì‹œìŠ¤í…œ ì‚¬ìš©
        blocks_with_y = build_blocks_with_y_ranges(page, last_col_start)
        if not blocks_with_y:
            continue

        # ë¶€ì„œì—´ì—ì„œ ë¶€ì„œ ì¶”ì¶œ (Yì¶• ê¸°ë°˜)
        dept_rows = extract_dept_rows(page, last_col_start, KNOWN_DEPARTMENTS)

        # Yì¶• ì¢Œí‘œ ê¸°ë°˜ ë¶€ì„œ-ë¸”ë¡ ë§¤ì¹­
        column_depts_per_block = assign_departments_by_y(blocks_with_y, dept_rows, KNOWN_DEPARTMENTS)

        # ê° ë¸”ë¡ ì²˜ë¦¬
        for block_idx, block_data in enumerate(blocks_with_y, start=1):
            block_text = block_data["text"]
            y_info = f"Y:{block_data['y_top']:.1f}-{block_data['y_bottom']:.1f}"
            
            log.info(f"Block {block_idx}: {y_info}")
            # ì²˜ë¦¬ê¸°í•œ ì¶”ì¶œ
            deadline = extract_deadline_from_text(block_text)
            
            # ë‚ ì§œ ì œê±°ëœ ë¸”ë¡ìœ¼ë¡œ ì œëª©/ë³¸ë¬¸ ë¶„ë¦¬
            clean_block = remove_all_dates_from_text(block_text)
            title, body, title_depts = process_block_content(clean_block)
            
            # ìµœì¢… ë‚ ì§œ ì œê±°
            title = remove_all_dates_from_text(title)
            body = remove_all_dates_from_text(body)
            
            # ìµœì†Œ í…ìŠ¤íŠ¸ ì •ì œ
            title = minimal_text_cleanup(title)
            body = minimal_text_cleanup(body)

            # ë¶€ì„œ í†µí•© (ë¶€ì„œì—´ + ì œëª©)
            column_depts = column_depts_per_block[block_idx - 1] if block_idx - 1 < len(column_depts_per_block) else []
            final_depts = merge_department_lists(column_depts, title_depts)

            # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¡œê¹…
            validate_and_log_block(block_idx, title, final_depts, deadline)

            # TXT í¬ë§· ìƒì„±
            txt_parts = [f"â—‹ {title}"]
            if body:
                txt_parts.append(body)
            if deadline:
                txt_parts.append(f"ì²˜ë¦¬ê¸°í•œ: {deadline}")
            if final_depts:
                txt_parts.append("ë¶€ì„œ: " + ", ".join(final_depts))
            
            pretty_block = "\n".join(txt_parts)
            all_pretty_blocks.append(pretty_block)

            # JSON ë ˆì½”ë“œ ìƒì„±
            record = {
                "source_file": source,
                "page": page_num,
                "index": block_idx,
                "category": category,
                "title": title,
                "body": body.replace("- ", "").replace("\n", " ").strip() if body else "",
                "deadline": deadline,
                "departments": final_depts,
                "lang": "ko",
                "doc_type": "gucheong_jisisa",
                "directive": clean_block
            }
            all_records.append(record)

    doc.close()

    # ì¤‘ë³µ ì œê±° (í…ìŠ¤íŠ¸ ê¸°ë°˜)
    seen = set()
    unique_records = []
    for r in all_records:
        key = f"{r.get('title', '')}||{r.get('body', '')}||{','.join(r.get('departments', []))}"
        key = re.sub(r'\s+', ' ', key)[:500]
        if key not in seen:
            seen.add(key)
            unique_records.append(r)

    seen_blocks = set()
    unique_blocks = []
    for block in all_pretty_blocks:
        key = re.sub(r'\s+', ' ', block)[:500]
        if key not in seen_blocks:
            seen_blocks.add(key)
            unique_blocks.append(block)

    pretty_text = "\n\n".join(unique_blocks)
    return unique_records, pretty_text

# ----------------------- ì €ì¥ ë° CLI -----------------------

def save_results(records: List[Dict], pretty_text: str, pdf_path: str):
    """ê²°ê³¼ íŒŒì¼ ì €ì¥"""
    stem = Path(pdf_path).with_suffix("")
    
    jsonl_path = f"{stem}_whitelist.jsonl"
    txt_path = f"{stem}_whitelist.txt"
    
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(pretty_text + ("\n" if not pretty_text.endswith("\n") else ""))
    
    log.info(f"Saved: {jsonl_path} ({len(records)} records)")
    log.info(f"Saved: {txt_path} ({len(pretty_text)} chars)")
    
    return jsonl_path, txt_path

def print_validation_summary(records: List[Dict]):
    """ìµœì¢… ê²€ì¦ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*50)
    print("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ì„œ ì¶”ì¶œ ê²€ì¦ ê²°ê³¼")
    print("="*50)
    
    total_blocks = len(records)
    blocks_with_depts = sum(1 for r in records if r.get('departments'))
    blocks_with_deadline = sum(1 for r in records if r.get('deadline'))
    
    # ì œëª© ë ë¶€ì„œ ì ‘ë¯¸ì‚¬ ì²´í¬
    org_suffixes = ["ê³¼", "ì†Œ", "êµ­", "ì‹¤", "ê´€", "ì„¼í„°", "ì‚¬ì—…ì†Œ", "íŒ€", "ë‹¨"]
    title_suffix_issues = 0
    for r in records:
        title = r.get('title', '').strip()
        if any(title.endswith(suffix) for suffix in org_suffixes):
            title_suffix_issues += 1
    
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°– ë¶€ì„œ ì²´í¬
    invalid_dept_issues = 0
    for r in records:
        depts = r.get('departments', [])
        if any(d not in KNOWN_DEPARTMENTS for d in depts):
            invalid_dept_issues += 1
    
    print(f"âœ“ ì´ ì¶”ì¶œ ë¸”ë¡: {total_blocks}ê°œ")
    print(f"âœ“ ë¶€ì„œ ìˆëŠ” ë¸”ë¡: {blocks_with_depts}ê°œ")
    print(f"âœ“ ì²˜ë¦¬ê¸°í•œ ìˆëŠ” ë¸”ë¡: {blocks_with_deadline}ê°œ")
    print(f"âœ“ ì œëª© ë ë¶€ì„œ ì ‘ë¯¸ì‚¬ ë‚¨ìŒ: {title_suffix_issues}ê°œ (0ì´ì–´ì•¼ í•¨)")
    print(f"âœ“ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°– ë¶€ì„œ: {invalid_dept_issues}ê°œ (0ì´ì–´ì•¼ í•¨)")
    print(f"âœ“ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¶€ì„œ ìˆ˜: {len(KNOWN_DEPARTMENTS)}ê°œ")
    
    if title_suffix_issues == 0 and invalid_dept_issues == 0:
        print("\nğŸ‰ ëª¨ë“  ê²€ì¦ í†µê³¼!")
    else:
        print(f"\nâš ï¸  ê²€ì¦ ì‹¤íŒ¨: {title_suffix_issues + invalid_dept_issues}ê°œ ì´ìŠˆ")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python directive_extractor_whitelist_final.py <PDF_íŒŒì¼_ê²½ë¡œ>")
        sys.exit(1)
    
    pdf_file = sys.argv[1]
    if not os.path.exists(pdf_file):
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file}")
        sys.exit(1)
    
    print(f"í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_file}")
    print(f"í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¶€ì„œ ìˆ˜: {len(KNOWN_DEPARTMENTS)}ê°œ")
    print(f"ì•ˆì „ ê²½ê³„ê°’: ë³¸ë¬¸({MAIN_BOUNDARY_OFFSET}pt), ë¶€ì„œì—´({DEPT_BOUNDARY_OFFSET}pt)")
    
    # ë©”ì¸ ì²˜ë¦¬
    records, pretty_text = process_pdf_with_whitelist(pdf_file)
    
    # ê²°ê³¼ ì €ì¥
    jsonl_path, txt_path = save_results(records, pretty_text, pdf_file)
    
    # ê²€ì¦ ìš”ì•½
    print_validation_summary(records)
    
    print(f"\nì™„ë£Œ!")
    print(f"JSONL: {jsonl_path}")
    print(f"TXT: {txt_path}")
    
    # ë¯¸ë¦¬ë³´ê¸°
    if pretty_text:
        print(f"\n--- ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ---")
        lines = pretty_text.split("\n\n")
        if lines:
            preview = lines[0][:300]
            if len(lines[0]) > 300:
                preview += "..."
            print(preview)