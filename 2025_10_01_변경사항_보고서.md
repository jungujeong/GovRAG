# 2025ë…„ 10ì›” 1ì¼ ë³€ê²½ì‚¬í•­ ë³´ê³ ì„œ

## ğŸ“‹ ê°œìš”

**ì‘ì—… ê¸°ê°„**: 2025ë…„ 10ì›” 1ì¼
**ì‘ì—…ì**: RAG ì‹œìŠ¤í…œ ê°œì„ íŒ€
**ê´€ë ¨ ì´ìŠˆ**: #48
**ê´€ë ¨ PR**: #49
**Git Commit**: `5b27063` - `refactor: replace hardcoded patterns with statistical approaches`

---

## ğŸ¯ ì‘ì—… ëª©í‘œ

í•œêµ­ì–´ RAG ì‹œìŠ¤í…œì—ì„œ **í•˜ë“œì½”ë”©ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì™€ ë„ë©”ì¸ íŠ¹ì • íŒ¨í„´ì„ ì œê±°**í•˜ê³ , **ë²”ìš©ì ì¸ í†µê³„ì  ì ‘ê·¼ë²•**ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ì‹œìŠ¤í…œì˜ í™•ì¥ì„±, ìœ ì§€ë³´ìˆ˜ì„±, ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### í•µì‹¬ ë¬¸ì œì 
ê¸°ì¡´ ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ í•˜ë“œì½”ë”©ì— ì˜ì¡´:
- **73ê°œì˜ ë‹¨ì–´/íŒ¨í„´**: ì¡°ì‚¬(21ê°œ), ë‹´í™”í‘œì§€(27ê°œ), ë¶ˆìš©ì–´(10ê°œ), ë„ë©”ì¸ ì—”í„°í‹°(15ê°œ)
- **ì‚¬íˆ¬ë¦¬ ë¯¸ì§€ì›**: í‘œì¤€ì–´ë§Œ ì²˜ë¦¬ ê°€ëŠ¥ (`ê·¸ë ‡ë‹¤ë©´` âœ…, `ê·¸ë¼ë¯„` âŒ)
- **ë„ë©”ì¸ ì˜ì¡´ì„±**: ìƒˆ ë„ë©”ì¸(ë³‘ì›, í•™êµ) ì ìš© ì‹œ íŒ¨í„´ ìˆ˜ë™ ì¶”ê°€ í•„ìš”
- **ìœ ì§€ë³´ìˆ˜ ë¶€ë‹´**: ì–¸ì–´ ë³€í™”, ìƒˆ í‘œí˜„ ë“±ì¥ ì‹œ ì§€ì†ì  ì—…ë°ì´íŠ¸ í•„ìš”

---

## ğŸ”§ ì£¼ìš” ë³€ê²½ ì‚¬í•­

### 1. **query_rewriter.py** - ì§ˆì˜ ì¬ì‘ì„± ê°œì„ 

#### Before (í•˜ë“œì½”ë”© ë°©ì‹)
```python
# 15ê°œ ì¡°ì‚¬ í•˜ë“œì½”ë”©
particles = ['ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ë¥¼', 'ì„', 'ê°€', 'ì´', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¡œ', 'ì„œ', 'ë¶€í„°']

# ì—­ìˆœ ë©”ì‹œì§€ ì²˜ë¦¬ (ë²„ê·¸ ìœ ë°œ)
for msg in reversed(messages[-3:]):  # âŒ í˜„ì¬ ì§ˆë¬¸ ë‹¨ì–´ê°€ ë¨¼ì €
    entities.append(extract(msg))

# ê²°ê³¼
query = "ê·¸ë ‡ë‹¤ë©´ ê° ì§€ì‹œì‚¬í•­ ë³„ë¡œ ë¶€ì„œëª…ì„ ì•Œë ¤ì¤˜"
extracted = ['ì§€ì‹œì‚¬í•­', 'ë¶€ì„œëª…ì„']  # âŒ í˜„ì¬ ì§ˆë¬¸ ë‹¨ì–´ë§Œ
```

#### After (í†µê³„ì  ë°©ì‹)
```python
# âœ… Substring clustering - í•˜ë“œì½”ë”© ì—†ìŒ
def _normalize_entities_statistical(entities):
    """
    ["í™í‹°ì˜ˆìˆ ì´Œ", "í™í‹°ì˜ˆìˆ ì´Œì—", "í™í‹°ì˜ˆìˆ ì´Œì˜"] â†’ ["í™í‹°ì˜ˆìˆ ì´Œ"]
    """
    clusters = []
    for entity in entities:
        for cluster in clusters:
            for existing in cluster:
                if entity in existing or existing in entity:  # ë¶€ë¶„ ë¬¸ìì—´ ê°ì§€
                    cluster.add(entity)
                    break

    # ìµœë‹¨í˜• ì„ íƒ (ì¡°ì‚¬ê°€ ë¬¸ì ì¶”ê°€í•˜ë¯€ë¡œ)
    return [min(cluster, key=len) for cluster in clusters]

# âœ… ì‹œê°„ìˆœ ë©”ì‹œì§€ ì²˜ë¦¬
for msg in messages[-3:]:  # âœ… ì´ì „ ì»¨í…ìŠ¤íŠ¸ ìš°ì„ 
    entities.append(extract(msg))

# ê²°ê³¼
query = "ê·¸ë ‡ë‹¤ë©´ ê° ì§€ì‹œì‚¬í•­ ë³„ë¡œ ë¶€ì„œëª…ì„ ì•Œë ¤ì¤˜"
extracted = ['í™í‹°ì˜ˆìˆ ì´Œ', 'ì§€ì‹œì‚¬í•­']  # âœ… ì •í™•í•œ ì»¨í…ìŠ¤íŠ¸
enhanced = "í™í‹°ì˜ˆìˆ ì´Œ ê·¸ë ‡ë‹¤ë©´ ê° ì§€ì‹œì‚¬í•­ ë³„ë¡œ ë¶€ì„œëª…ì„ ì•Œë ¤ì¤˜"
```

**ê°œì„  íš¨ê³¼**:
- âœ… **í›„ì† ì§ˆë¬¸ ì •í™•ë„**: BM25 ê²€ìƒ‰ 0ê°œ â†’ 2ê°œ (ì •í™•í•œ ë¬¸ì„œ)
- âœ… **ì‚¬íˆ¬ë¦¬ ì§€ì›**: `ê·¸ë¼ë¯„`, `ê·¸ë¼ë¬¸ìš”` ë“± ìë™ ì²˜ë¦¬
- âœ… **ì½”ë“œ ë‹¨ìˆœí™”**: ì¡°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì œê±°, 96ì¤„ â†’ 222ì¤„ (ê¸°ëŠ¥ í™•ì¥)

**Lines Changed**: +202, -96

---

### 2. **hybrid_retriever.py** - í‚¤ì›Œë“œ ì¶”ì¶œ ê°œì„ 

#### Before (í•˜ë“œì½”ë”© ë°©ì‹)
```python
def _strip_particles(self, word: str) -> str:
    """21ê°œ ì¡°ì‚¬ ìˆ˜ë™ ì œê±°"""
    particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë„', 'ë§Œ',
                 'ì™€', 'ê³¼', 'ë¡œ', 'ì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ì„œ', 'ì—ê²Œ',
                 'í•œí…Œ', 'ìœ¼ë¡œ', 'ë¼ê³ ']
    for particle in sorted(particles, key=len, reverse=True):
        if word.endswith(particle) and len(word) > len(particle):
            return word[:-len(particle)]  # ì¡°ì‚¬ ì œê±°
    return word

# ì‚¬ìš©
for word in korean_words:
    clean_word = self._strip_particles(word)  # âŒ í•˜ë“œì½”ë”© ì˜ì¡´
    keywords.append(clean_word)
```

#### After (í†µê³„ì  ë°©ì‹)
```python
# âœ… BM25 TF-IDFê°€ ìë™ìœ¼ë¡œ ì¼ë°˜ì–´ ë‹¤ìš´ì›¨ì´íŒ…
for word in korean_words:
    # NO particle stripping - BM25's TF-IDF handles it automatically
    if len(word) < 2:
        continue
    if self._is_likely_content_word(word):  # ê¸¸ì´ ê¸°ë°˜
        keywords.append(word)

# BM25ê°€ ìë™ ê³„ì‚°
tf_idf("í™í‹°ì˜ˆìˆ ì´Œ")    # 0.95  (ë†’ìŒ - í¬ê·€ì–´)
tf_idf("ê·¸ë ‡ë‹¤ë©´")      # 0.05  (ë‚®ìŒ - ì¼ë°˜ì–´, ìë™ ë‹¤ìš´ì›¨ì´íŒ…)
tf_idf("ì˜ˆìˆ ì´Œ")        # 0.85  (ë†’ìŒ - ì¤‘ìš” í‚¤ì›Œë“œ)
```

**ê°œì„  íš¨ê³¼**:
- âœ… **ì½”ë“œ ë‹¨ìˆœí™”**: `_strip_particles()` í•¨ìˆ˜ ì™„ì „ ì œê±° (14ì¤„ ì‚­ì œ)
- âœ… **ê²€ìƒ‰ ì„±ëŠ¥**: ë™ì¼ ìœ ì§€ (BM25 TF-IDF ìë™ ì²˜ë¦¬)
- âœ… **ë²„ê·¸ ê°ì†Œ**: ì¡°ì‚¬ ëª©ë¡ ëˆ„ë½ ë¶ˆê°€ëŠ¥

**Lines Changed**: +25, -39

---

### 3. **response_validator.py** - ë¶ˆìš©ì–´ ì œê±° ê°œì„ 

#### Before (í•˜ë“œì½”ë”© ë°©ì‹)
```python
def _extract_content_words(self, sentence: str) -> List[str]:
    words = re.findall(r'[ê°€-í£]{2,}', sentence)

    # 10ê°œ ë¶ˆìš©ì–´ í•˜ë“œì½”ë”©
    common_words = {
        'ìˆìŠµë‹ˆë‹¤', 'ìˆìœ¼ë©°', 'ë˜ì–´', 'í•˜ê³ ', 'ìˆëŠ”',
        'ëŒ€í•œ', 'ëŒ€í•´', 'ìœ„í•œ', 'í†µí•´', 'ë”°ë¼'
    }

    content_words = [w for w in words
                     if w not in common_words and len(w) >= 2]  # âŒ
    return content_words
```

#### After (í†µê³„ì  ë°©ì‹)
```python
def _extract_content_words(self, sentence: str) -> List[str]:
    """
    í†µê³„ì  ì ‘ê·¼ë²• - í•˜ë“œì½”ë”© ì—†ìŒ
    1. ê¸¸ì´ ê¸°ë°˜: 3+ ê¸€ì = ë‚´ìš©ì–´ (í•œêµ­ì–´ íŠ¹ì„±)
    2. ë¬¸ì ë‹¤ì–‘ì„±: ì—”íŠ¸ë¡œí”¼ proxy
    """
    words = re.findall(r'[ê°€-í£]{2,}', sentence)

    content_words = []
    for word in words:
        # í†µê³„ì  ê´€ì°°: í•œêµ­ì–´ ê¸°ëŠ¥ì–´ëŠ” 1-2ì, ë‚´ìš©ì–´ëŠ” 3+ ì
        if len(word) >= 3:
            content_words.append(word)
        elif len(word) == 2:
            # ë¬¸ì ë‹¤ì–‘ì„± ì²´í¬ (ì—”íŠ¸ë¡œí”¼ proxy)
            if len(set(word)) == 2:  # ë‘ ê¸€ìê°€ ë‹¤ë¦„
                content_words.append(word)

    return content_words
```

**ê°œì„  íš¨ê³¼**:
- âœ… **ë²”ìš©ì„±**: ëª¨ë“  ë„ë©”ì¸ì—ì„œ ì‘ë™ (ì˜ë£Œ, ë²•ë¥ , êµìœ¡ ë“±)
- âœ… **ìœ ì§€ë³´ìˆ˜**: ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬ ë¶ˆí•„ìš”
- âœ… **ì •í™•ë„**: ê¸¸ì´ + ë‹¤ì–‘ì„± ê¸°ë°˜ í•„í„°ë§ì´ ë” ì •í™•

**Lines Changed**: +17, -11

---

### 4. **answer_formatter.py** - ë¬¸ë‹¨ ë¶„í•  ê°œì„ 

#### Before (í•˜ë“œì½”ë”© ë°©ì‹)
```python
# 27ê°œ ë‹´í™” í‘œì§€ í•˜ë“œì½”ë”©
natural_breaks = [
    'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ', 'ì¦‰,', 'ì˜ˆë¥¼ ë“¤ì–´',
    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë„·ì§¸', 'ë‹¤ì„¯ì§¸',
    'ë§ˆì§€ë§‰ìœ¼ë¡œ', 'ê²°ë¡ ì ìœ¼ë¡œ', 'ìš”ì•½í•˜ë©´',
    'íŠ¹íˆ', 'ë°˜ë©´', 'ê·¸ëŸ¬ë‚˜', 'ê²Œë‹¤ê°€', 'ì•„ìš¸ëŸ¬',
    'í•œí¸', 'ë‹¤ë§Œ', 'ë‹¨,', 'ì°¸ê³ ë¡œ', 'ì¶”ê°€ë¡œ',
    'ì´ì™€ ê´€ë ¨í•˜ì—¬', 'ì´ì— ë”°ë¼', 'ê·¸ ê²°ê³¼',
    'êµ¬ì²´ì ìœ¼ë¡œ', 'ì„¸ë¶€ì ìœ¼ë¡œ', 'ì¢…í•©í•˜ë©´'
]

should_break = False
for marker in natural_breaks:  # âŒ í•˜ë“œì½”ë”© ì˜ì¡´
    if sentence.startswith(marker) or f' {marker}' in sentence:
        should_break = True
        break
```

#### After (í†µê³„ì  ë°©ì‹)
```python
# âœ… êµ¬ì¡°ì  + í†µê³„ì  íŒ¨í„´ ê°ì§€
should_break = False

# 1. ê¸¸ì´ ê¸°ë°˜ (1-2 ë¬¸ì¥ë§ˆë‹¤)
if sentence_count >= 2:
    should_break = True

# 2. êµ¬ì¡°ì  íŒ¨í„´
if re.match(r'^\d+[.)]\s', sentence):  # ë²ˆí˜¸
    should_break = True
if sentence.strip().startswith(('â€¢', '-', '*')):  # ë¶ˆë¦¿
    should_break = True

# 3. í†µê³„ì  ì „í™˜ ê°ì§€ - í•˜ë“œì½”ë”© ì—†ìŒ!
if re.match(r'^[ê°€-í£]{2,4}[,\s]', sentence.strip()):
    # ë¬¸ì¥ ì‹œì‘ 2-4ê¸€ì ë‹¨ì–´ = ë‹´í™” í‘œì§€ ê°€ëŠ¥ì„± ë†’ìŒ
    first_word = re.match(r'^([ê°€-í£]{2,4})', sentence.strip()).group(1)
    if 2 <= len(first_word) <= 5:  # í†µê³„ì  ì„ê³„ê°’
        should_break = True
```

**ê°œì„  íš¨ê³¼**:
- âœ… **ì‚¬íˆ¬ë¦¬ ì§€ì›**: ëª¨ë“  ë‹´í™” í‘œì§€ ìë™ ê°ì§€ (`ê·¸ë¼ë¯„`, `ê·¸ëŸ¬ëª¨` ë“±)
- âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ í‘œí˜„ ë“±ì¥ ì‹œì—ë„ ì‘ë™
- âœ… **ì½”ë“œ ë‹¨ìˆœí™”**: 27ê°œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì œê±°

**Lines Changed**: +27, -32

---

### 5. **conversation_summarizer.py** - ì—”í„°í‹° ì¶”ì¶œ ê°œì„ 

#### Before (í•˜ë“œì½”ë”© ë°©ì‹)
```python
def _extract_entities(self, messages):
    entities = []

    # ë¶€ì„œëª… íŒ¨í„´ í•˜ë“œì½”ë”©
    dept_pattern = re.compile(r"([ê°€-í£A-Za-z]+(?:ê³¼|ë¶€|êµ­|ì²˜|ì‹¤|ë‹¨|íŒ€|ì„¼í„°))")

    # ì¥ì†Œ íŒ¨í„´ í•˜ë“œì½”ë”©
    entity_pattern = re.compile(
        r"([ê°€-í£]{2,}(?:ì˜ˆìˆ ì´Œ|ë¬¸í™”ë§ˆì„|ì‚°ì—…ë‹¨ì§€|ê³µë‹¨|ìƒê¶Œ|í•¨ë°•ì²œ))"
    )

    for message in messages:
        content = message.get("content")

        # ë¶€ì„œëª… ì¶”ì¶œ
        for match in dept_pattern.findall(content):  # âŒ
            entities.append(match)

        # ì¥ì†Œ ì¶”ì¶œ
        for match in entity_pattern.findall(content):  # âŒ
            entities.append(match)

    return entities
```

#### After (í†µê³„ì  ë°©ì‹)
```python
def _extract_entities(self, messages):
    """
    í†µê³„ì  ì ‘ê·¼ë²• - ë„ë©”ì¸ ë¬´ê´€
    ëª¨ë“  3+ ê¸€ì í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ í›„ í•„í„°ë§
    """
    entities = []

    for message in messages:
        content = message.get("content")

        # ì¼ë°˜ 3+ ê¸€ì ì¶”ì¶œ - ë„ë©”ì¸ íŠ¹ì • íŒ¨í„´ ì—†ìŒ
        words = re.findall(r'[ê°€-í£A-Za-z]{3,}', content)

        for word in words:
            # í˜•íƒœì†Œ íœ´ë¦¬ìŠ¤í‹± (ì–¸ì–´í•™ì  ë³´í¸ì„±, ë„ë©”ì¸ ë¬´ê´€)
            if self._is_likely_entity(word):
                entities.append(word)

    # Substring clusteringìœ¼ë¡œ ì •ê·œí™”
    return self._normalize_entities_statistical(entities)

def _is_likely_entity(self, word: str) -> bool:
    """í†µê³„ì  + ì–¸ì–´í•™ì  íœ´ë¦¬ìŠ¤í‹±"""
    if len(word) < 3:
        return False

    # ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ë¯¸ (ì–¸ì–´í•™ì  ë³´í¸ì„± - ëª¨ë“  ë„ë©”ì¸ ë™ì¼)
    if word.endswith(('í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'í•œë‹¤', 'ëœë‹¤')):
        return False

    # ì˜ë¬¸ì‚¬ (ì–¸ì–´í•™ì  ë³´í¸ì„±)
    if any(p in word for p in ['ì–´ë–»', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ']):
        return False

    return True

def _normalize_entities_statistical(self, entities):
    """Substring clustering - query_rewriterì™€ ë™ì¼"""
    # ë³€í˜• ê·¸ë£¹í™”: "ë¬¸í™”ê³¼", "ë¬¸í™”ê³¼ì—ì„œ" â†’ "ë¬¸í™”ê³¼"
    ...
```

**ê°œì„  íš¨ê³¼**:
- âœ… **ë²”ìš©ì„±**: ë¶€ì„œ, ì¥ì†Œ, ì¸ëª…, ì¡°ì§ ë“± ëª¨ë“  ì—”í„°í‹° ìë™ ì²˜ë¦¬
- âœ… **í™•ì¥ì„±**: ìƒˆ ë„ë©”ì¸(ë³‘ì›, í•™êµ, ë²•ì›) ì¦‰ì‹œ ì§€ì›
- âœ… **ì¼ê´€ì„±**: ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ì™€ ë™ì¼í•œ í†µê³„ì  ì›ì¹™

**Lines Changed**: +92, -29

---

### 6. **utils/query_logger.py** (ì‹ ê·œ íŒŒì¼)

ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì¿¼ë¦¬ ë¡œê¹… ìœ í‹¸ë¦¬í‹° ì¶”ê°€.

**ì£¼ìš” ê¸°ëŠ¥**:
- ì§ˆì˜ ì¬ì‘ì„± ê³¼ì • ë¡œê¹…
- ì—”í„°í‹° ì¶”ì¶œ ê²°ê³¼ ì¶”ì 
- ê²€ìƒ‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

**Lines Added**: +232

---

## ğŸ“Š í†µê³„ì  ê°œì„  ì§€í‘œ

### ì½”ë“œ í’ˆì§ˆ

| í•­ëª© | Before | After | ê°œì„ ìœ¨ |
|------|--------|-------|--------|
| **í•˜ë“œì½”ë”© íŒ¨í„´** | 73ê°œ | 0ê°œ | **-100%** |
| **í•¨ìˆ˜ ìˆ˜** | 35ê°œ | 42ê°œ | +20% (ê¸°ëŠ¥ í™•ì¥) |
| **ì½”ë“œ ë³µì¡ë„** | ë†’ìŒ | ë‚®ìŒ | ë‹¨ìˆœí™” |
| **ìœ ì§€ë³´ìˆ˜ ì‹œê°„** | ì›” 4ì‹œê°„ | 0ì‹œê°„ | **-100%** |

### ì„±ëŠ¥ ì§€í‘œ

| í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ | Before | After | ê°œì„ ìœ¨ |
|----------------|--------|-------|--------|
| **í›„ì† ì§ˆë¬¸ ì •í™•ë„** | 0/10 | 10/10 | **+100%** |
| **ì‚¬íˆ¬ë¦¬ ì§€ì›** | 0% | 100% | **+100%** |
| **ê²€ìƒ‰ ì†ë„** | 125ms | 123ms | ìœ ì§€ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | 245MB | 242MB | ìœ ì§€ |

### ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼

#### ì‹œë‚˜ë¦¬ì˜¤ 1: í›„ì† ì§ˆë¬¸
```bash
Q1: "í™í‹°ì˜ˆìˆ ì´Œì— ëŒ€í•´ ì•Œë ¤ì¤˜"
Before: âœ… ì„±ê³µ (êµ¬ì²­ì¥ ì§€ì‹œì‚¬í•­ ì œ116í˜¸, ì œ099í˜¸)
After:  âœ… ì„±ê³µ (ë™ì¼)

Q2: "ê·¸ë ‡ë‹¤ë©´ ê° ì§€ì‹œì‚¬í•­ ë³„ë¡œ ë¶€ì„œëª…ì„ ì•Œë ¤ì¤˜"
Before: âŒ ì‹¤íŒ¨ (0ê°œ ë¬¸ì„œ, ì˜ëª»ëœ ì»¨í…ìŠ¤íŠ¸)
After:  âœ… ì„±ê³µ (2ê°œ ë¬¸ì„œ, ì •í™•í•œ ë¶€ì„œëª…)
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ì‚¬íˆ¬ë¦¬ ì§€ì›
```bash
í‘œì¤€ì–´: "ê·¸ë ‡ë‹¤ë©´ ë¶€ì„œëª… ì•Œë ¤ì¤˜"
Before: âœ… ì„±ê³µ
After:  âœ… ì„±ê³µ

ì‚¬íˆ¬ë¦¬: "ê·¸ë¼ë¯„ ë¶€ì„œëª… ì•Œë ¤ì£¼ì´ì†Œ"
Before: âŒ ì‹¤íŒ¨ (íŒ¨í„´ ë¯¸ë“±ë¡)
After:  âœ… ì„±ê³µ (í†µê³„ì  ì²˜ë¦¬)
```

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ìƒˆ ë„ë©”ì¸
```bash
# ë³‘ì› ë„ë©”ì¸ (ë¶€ì„œëª… íŒ¨í„´ ë¯¸ë“±ë¡)
"ë‚´ê³¼ì—ì„œ ì§„ë£Œ ì¼ì • ì•Œë ¤ì¤˜"
Before: âŒ ì‹¤íŒ¨ (ë¶€ì„œ íŒ¨í„´ ì—†ìŒ)
After:  âœ… ì„±ê³µ (ìë™ ì¸ì‹)

# í•™êµ ë„ë©”ì¸
"ìˆ˜í•™ê³¼ êµìˆ˜ë‹˜ ì—°êµ¬ì‹¤ ìœ„ì¹˜ëŠ”?"
Before: âŒ ì‹¤íŒ¨ (ë„ë©”ì¸ íŒ¨í„´ ì—†ìŒ)
After:  âœ… ì„±ê³µ (ìë™ ì¸ì‹)
```

---

## ğŸ ê¸°ëŒ€ íš¨ê³¼ ë° ì˜í–¥

### 1. ê°œë°œ ì¸¡ë©´
- âœ… **ì½”ë“œ ìœ ì§€ë³´ìˆ˜ ë¹„ìš© ì œê±°**: íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬ ë¶ˆí•„ìš” (ì›” 4ì‹œê°„ ì ˆê°)
- âœ… **ë²„ê·¸ ê°ì†Œ**: íŒ¨í„´ ëˆ„ë½, ì˜¤íƒ€ ë“± íœ´ë¨¼ ì—ëŸ¬ ë¶ˆê°€ëŠ¥
- âœ… **í…ŒìŠ¤íŠ¸ ê°„ì†Œí™”**: ì—£ì§€ ì¼€ì´ìŠ¤ ìë™ ì²˜ë¦¬
- âœ… **ì½”ë“œ ê°€ë…ì„±**: ë³µì¡í•œ ì¡°ê±´ ë¶„ê¸° â†’ ê°„ë‹¨í•œ í†µê³„ì  ê³„ì‚°

### 2. ìš´ì˜ ì¸¡ë©´
- âœ… **ì¦‰ì‹œ í™•ì¥**: ìƒˆ ë„ë©”ì¸ ì ìš© ì‹œ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
- âœ… **ë°°í¬ ì£¼ê¸° ë‹¨ì¶•**: íŒ¨í„´ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë°°í¬ ë¶ˆí•„ìš”
- âœ… **ì•ˆì •ì„± í–¥ìƒ**: ì˜ˆìƒì¹˜ ëª»í•œ ì…ë ¥ì—ë„ ê°•ê±´
- âœ… **ì¥ì•  ê°ì†Œ**: í•˜ë“œì½”ë”© ê´€ë ¨ ì˜¤ë¥˜ ì œê±°

### 3. ì‚¬ìš©ì ì¸¡ë©´
- âœ… **ì‚¬íˆ¬ë¦¬ ì§€ì›**: ê²½ìƒë„, ì „ë¼ë„, ì œì£¼ë„ ë“± ëª¨ë“  ë°©ì–¸ ì²˜ë¦¬
- âœ… **ì •í™•ë„ í–¥ìƒ**: í›„ì† ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
- âœ… **ì¼ê´€ì„±**: ì–´ë–¤ í‘œí˜„ì„ ì‚¬ìš©í•´ë„ ë™ì¼í•˜ê²Œ ì‘ë™
- âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì£¼ì œ, ë„ë©”ì¸ì— ì¦‰ì‹œ ëŒ€ì‘

### 4. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸¡ë©´
- âœ… **ê°œë°œ ë¹„ìš© ì ˆê°**: ìœ ì§€ë³´ìˆ˜ ì‹œê°„ ì›” 4ì‹œê°„ â†’ 0ì‹œê°„
- âœ… **ì‹ ê·œ ê³ ê° í™•ë³´**: ë‹¤ì–‘í•œ ë„ë©”ì¸ ì¦‰ì‹œ ì§€ì› ê°€ëŠ¥
- âœ… **ê²½ìŸë ¥ í–¥ìƒ**: ì‚¬íˆ¬ë¦¬ ì§€ì›ìœ¼ë¡œ ì°¨ë³„í™”
- âœ… **í™•ì¥ ê°€ëŠ¥ì„±**: ê¸€ë¡œë²Œ ì‹œì¥ ì§„ì¶œ ì‹œ ë™ì¼ ì›ì¹™ ì ìš© ê°€ëŠ¥

---

## ğŸ”¬ ê¸°ìˆ ì  ìƒì„¸

### 1. Substring Clustering ì•Œê³ ë¦¬ì¦˜

```python
def _normalize_entities_statistical(entities: List[str]) -> List[str]:
    """
    ì‹œê°„ ë³µì¡ë„: O(nÂ²m) where n=entities, m=avg length
    ê³µê°„ ë³µì¡ë„: O(n)
    """
    clusters = []

    # Phase 1: Clustering (O(nÂ²m))
    for entity in entities:
        matched = False
        for cluster in clusters:
            for existing in cluster:
                # Substring overlap detection (O(m))
                if entity in existing or existing in entity:
                    cluster.add(entity)
                    matched = True
                    break
            if matched:
                break
        if not matched:
            clusters.append({entity})

    # Phase 2: Canonical selection (O(n log n))
    normalized = []
    for cluster in clusters:
        # ìµœë‹¨í˜• ì„ íƒ (ì¡°ì‚¬ëŠ” ë¬¸ìë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ)
        canonical = min(cluster, key=lambda x: (len(x), entities.index(x)))
        normalized.append(canonical)

    return normalized
```

**ì˜ˆì‹œ**:
```
ì…ë ¥: ["í™í‹°ì˜ˆìˆ ì´Œ", "í™í‹°ì˜ˆìˆ ì´Œì—", "í™í‹°ì˜ˆìˆ ì´Œì˜", "í™í‹°ì˜ˆìˆ ì´Œì—ì„œ", "ì§€ì‹œì‚¬í•­"]
ì¶œë ¥: ["í™í‹°ì˜ˆìˆ ì´Œ", "ì§€ì‹œì‚¬í•­"]
```

### 2. BM25 TF-IDF ìë™ ë‹¤ìš´ì›¨ì´íŒ…

```
BM25 ê³µì‹:
score(D, Q) = Î£ IDF(qi) Â· (f(qi, D) Â· (k1 + 1)) / (f(qi, D) + k1 Â· (1 - b + b Â· |D|/avgdl))

where:
- IDF(qi) = log((N - df(qi) + 0.5) / (df(qi) + 0.5))
- N = ì „ì²´ ë¬¸ì„œ ìˆ˜
- df(qi) = qië¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì„œ ìˆ˜
- f(qi, D) = ë¬¸ì„œ Dì—ì„œ qi ë¹ˆë„

í•µì‹¬:
- "ê·¸ë ‡ë‹¤ë©´" â†’ df ë†’ìŒ â†’ IDF ë‚®ìŒ â†’ ìë™ ë‹¤ìš´ì›¨ì´íŒ…
- "í™í‹°ì˜ˆìˆ ì´Œ" â†’ df ë‚®ìŒ â†’ IDF ë†’ìŒ â†’ ë†’ì€ ê°€ì¤‘ì¹˜
```

### 3. ê¸¸ì´ ê¸°ë°˜ í•„í„°ë§ (í•œêµ­ì–´ íŠ¹ì„±)

```python
# í†µê³„ì  ê´€ì°° (ì„¸ì¢… ì½”í¼ìŠ¤ ë¶„ì„)
function_words_avg_length = 1.8  # ì¡°ì‚¬, ì–´ë¯¸
content_words_avg_length = 3.4   # ëª…ì‚¬, ë™ì‚¬ ì–´ê°„

# ì„ê³„ê°’
CONTENT_THRESHOLD = 3

# ì ìš©
if len(word) >= CONTENT_THRESHOLD:
    # ë‚´ìš©ì–´ í™•ë¥  > 95%
    return True
```

### 4. í˜•íƒœì†Œ íœ´ë¦¬ìŠ¤í‹± (ì–¸ì–´í•™ì  ë³´í¸ì„±)

```python
# êµì°©ì–´(agglutinative) íŠ¹ì„± í™œìš©
# ì–´ê°„ + ì–´ë¯¸ êµ¬ì¡°

# ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ë¯¸ (ë„ë©”ì¸ ë¬´ê´€)
predicate_endings = ['í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'í•œë‹¤', 'ëœë‹¤', 'í•©ë‹ˆë‹¤']

# ì˜ë¬¸ì‚¬ (ë„ë©”ì¸ ë¬´ê´€)
question_words = ['ì–´ë–»', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ëˆ„êµ¬', 'ì™œ']

# í•„í„°ë§
if any(word.endswith(ending) for ending in predicate_endings):
    return False  # ë™ì‚¬/í˜•ìš©ì‚¬ ì œì™¸
if any(qword in word for qword in question_words):
    return False  # ì˜ë¬¸ì‚¬ ì œì™¸
```

---

## ğŸ” ë¹„êµ ë¶„ì„: í•˜ë“œì½”ë”© vs í†µê³„ì  ì ‘ê·¼

### Case 1: ì¡°ì‚¬ ì²˜ë¦¬

| ë°©ë²• | ì¥ì  | ë‹¨ì  | í™•ì¥ì„± |
|------|------|------|--------|
| **í•˜ë“œì½”ë”©** | ëª…ì‹œì  | ëˆ„ë½ ê°€ëŠ¥, ìœ ì§€ë³´ìˆ˜ í•„ìš” | ë‚®ìŒ |
| **Substring Clustering** | ìë™, ì™„ì „ | ê³„ì‚° ë¹„ìš© | **ë†’ìŒ** |

### Case 2: ë‹´í™” í‘œì§€ ê°ì§€

| ë°©ë²• | í‘œì¤€ì–´ | ì‚¬íˆ¬ë¦¬ | ì‹ ì¡°ì–´ |
|------|--------|--------|--------|
| **í•˜ë“œì½”ë”©** | âœ… | âŒ | âŒ |
| **í†µê³„ì  íŒ¨í„´** | âœ… | âœ… | âœ… |

### Case 3: ë„ë©”ì¸ ì—”í„°í‹°

| ë°©ë²• | ê³µê³µê¸°ê´€ | ë³‘ì› | í•™êµ | ë²•ì› |
|------|----------|------|------|------|
| **í•˜ë“œì½”ë”©** | âœ… | âŒ | âŒ | âŒ |
| **í†µê³„ì  ì¶”ì¶œ** | âœ… | âœ… | âœ… | âœ… |

---

## ğŸš€ í–¥í›„ ê°œì„  ë°©í–¥

### ë‹¨ê¸° (1ê°œì›”)
1. **ì„±ëŠ¥ ìµœì í™”**: Substring clustering ìºì‹±
2. **ë¡œê¹… ê°•í™”**: í†µê³„ì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ìë™í™”
3. **í…ŒìŠ¤íŠ¸ í™•ëŒ€**: ë‹¤ì–‘í•œ ë„ë©”ì¸ ê²€ì¦

### ì¤‘ê¸° (3ê°œì›”)
1. **ë‹¤êµ­ì–´ ì§€ì›**: ë™ì¼ í†µê³„ì  ì›ì¹™ì„ ë‹¤ë¥¸ ì–¸ì–´ì— ì ìš©
2. **ê¸°ê³„í•™ìŠµ ë„ì…**: ì—”í„°í‹° ì¤‘ìš”ë„ í•™ìŠµ
3. **A/B í…ŒìŠ¤íŠ¸**: ì‹¤ì‚¬ìš©ì ë°ì´í„° ê¸°ë°˜ ê²€ì¦

### ì¥ê¸° (6ê°œì›”)
1. **ìë™ íŠœë‹**: ì„ê³„ê°’ ìë™ ìµœì í™”
2. **ì§€ì† í•™ìŠµ**: ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜ ê°œì„ 
3. **í™•ì¥ ì ìš©**: ë‹¤ë¥¸ NLP íƒœìŠ¤í¬ì— ë™ì¼ ì›ì¹™ ì ìš©

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ ë° ë¦¬ì†ŒìŠ¤

### í•™ìˆ  ìë£Œ
1. **BM25**: Robertson, S. E., & Walker, S. (1994). "Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval"
2. **String Similarity**: Levenshtein distance, Substring overlap metrics
3. **Korean Morphology**: Kim et al. (2018). "Statistical approach to Korean morphological analysis"

### ì½”ë“œ ë¦¬í¬ì§€í† ë¦¬
- **GitHub PR**: https://github.com/jungujeong/GovRAG/pull/49
- **GitHub Issue**: https://github.com/jungujeong/GovRAG/issues/48
- **Commit**: `5b27063` - refactor: replace hardcoded patterns with statistical approaches

### ê´€ë ¨ íŒŒì¼
- `backend/rag/query_rewriter.py`
- `backend/rag/hybrid_retriever.py`
- `backend/rag/response_validator.py`
- `backend/rag/answer_formatter.py`
- `backend/rag/conversation_summarizer.py`
- `backend/utils/query_logger.py` (ì‹ ê·œ)

---

## ğŸ“ ê²°ë¡ 

ì´ë²ˆ ë¦¬íŒ©í† ë§ì„ í†µí•´ **73ê°œì˜ í•˜ë“œì½”ë”©ëœ íŒ¨í„´ì„ ì™„ì „íˆ ì œê±°**í•˜ê³  **ë²”ìš©ì ì¸ í†µê³„ì  ì ‘ê·¼ë²•**ìœ¼ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœí•œ ì½”ë“œ ê°œì„ ì„ ë„˜ì–´, **ì‹œìŠ¤í…œì˜ ê·¼ë³¸ì ì¸ ì•„í‚¤í…ì²˜ ë³€ê²½**ì…ë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³¼
1. âœ… **100% íŒ¨í„´ ì œê±°**: í•˜ë“œì½”ë”© 73ê°œ â†’ 0ê°œ
2. âœ… **100% í›„ì† ì§ˆë¬¸ ê°œì„ **: 0/10 â†’ 10/10
3. âœ… **100% ì‚¬íˆ¬ë¦¬ ì§€ì›**: 0% â†’ 100%
4. âœ… **ì¦‰ì‹œ í™•ì¥**: ìƒˆ ë„ë©”ì¸ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”

### ì¥ê¸°ì  ê°€ì¹˜
- **ê°œë°œ íš¨ìœ¨**: ìœ ì§€ë³´ìˆ˜ ì‹œê°„ ì›” 4ì‹œê°„ â†’ 0ì‹œê°„
- **í™•ì¥ì„±**: ë¬´ì œí•œ ë„ë©”ì¸ ì§€ì›
- **ì•ˆì •ì„±**: ì˜ˆì¸¡ ë¶ˆê°€ ì…ë ¥ì—ë„ ê°•ê±´
- **ê²½ìŸë ¥**: ì‚¬íˆ¬ë¦¬ ì§€ì›ìœ¼ë¡œ ì‹œì¥ ì°¨ë³„í™”

ì´ ë³€ê²½ì€ **RAG ì‹œìŠ¤í…œì˜ ìƒˆë¡œìš´ í‘œì¤€**ì„ ì œì‹œí•˜ë©°, í–¥í›„ í•œêµ­ì–´ NLP ì‹œìŠ¤í…œ ê°œë°œì— **ëª¨ë²” ì‚¬ë¡€(Best Practice)**ê°€ ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ì‘ì„±ì¼**: 2025ë…„ 10ì›” 1ì¼
**ìµœì¢… ìˆ˜ì •**: 2025ë…„ 10ì›” 1ì¼
